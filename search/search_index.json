{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SALT Documentation","text":""},{"location":"#welcome-to-the-salt-project-documentation","title":"Welcome to the SALT project documentation!","text":"<p>This documentation serves as the official guide for the SALT project, which is part of the Sunbird AI Language Projects. The goal of this documentation is to provide you with comprehensive information on how to use the Leb project effectively.</p>"},{"location":"reference/","title":"Reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the <code>SALT</code> project code.</p>"},{"location":"API/","title":"Sunbird API \u2014 Tasks Endpoints (Developer Reference)","text":"<p>Version: 1.0 Last updated: 2025-10-08 Maintainer: Sunbird AI</p> <p>Welcome to the Sunbird AI API documentation. The Sunbird AI API provides you access to Sunbird's language models. </p>"},{"location":"API/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quickstart</li> <li>Global behavior and requirements</li> <li>POST /tasks/stt \u2014 Speech-to-text</li> <li>POST /tasks/nllb_translate \u2014 Neural translation</li> <li>POST /tasks/language_id \u2014 Language identification</li> <li>POST /tasks/summarise \u2014 Text summarisation</li> <li>POST /tasks/tts \u2014 Text-to-Speech (TTS)</li> <li>Error handling &amp; retries</li> <li>Practical tips &amp; checklist</li> <li>Troubleshooting &amp; Support</li> </ul>"},{"location":"API/#quickstart","title":"Quickstart","text":"<ol> <li>Get an API key from your Sunbird dashboard.</li> <li>Call any endpoint with Authorization: Bearer YOUR_API_KEY.</li> </ol> <p>Example header (all requests):</p> <pre><code>Authorization: Bearer YOUR_API_TOKEN\nContent-Type: application/json\n</code></pre> <p>If you use multipart uploads (STT), set Content-Type to multipart/form-data in your client.</p> <p></p>"},{"location":"API/#rate-limiting-and-quotas","title":"Rate limiting and quotas","text":"<p>Rate limits are enforced by account type (derived from JWT <code>account_type</code>):</p> <ul> <li>Free Tier: Limited requests per hour</li> <li>Professional: Higher rate limits</li> <li>Enterprise: Custom rate limits</li> </ul> <p>On 429 responses, implement exponential backoff and jitter. Monitor usage to avoid throttling.</p>"},{"location":"API/#sunbird-api-tasks-endpoints-authoritative-reference","title":"Sunbird API \u2014 Tasks Endpoints (Authoritative Reference)","text":"<p>Version: 1.3 Last updated: 2025-10-08 Maintainer: Sunbird AI</p> <p>This document covers exactly the five /tasks endpoints requested:</p> <ul> <li>POST /tasks/stt</li> <li>POST /tasks/nllb_translate</li> <li>POST /tasks/language_id</li> <li>POST /tasks/summarise</li> <li>POST /tasks/tts</li> </ul> <p>Each section below contains: purpose, precise request/response schema, cURL/Python examples, common errors, limits, and integration tips.</p>"},{"location":"API/#global-behavior-and-requirements","title":"Global behavior and requirements","text":"<ul> <li>Authentication: All endpoints require an <code>Authorization: Bearer &lt;API_KEY&gt;</code> header.</li> <li>Content-Type: <code>application/json</code> for JSON endpoints; <code>multipart/form-data</code> for file uploads (STT).</li> <li>Rate limiting: enforced per account_type (admin: 1000/min, premium: 100/min, standard: 50/min).</li> <li>Timeouts: Server-side model/worker timeouts may return 408/503/504. These are retryable in most cases.</li> </ul> <p>Signed storage URLs - TTS returns a signed Google Cloud Storage URL in <code>output.audio_url</code>. These URLs are time-limited (typically ~1800s). If you need persistent access, download and store the file server-side, or use <code>output.blob</code> to re-create a signed URL from server-side credentials.</p> <p>Error response format</p> <pre><code>{ \"detail\": \"Human readable error message\" }\n</code></pre> <p></p>"},{"location":"API/#post-tasksstt-speech-to-text-file-upload","title":"POST /tasks/stt \u2014 Speech-to-text (file upload)","text":""},{"location":"API/#purpose","title":"Purpose","text":"<ul> <li>Upload an audio file and receive a transcription. Optional speaker diarization and whisper-assisted decoding are available.</li> </ul>"},{"location":"API/#limits-behaviour","title":"Limits &amp; behaviour","text":"<ul> <li>Supported file types: mp3, wav, ogg, m4a, aac.</li> <li>Max processed duration: 10 minutes. Longer audio is trimmed to first 10 minutes server-side; <code>was_audio_trimmed</code> will be set to <code>true</code> in the response.</li> <li>For very large files (&gt;100MB) use the signed upload flow and call <code>/tasks/stt_from_gcs</code>.</li> </ul>"},{"location":"API/#request-multipartform-data","title":"Request (multipart/form-data)","text":"<ul> <li>audio: file (required)</li> <li>language: <code>SttbLanguage</code> enum (default: <code>lug</code>)</li> <li>adapter: <code>SttbLanguage</code> enum (default: <code>lug</code>)</li> <li>recognise_speakers: boolean (default: false)</li> <li>whisper: boolean (default: false)</li> </ul>"},{"location":"API/#examples-curl-python","title":"Examples (cURL / Python)","text":""},{"location":"API/#curl-example-multipart","title":"cURL example (multipart)","text":"<pre><code>curl -X POST \"https://api.sunbird.ai/tasks/stt\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -F \"audio=@sample.mp3;type=audio/mpeg\" \\\n  -F \"language=lug\" \\\n  -F \"adapter=lug\" \\\n  -F \"recognise_speakers=false\" \\\n  -F \"whisper=false\"\n</code></pre> <p>Python example (requests)</p> <pre><code>import requests\n\nurl = 'https://api.sunbird.ai/tasks/stt'\nheaders = {'Authorization': 'Bearer YOUR_API_KEY'}\nfiles = {'audio': open('sample.mp3', 'rb')}\ndata = {'language': 'lug', 'adapter': 'lug'}\n\nresp = requests.post(url, headers=headers, files=files, data=data, timeout=180)\nresp.raise_for_status()\nprint(resp.json())\n</code></pre>"},{"location":"API/#successful-response-stttranscript","title":"Successful response (STTTranscript)","text":"<pre><code>{\n  \"audio_transcription\": \"Transcribed text...\",\n  \"diarization_output\": {},\n  \"formatted_diarization_output\": \"Speaker 1: ...\\nSpeaker 2: ...\",\n  \"audio_transcription_id\": 123,\n  \"audio_url\": \"gs://&lt;bucket&gt;/&lt;path&gt;\",\n  \"language\": \"lug\",\n  \"was_audio_trimmed\": false,\n  \"original_duration_minutes\": null\n}\n</code></pre>"},{"location":"API/#request-parameters-table","title":"Request Parameters (table)","text":"Name Type Required Default Description audio file Yes - Audio file to transcribe (mp3/wav/ogg/m4a/aac) language string No \"lug\" Target language / transcription model adapter adapter string No \"lug\" Adapter preference for model (same enum as language) recognise_speakers boolean No false Enable speaker diarization whisper boolean No false Use whisper-assisted decoding if available"},{"location":"API/#response-fields-table","title":"Response Fields (table)","text":"Name Type Description audio_transcription string Full transcription text diarization_output object Detailed diarization data (timestamps, speaker ids) formatted_diarization_output string Human-readable speaker-labelled transcript audio_transcription_id integer/null Optional DB id for stored transcription audio_url string GCS path (if stored) language string Detected/used language code was_audio_trimmed boolean True if original audio exceeded processing limit original_duration_minutes float/null Original audio length when trimmed"},{"location":"API/#common-errors","title":"Common errors","text":"<p>Error response examples</p> <ul> <li>400 Bad Request \u2014 Missing file</li> </ul> <pre><code>curl -i -X POST \"https://api.sunbird.ai/tasks/stt\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\"\n</code></pre> <p>HTTP/1.1 400 Bad Request <pre><code>{ \"detail\": \"Missing 'audio' file in request\" }\n</code></pre></p> <ul> <li>422 Unprocessable Entity \u2014 No transcription produced</li> </ul> <pre><code>curl -i -X POST \"https://api.sunbird.ai/tasks/stt\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -F \"audio=@silence.mp3;type=audio/mpeg\"\n</code></pre> <p>HTTP/1.1 422 Unprocessable Entity <pre><code>{ \"detail\": \"No transcription generated from the provided audio\" }\n</code></pre></p> <ul> <li>503 Service Unavailable \u2014 Worker timeout</li> </ul> <pre><code>curl -i -X POST \"https://api.sunbird.ai/tasks/stt\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -F \"audio=@large_file.mp3;type=audio/mpeg\"\n</code></pre> <p>HTTP/1.1 503 Service Unavailable <pre><code>{ \"detail\": \"Transcription worker timed out. Please retry later.\" }\n</code></pre> - 400 \u2014 Missing file or invalid parameters. - 415 \u2014 Unsupported media type. - 422 \u2014 No transcription produced. - 503 / 504 \u2014 Worker timeout or backend unavailable (retryable).</p> <p>Integration tips - Re-encode to 16 kHz, mono, 16-bit PCM to minimize decoding errors. - If you control the client, chunk long audio into smaller segments and send multiple requests.</p> <p></p>"},{"location":"API/#post-tasksnllb_translate-neural-translation","title":"POST /tasks/nllb_translate \u2014 Neural translation","text":""},{"location":"API/#purpose_1","title":"Purpose","text":"<ul> <li>Translate text between English and supported local languages.</li> </ul> <p>Supported codes: <code>ach</code>, <code>teo</code>, <code>eng</code>, <code>lug</code>, <code>lgg</code>, <code>nyn</code>.</p>"},{"location":"API/#request-applicationjson","title":"Request (application/json)","text":"<pre><code>{\n  \"source_language\": \"eng\",\n  \"target_language\": \"lug\",\n  \"text\": \"How are you?\"\n}\n</code></pre>"},{"location":"API/#examples-curl","title":"Examples (cURL)","text":"<pre><code>curl -X POST \"https://api.sunbird.ai/tasks/nllb_translate\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"source_language\":\"eng\",\"target_language\":\"lug\",\"text\":\"How are you?\"}'\n</code></pre>"},{"location":"API/#response-normalized-worker-wrapper","title":"Response (normalized worker wrapper)","text":"<pre><code>{\n  \"id\": \"job-id\",\n  \"status\": \"success\",\n  \"executionTime\": 123,\n  \"output\": {\n    \"text\": \"How are you?\",\n    \"translated_text\": \"Oli otya?\",\n    \"source_language\": \"eng\",\n    \"target_language\": \"lug\",\n    \"Error\": null\n  }\n}\n</code></pre> <p>Request Parameters (table)</p> Name Type Required Default Description source_language string Yes - Source language code (ach target_language string Yes - Target language code text string Yes - Text to translate <p>Response Fields (table)</p> Name Type Description id string Job identifier status string Job status (success/failure) executionTime integer Execution time in ms output.text string Original input text (optional) output.translated_text string Translated text output.source_language string Source language returned by worker output.target_language string Target language returned by worker output.Error string/null Error message if translation failed workerId string/null Worker instance id"},{"location":"API/#errors-retries","title":"Errors &amp; retries","text":"<ul> <li>400 \u2014 invalid or missing fields.</li> <li>503 \u2014 transient worker failures (retry with backoff).</li> </ul> <p>Integration tips - Keep text short (&lt;512 tokens) for consistent quality. - For bulk translations, run sequential requests or implement a batching layer.</p> <p></p>"},{"location":"API/#post-taskslanguage_id-language-identification","title":"POST /tasks/language_id \u2014 Language identification","text":""},{"location":"API/#purpose_2","title":"Purpose","text":"<ul> <li>Identify which language (from a limited set) a short text is written in.</li> </ul>"},{"location":"API/#request","title":"Request","text":"<pre><code>{ \"text\": \"Nkwagala nnyo\" }\n</code></pre> <p>cURL example</p> <pre><code>curl -X POST \"https://api.sunbird.ai/tasks/language_id\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\":\"Nkwagala nnyo\"}'\n</code></pre>"},{"location":"API/#response","title":"Response","text":"<pre><code>{ \"language\": \"lug\" }\n</code></pre> <p>Request Parameters (table)</p> Name Type Required Default Description text string Yes - Text to identify language for <p>Response Fields (table)</p> Name Type Description language string Detected language code (ach confidence float/null (Optional) Confidence score if provided by the service <p>Notes - Supported output labels: <code>ach</code>, <code>teo</code>, <code>eng</code>, <code>lug</code>, <code>lgg</code>, <code>nyn</code>. - If confidence is low the service may return <code>language not detected</code>.</p> <ul> <li>Aggregate predictions across sentences for mixed or longer text.</li> </ul>"},{"location":"API/#errors","title":"Errors","text":"<p>Integration tips - Aggregate predictions across sentences for mixed or longer text. - Aggregate predictions across sentences for mixed or longer text.</p> <p>Error response examples</p> <ul> <li>400 Bad Request \u2014 Missing <code>text</code> field</li> </ul> <pre><code>curl -i -X POST \"https://api.sunbird.ai/tasks/language_id\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{}'\n</code></pre> <p>HTTP/1.1 400 Bad Request <pre><code>{ \"detail\": \"Missing required field: text\" }\n</code></pre></p> <ul> <li>422 Unprocessable Entity \u2014 Low confidence / not detected</li> </ul> <pre><code>curl -i -X POST \"https://api.sunbird.ai/tasks/language_id\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\":\"...\"}'\n</code></pre> <p>HTTP/1.1 422 Unprocessable Entity <pre><code>{ \"detail\": \"Language not detected with sufficient confidence\" }\n</code></pre></p> <ul> <li>503 Service Unavailable \u2014 Worker unavailable</li> </ul> <pre><code>curl -i -X POST \"https://api.sunbird.ai/tasks/language_id\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\":\"Nkwagala nnyo\"}'\n</code></pre> <p>HTTP/1.1 503 Service Unavailable <pre><code>{ \"detail\": \"Language identification service unavailable. Please retry later.\" }\n</code></pre></p> <p></p>"},{"location":"API/#post-taskssummarise-text-summarisation","title":"POST /tasks/summarise \u2014 Text summarisation","text":""},{"location":"API/#purpose_3","title":"Purpose","text":"<ul> <li>Produce anonymised short summaries of long text.</li> </ul>"},{"location":"API/#request_1","title":"Request","text":"<pre><code>{ \"text\": \"Very long article or conversation...\" }\n</code></pre> <p>cURL example</p> <pre><code>curl -X POST \"https://api.sunbird.ai/tasks/summarise\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\":\"Very long text to summarise...\"}'\n</code></pre>"},{"location":"API/#response_1","title":"Response","text":"<pre><code>{ \"summarized_text\": \"Short summary...\" }\n</code></pre> <p>Request Parameters (table)</p> Name Type Required Default Description text string Yes - Text to summarise (longer texts can be chunked) <p>Response Fields (table)</p> Name Type Description summarized_text string Generated short summary language string/null (Optional) Detected language of the input processing_time float/null (Optional) Server processing time in seconds <ul> <li>Supported languages: English and Luganda.</li> <li>For very long texts chunk and summarise incrementally, then combine.</li> <li>The service applies basic anonymisation heuristics; do not rely on it for legal PII removal.</li> </ul>"},{"location":"API/#notes","title":"Notes","text":"<ul> <li>Supported languages: English and Luganda.</li> <li>For very long texts chunk and summarise incrementally, then combine.</li> <li>The service applies basic anonymisation heuristics; do not rely on it for legal PII removal.</li> <li>Supported languages: English and Luganda.</li> <li>For very long texts chunk and summarise incrementally, then combine.</li> <li>The service applies basic anonymisation heuristics; do not rely on it for legal PII removal.</li> </ul>"},{"location":"API/#post-taskstts-text-to-speech-tts","title":"POST /tasks/tts \u2014 Text-to-Speech (TTS)","text":""},{"location":"API/#text-to-speech-tts-api-documentation","title":"Text-to-Speech (TTS) API Documentation","text":""},{"location":"API/#overview","title":"Overview","text":"<ul> <li>The TTS endpoint converts text to speech using Sunbird AI's multilingual text-to-speech models. This service supports multiple Ugandan languages and provides high-quality voice synthesis for applications such as IVR, accessibility, language learning, and content generation.</li> </ul> <p>Endpoint Details - URL: POST /tasks/tts - Authentication: Bearer Token required - Rate Limiting: Applied based on account type (Admin: 1000/min, Premium: 100/min, Standard: 50/min)</p>"},{"location":"API/#supported-languages-voices","title":"Supported Languages &amp; Voices","text":"Language Speaker ID Voice Type Description Acholi 241 Female Native Acholi speaker Ateso 242 Female Native Ateso speaker Runyankole 243 Female Native Runyankole speaker Lugbara 245 Female Native Lugbara speaker Swahili 246 Male Native Swahili speaker Luganda 248 Female Native Luganda speaker (default)"},{"location":"API/#request-format","title":"Request Format","text":"<p>Headers</p> <pre><code>Authorization: Bearer YOUR_API_TOKEN\nContent-Type: application/json\n</code></pre> <p>Request Body (JSON)</p> <pre><code>{\n  \"text\": \"Oli otya? Nkwagala nnyo.\",\n  \"speaker_id\": 248,\n  \"temperature\": 0.7,\n  \"max_new_audio_tokens\": 2000\n}\n</code></pre> <p>Parameters</p> Parameter Type Required Default Range Description text string Yes - 1-5000 Text to convert to speech speaker_id integer No 248 See table Voice/speaker selection temperature float No 0.7 0.0-2.0 Voice expression control max_new_audio_tokens integer No 2000 100-5000 Maximum audio length (generation budget) <p>Parameter Details</p> <ul> <li>text: The input text to be synthesized. Supports Unicode characters for local languages. Keep individual requests under 5,000 characters where possible.</li> <li>speaker_id: Selects the voice and language. Each ID corresponds to a specific language and gender (see table above).</li> <li>temperature: Controls voice expressiveness:</li> <li>0.0-0.3: More monotone, consistent pronunciation</li> <li>0.4-0.7: Balanced, natural expression (recommended)</li> <li>0.8-2.0: More expressive, variable intonation</li> <li>max_new_audio_tokens: Limits the maximum length of generated audio to control processing time and costs. Increase for longer outputs, but keep within sensible bounds to avoid timeouts.</li> </ul>"},{"location":"API/#response-format","title":"Response Format","text":"<p>Success Response (200 OK)</p> <pre><code>{\n  \"output\": {\n    \"audio_url\": \"https://storage.googleapis.com/sb-asr-audio-content-sb-gcp-project-01/tts/20251003082338_uuid.mp3?X-Goog-...\",\n    \"duration_seconds\": 4.2,\n    \"blob\": \"tts/20251003082338_uuid.mp3\",\n    \"sample_rate\": 16000,\n    \"format\": \"mp3\",\n    \"speaker_id\": 248,\n    \"processing_time\": 1.8\n  }\n}\n</code></pre> <p>Response Fields</p> Field Type Description output.audio_url string Signed URL to directly download the generated audio file output.duration_seconds float Length of the generated audio in seconds output.blob string Cloud storage blob path / identifier for the generated audio file output.sample_rate integer Audio sample rate in Hz output.format string Audio file format (e.g., \"mp3\") output.speaker_id integer Confirmation of speaker used output.processing_time float Server processing time in seconds <p>Important notes about the response</p> <ol> <li>Signed URL expiry: The <code>audio_url</code> is a time-limited signed URL that expires after 2 minutes (120 seconds). Download the file immediately or store the <code>blob</code> and generate a new signed URL server-side when needed.</li> <li>File format: The service produces MP3 audio by default; <code>format</code> indicates the actual container/codec returned.</li> <li>Sample rate: Typical sample rate is 16000 Hz unless otherwise noted.</li> <li>Blob naming: <code>blob</code> includes a timestamp and unique identifier for traceability (e.g., <code>tts/YYYYMMDD_HHMMSS_uuid.mp3</code>).</li> </ol>"},{"location":"API/#error-responses","title":"Error Responses","text":"<ul> <li>400 Bad Request \u2014 validation errors (missing <code>text</code>, invalid <code>speaker_id</code>, out-of-range <code>temperature</code>)</li> <li>503 Service Unavailable \u2014 worker timeout or connection issues (retryable with backoff)</li> <li>500 Internal Server Error \u2014 unexpected internal errors</li> </ul> <p>Error response examples</p> <ul> <li>400 Bad Request \u2014 Invalid or missing parameter (example: invalid speaker_id)</li> </ul> <pre><code>curl -i -X POST \"https://api.sunbird.ai/tasks/tts\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\":\"Webale\",\"speaker_id\":9999}'\n</code></pre> <p>HTTP/1.1 400 Bad Request <pre><code>{ \"detail\": \"Invalid speaker_id: 9999\" }\n</code></pre></p> <ul> <li>503 Service Unavailable \u2014 Worker timeout / transient failure</li> </ul> <pre><code>curl -i -X POST \"https://api.sunbird.ai/tasks/tts\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\":\"Long text that may trigger a timeout...\",\"speaker_id\":248}'\n</code></pre> <p>HTTP/1.1 503 Service Unavailable <pre><code>{ \"detail\": \"TTS worker timed out. Please retry later.\" }\n</code></pre></p> <ul> <li>500 Internal Server Error \u2014 Generation error</li> </ul> <pre><code>curl -i -X POST \"https://api.sunbird.ai/tasks/tts\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\":\"Webale nyo\",\"speaker_id\":248}'\n</code></pre> <p>HTTP/1.1 500 Internal Server Error <pre><code>{ \"detail\": \"Internal server error during audio generation\" }\n</code></pre></p>"},{"location":"API/#examples-curl-python-node","title":"Examples (cURL / Python / Node)","text":""},{"location":"API/#example-curl-request-immediate-download","title":"Example cURL (request + immediate download)","text":"<pre><code># 1) Request TTS and get signed URL\ncurl -s -X POST \"https://api.sunbird.ai/tasks/tts\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\":\"Webale nyo\",\"speaker_id\":248}' \\\n  -o tts_response.json\n\n# 2) Extract audio_url and download (shell example assumes jq is installed)\naudio_url=$(jq -r '.output.audio_url' tts_response.json)\ncurl -L \"$audio_url\" --output output.mp3\n</code></pre> <p>Client considerations</p> <ul> <li>Immediate download: Because the signed <code>audio_url</code> expires in 2 minutes, clients should download the audio immediately after receiving the TTS response.</li> <li>Server-side re-signing: If you need long-term access, call your server with the stored <code>blob</code> path and have the server generate a fresh signed URL using service credentials.</li> <li>Cost control: Cache <code>blob</code> for identical texts to avoid repeated synthesis costs.</li> </ul> <p>Python example \u2014 request TTS and download immediately</p> <pre><code>import requests\n\nAPI_URL = 'https://api.sunbird.ai/tasks/tts'\nHEADERS = {'Authorization': 'Bearer YOUR_API_KEY', 'Content-Type': 'application/json'}\n\npayload = {\"text\": \"Webale nyo\", \"speaker_id\": 248}\n\n# 1) Request TTS\nresp = requests.post(API_URL, headers=HEADERS, json=payload, timeout=60)\nresp.raise_for_status()\ndata = resp.json()\naudio_url = data['output']['audio_url']\n\n# 2) Download signed URL immediately\nwith requests.get(audio_url, stream=True, timeout=30) as r:\n    r.raise_for_status()\n    with open('output.mp3', 'wb') as f:\n        for chunk in r.iter_content(chunk_size=8192):\n            if chunk:\n                f.write(chunk)\n\nprint('Saved output.mp3')\n</code></pre> <p>Node.js example \u2014 request TTS and stream download</p> <pre><code>const axios = require('axios');\nconst fs = require('fs');\n\nasync function ttsAndDownload(apiKey, text, outFile='output.mp3'){\n  const url = 'https://api.sunbird.ai/tasks/tts';\n  const payload = { text, speaker_id: 248 };\n\n  // 1) Request TTS\n  const resp = await axios.post(url, payload, {\n    headers: { Authorization: `Bearer ${apiKey}` },\n    timeout: 60000,\n  });\n\n  const audioUrl = resp.data.output.audio_url;\n\n  // 2) Stream download\n  const audioResp = await axios.get(audioUrl, { responseType: 'stream', timeout: 30000 });\n  const writer = fs.createWriteStream(outFile);\n  audioResp.data.pipe(writer);\n\n  return new Promise((resolve, reject) =&gt; {\n    writer.on('finish', () =&gt; resolve(outFile));\n    writer.on('error', reject);\n  });\n}\n\n// Example usage:\n(async () =&gt; {\n  const apiKey = 'YOUR_API_KEY';\n  try {\n    const file = await ttsAndDownload(apiKey, 'Webale nyo');\n    console.log('Saved', file);\n  } catch (err) {\n    console.error('Error:', err.message);\n  }\n})();\n</code></pre> <p></p>"},{"location":"API/#error-handling-retries","title":"Error handling &amp; retries","text":"<ul> <li>400 \u2014 Client validation error. Check required fields and types.</li> <li>401 \u2014 Unauthorized. Check API key.</li> <li>422 \u2014 Unprocessable (STT failure to produce transcription).</li> <li>429 \u2014 Rate limited. Backoff with jitter.</li> <li>503 / 504 \u2014 Transient worker/network issues. Retry with exponential backoff.</li> </ul> <p>Suggested retry pattern (pseudo):</p> <pre><code>import time, random\n\ndef retry(fn, attempts=4):\n    for i in range(attempts):\n        try:\n            return fn()\n        except (TemporaryFailure,) as e:\n            delay = (2 ** i) + random.random()\n            time.sleep(delay)\n    raise\n</code></pre> <p></p>"},{"location":"API/#practical-tips-checklist-for-integration","title":"Practical tips &amp; checklist for integration","text":"<ul> <li>Validate inputs client-side (length, encoding, supported language codes).</li> <li>For STT, prefer uploading re-encoded audio (16kHz mono) to reduce failures.</li> <li>Always check and handle <code>was_audio_trimmed</code> for long recordings.</li> <li>Download TTS audio immediately or store <code>blob</code> to regenerate signed URLs server-side.</li> <li>Monitor API usage to avoid rate-limits; implement queuing for bursts.</li> </ul> <p>If you want, I can also:</p> <ul> <li>generate an HTML version of this single-file reference for hosting,</li> <li>add additional sample languages for <code>nllb_translate</code>, or</li> <li>include sample unit tests that exercise each endpoint (mocked).</li> </ul> <p>For support: info@sunbird.ai</p> <p>Copyright \u00a9 2025 Sunbird AI. All rights reserved.</p> <ul> <li>TTS: Cache generated audio by <code>blob</code> key. Re-generate signed URL server-side when needed.</li> <li>STT: For best results re-encode incoming audio to 16k/16-bit mono before upload if you control the client\u2014this reduces decoding failures.</li> <li>Translation: Keep inputs short (&lt; 512 tokens) for consistent quality.</li> </ul> <p></p>"},{"location":"API/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>415 Unsupported Media Type \u2014 verify client sets correct MIME type and extension.</li> <li>422 No transcription \u2014 try re-encoding the audio to mp3/wav.</li> <li>408/504 Timeouts \u2014 try splitting input or re-trying with backoff.</li> </ul> <p>If you need further help contact support@sunbird.ai or visit https://api.sunbird.ai/docs.</p> <p>Copyright \u00a9 2025 Sunbird AI. All rights reserved.</p>"},{"location":"API/index_v1/","title":"SUNBIRDAI API","text":"<p>Welcome to the Sunbird AI API documentation. The Sunbird AI API provides you access to Sunbird's language models. The currently supported models are: </p> <ul> <li> <p>Translation (English to Multiple): translate from English to Acholi, Ateso, Luganda, Lugbara and Runyankole.</p> </li> <li> <p>Translation (Multiple to English): translate from the 5 local language above to English.</p> </li> <li> <p>Speech To Text: Convert speech audio to text. Currently the supported languages are (English, Acholi, Ateso, Luganda, Lugbara and Runyankole)</p> </li> </ul>"},{"location":"API/index_v1/#login-and-signup","title":"Login and Signup","text":"<p>If you don't already have an account, visit the sunbird AI API page here. If You already have an account just proceed by logging in.</p>"},{"location":"API/index_v1/#logging-in-and-getting-an-access-token","title":"Logging in and getting an access token.","text":"<p>Authentication is done via a Bearer token. After you have created an account and you are logged in just visit the tokens page to get your access token. This is the <code>auth token</code> that is required when making calls to the sunbird AI api.</p> <p>To see the full api endpoint documentations, visit the api docs here.</p>"},{"location":"API/index_v1/#ai-tasks","title":"AI Tasks","text":"<ul> <li>Use the <code>/tasks/stt</code> endpoint for speech to text inference for one audio file.</li> <li>Use the <code>tasks/nllb-translate</code> endpoint for translation of text input with the NLLB model.</li> <li>Use the <code>/tasks/language_id</code> endpoint for auto language detection of text input.  This endpoint identifies the language of a given text. It supports a limited set  of local languages including Acholi (ach), Ateso (teo), English (eng),Luganda (lug),  Lugbara (lgg), and Runyankole (nyn).</li> <li>Use the <code>/tasks/summarise</code> endpoint for anonymised summarization of text input.  This endpoint does anonymised summarisation of a given text. The text languages supported for now are English (eng) and Luganda (lug).</li> </ul>"},{"location":"API/index_v1/#getting-started","title":"Getting started","text":"<p>The guides below demonstrate how to make endpoint calls to the api programmatically. Select your programming language of choice to see the example usage.</p>"},{"location":"API/index_v1/#sunbird-ai-api-tutorial","title":"Sunbird AI API Tutorial","text":"<p>This page describes how to use the Sunbird AI API and includes code samples in Python and Javascript.</p>"},{"location":"API/index_v1/#part-1-how-to-authenticate","title":"Part 1: How to authenticate","text":"<ol> <li>If you don't already have an account, create one at https://api.sunbird.ai/register and login.</li> <li>Go to the tokens page to get your access token which you'll use to authenticate</li> </ol> <p>Add an <code>.env</code> file in the same directory as the script and define <code>AUTH_TOKEN</code> in it:</p> <pre><code>AUTH_TOKEN=your_token_here\n</code></pre>"},{"location":"API/index_v1/#part-2-how-to-call-the-translation-endpoint","title":"Part 2: How to call the translation endpoint","text":"<p>Refer to the sample code below. Replace <code>{access_token}</code> with the token you received above.</p> PythonJavascript <p>Install the required dependencies:</p> <pre><code>pip install requests python-dotenv\n</code></pre> <pre><code>import os\nimport requests\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nurl = \"https://api.sunbird.ai/tasks/nllb_translate\"\naccess_token = os.getenv(\"AUTH_TOKEN\")\nheaders = {\n    \"accept\": \"application/json\",\n    \"Authorization\": f\"Bearer {access_token}\",\n    \"Content-Type\": \"application/json\",\n}\n\ndata = {\n    \"source_language\": \"lug\",\n    \"target_language\": \"eng\",\n    \"text\": \"Ekibiina ekiddukanya omuzannyo gw\u2019emisinde mu ggwanga ekya Uganda Athletics Federation kivuddeyo nekitegeeza nga lawundi esooka eyemisinde egisunsulamu abaddusi abanakiika mu mpaka ezenjawulo ebweru w\u2019eggwanga egya National Athletics Trials nga bwegisaziddwamu.\",\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(response.json())\n</code></pre> <p>Install the required dependencies:</p> <pre><code>npm install axios dotenv\n</code></pre> <pre><code>const axios = require('axios');\nrequire('dotenv').config();\n\nconst url = \"https://api.sunbird.ai/tasks/nllb_translate\";\nconst accessToken = process.env.AUTH_TOKEN;\n\nconst headers = {\n    \"accept\": \"application/json\",\n    \"Authorization\": `Bearer ${accessToken}`,\n    \"Content-Type\": \"application/json\"\n};\n\nconst data = {\n    source_language: \"lug\",\n    target_language: \"eng\",\n    text: \"Ekibiina ekiddukanya omuzannyo gw\u2019emisinde mu ggwanga ekya Uganda Athletics Federation kivuddeyo nekitegeeza nga lawundi esooka eyemisinde egisunsulamu abaddusi abanakiika mu mpaka ezenjawulo ebweru w\u2019eggwanga egya National Athletics Trials nga bwegisaziddwamu.\"\n};\n\naxios.post(url, data, { headers })\n    .then(response =&gt; {\n        console.log(response.data);\n    })\n    .catch(error =&gt; {\n        console.error(error.response ? error.response.data : error.message);\n    });\n</code></pre> <p>The dictionary below represents the language codes available now for the translate endpoint</p> PythonJavascript <pre><code>language_codes: {\n    \"English\": \"eng\",\n    \"Luganda\": \"lug\",\n    \"Runyankole\": \"nyn\",\n    \"Acholi\": \"ach\",\n    \"Ateso\": \"teo\",\n    \"Lugbara\": \"lgg\"\n}\n</code></pre> <pre><code>const languageCodes = {\n    English: \"eng\",\n    Luganda: \"lug\",\n    Runyankole: \"nyn\",\n    Acholi: \"ach\",\n    Ateso: \"teo\",\n    Lugbara: \"lgg\"\n};\n</code></pre>"},{"location":"API/index_v1/#part-3-how-to-call-the-speech-to-text-asr-endpoint","title":"Part 3: How to call the speech-to-text (ASR) endpoint","text":"<p>Refer to the sample code below. Replace <code>{access_token}</code> with the token you got from the <code>/auth/token</code> endpoint. And replace <code>/path/to/audio_file</code> with the path to the audio file you want to transcribe and <code>FILE_NAME</code> with audio filename. </p> PythonJavascript <pre><code>import os\nimport requests\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nurl = \"https://api.sunbird.ai/tasks/stt\"\naccess_token = os.getenv(\"AUTH_TOKEN\")\nheaders = {\n    \"accept\": \"application/json\",\n    \"Authorization\": f\"Bearer {access_token}\",\n}\n\nfiles = {\n    \"audio\": (\n        \"FILE_NAME\",\n        open(\"/path/to/audio_file\", \"rb\"),\n        \"audio/mpeg\",\n    ),\n}\ndata = {\n    \"language\": \"lug\",\n    \"adapter\": \"lug\",\n    \"whisper\": True,\n}\n\nresponse = requests.post(url, headers=headers, files=files, data=data)\n\nprint(response.json())\n</code></pre> <p>Install the required dependencies:</p> <pre><code>npm install axios form-data dotenv\n</code></pre> <pre><code>const axios = require('axios');\nconst FormData = require('form-data');\nconst fs = require('fs');\nrequire('dotenv').config();\n\nconst url = \"https://api.sunbird.ai/tasks/stt\";\nconst accessToken = process.env.AUTH_TOKEN;\n\nconst headers = {\n    \"accept\": \"application/json\",\n    \"Authorization\": `Bearer ${accessToken}`\n};\n\n// Create FormData\nconst formData = new FormData();\nformData.append(\"audio\", fs.createReadStream(\"/path/to/audio_file\"), {\n    filename: \"FILE_NAME\",\n    contentType: \"audio/mpeg\"\n});\nformData.append(\"language\", \"lug\");\nformData.append(\"adapter\", \"lug\");\nformData.append(\"whisper\", true);\n\n// Merge headers\nconst requestHeaders = {\n    ...headers,\n    ...formData.getHeaders()\n};\n\n// Send POST request\naxios.post(url, formData, { headers: requestHeaders })\n    .then(response =&gt; {\n        console.log(response.data);\n    })\n    .catch(error =&gt; {\n        console.error(error.response ? error.response.data : error.message);\n    });\n</code></pre>"},{"location":"API/index_v1/#part-4-how-to-call-the-summary-endpoint","title":"Part 4: How to call the summary endpoint","text":"PythonJavascript <pre><code>import os\n\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nurl = \"https://api.sunbird.ai/tasks/summarise\"\ntoken = os.getenv(\"AUTH_TOKEN\")\nheaders = {\n    \"accept\": \"application/json\",\n    \"Authorization\": f\"Bearer {token}\",\n    \"Content-Type\": \"application/json\",\n}\n\ntext = (\n    \"ndowooza yange ku baana bano abato abatalina tufuna funa ya uganda butuufu \"\n    \"eserbamby omwana oyo bingi bye yeegomba okuva mu buto bwe ate by'atasobola \"\n    \"kwetuusaako bw'afuna mu naawumuwaamagezi nti ekya mazima nze kaboyiaadeyaatei \"\n    \"ebintu kati bisusse mu uganda wano ebyegombebw'omwana by'atasobola kwetuusaako \"\n    \"ng'ate abazadde nabo bambi bwe beetunulamubamufuna mpola tebasobola kulabirira \"\n    \"mwana oyo bintu by'ayagala ekivaamu omwana akemererwan'ayagala omulenzi omulenzi \"\n    \"naye n'atoba okuatejukira ba mbi ba tannategeera bigambo bya kufuna famire fulani \"\n    \"bakola kyagenda layivu n'afuna embuto eky'amazima nze mbadde nsaba be kikwata \"\n    \"govenment sembera embeera etuyisa nnyo abaana ne tubafaako embeera gwe nyiga gwa \"\n    \"omuzadde olina olabirira maama we olina olabirira n'abato kati kano akasuumuseemu \"\n    \"bwe ka kubulako ne keegulirayooba kapalaobakakioba tokyabisobola ne keyiiyabatuyambe \"\n    \"buduufuembeera bagikyusa mu tulemye\"\n)\n\ndata = {\"text\": text}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(response.json())\n</code></pre> <pre><code>const axios = require('axios');\nrequire('dotenv').config();\n\nconst url = \"https://api.sunbird.ai/tasks/summarise\";\nconst token = process.env.AUTH_TOKEN;\n\nconst headers = {\n    \"accept\": \"application/json\",\n    \"Authorization\": `Bearer ${token}`,\n    \"Content-Type\": \"application/json\"\n};\n\nconst text = \n    \"ndowooza yange ku baana bano abato abatalina tufuna funa ya uganda butuufu \" +\n    \"eserbamby omwana oyo bingi bye yeegomba okuva mu buto bwe ate by'atasobola \" +\n    \"kwetuusaako bw'afuna mu naawumuwaamagezi nti ekya mazima nze kaboyiaadeyaatei \" +\n    \"ebintu kati bisusse mu uganda wano ebyegombebw'omwana by'atasobola kwetuusaako \" +\n    \"ng'ate abazadde nabo bambi bwe beetunulamubamufuna mpola tebasobola kulabirira \" +\n    \"mwana oyo bintu by'ayagala ekivaamu omwana akemererwan'ayagala omulenzi omulenzi \" +\n    \"naye n'atoba okuatejukira ba mbi ba tannategeera bigambo bya kufuna famire fulani \" +\n    \"bakola kyagenda layivu n'afuna embuto eky'amazima nze mbadde nsaba be kikwata \" +\n    \"govenment sembera embeera etuyisa nnyo abaana ne tubafaako embeera gwe nyiga gwa \" +\n    \"omuzadde olina olabirira maama we olina olabirira n'abato kati kano akasuumuseemu \" +\n    \"bwe ka kubulako ne keegulirayooba kapalaobakakioba tokyabisobola ne keyiiyabatuyambe \" +\n    \"buduufuembeera bagikyusa mu tulemye\";\n\nconst data = { text };\n\naxios.post(url, data, { headers })\n    .then(response =&gt; {\n        console.log(response.data);\n    })\n    .catch(error =&gt; {\n        console.error(error.response ? error.response.data : error.message);\n    });\n</code></pre>"},{"location":"API/index_v1/#part-5-how-to-call-the-language_id-endpoint","title":"Part 5: How to call the language_id endpoint","text":"PythonJavascript <pre><code>import os\n\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nurl = \"https://api.sunbird.ai/tasks/language_id\"\ntoken = os.getenv(\"AUTH_TOKEN\")\nheaders = {\n    \"accept\": \"application/json\",\n    \"Authorization\": f\"Bearer {token}\",\n    \"Content-Type\": \"application/json\",\n}\n\ntext = \"ndowooza yange ku baana bano abato abatalina tufuna funa ya uganda butuufu\"\n\ndata = {\"text\": text}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(response.json())\n</code></pre> <pre><code>const axios = require('axios');\nrequire('dotenv').config();\n\nconst url = \"https://api.sunbird.ai/tasks/language_id\";\nconst token = process.env.AUTH_TOKEN;\n\nconst headers = {\n    \"accept\": \"application/json\",\n    \"Authorization\": `Bearer ${token}`,\n    \"Content-Type\": \"application/json\"\n};\n\nconst text = \"ndowooza yange ku baana bano abato abatalina tufuna funa ya uganda butuufu\";\n\nconst data = { text };\n\naxios.post(url, data, { headers })\n    .then(response =&gt; {\n        console.log(response.data);\n    })\n    .catch(error =&gt; {\n        console.error(error.response ? error.response.data : error.message);\n    });\n</code></pre> <p>You can refer to the docs for more info about the endpoints.</p>"},{"location":"API/index_v1/#feedback-and-questions","title":"Feedback and Questions.","text":"<p>Don't hesitate to leave us any feedback or questions you have by opening an issue in this repo.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"sunflower/inference/","title":"Sunbird Sunflower Quantized Model Usage","text":""},{"location":"sunflower/inference/#installation","title":"Installation","text":"<pre><code>## Installation\n\nInstall the required Python packages:\n\n```bash\n!pip install torch transformers accelerate bitsandbytes\n</code></pre> <ul> <li>torch: PyTorch library for model computation.</li> <li>transformers: Hugging Face Transformers library for model handling.</li> <li>accelerate: Optimizes model loading across devices.</li> <li>bitsandbytes: Enables efficient 4-bit and 8-bit model quantization.</li> </ul>"},{"location":"sunflower/inference/#import-the-necessary-libraries-for-loading-and-running-the-model","title":"Import the necessary libraries for loading and running the model.","text":"<pre><code>import torch\nimport transformers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nimport os\nfrom getpass import getpass\n</code></pre>"},{"location":"sunflower/inference/#3-authentication","title":"3. Authentication","text":"<pre><code># Set your Hugging Face token to access models\nos.environ[\"HF_TOKEN\"] = getpass(\"HF_TOKEN: \")\n</code></pre>"},{"location":"sunflower/inference/#4-load-tokenizer-and-model","title":"4. Load Tokenizer and Model","text":"<p>Replace <code>&lt;MODEL_NAME&gt;</code> with the specific model you want to use, e.g., <code>Sunbird/Sunflower-14B-4bit-nf4-bnb</code> or <code>Sunbird/Sunflower-14B-8bit-bnb</code>.</p> <pre><code># Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"&lt;MODEL_NAME&gt;\")\n\n# Configure quantization (choose one)\nquantization_config = BitsAndBytesConfig(\n    # For 4-bit models\n    load_in_4bit=True,                  \n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=True,\n\n    # For 8-bit models, comment out above and use:\n    # load_in_8bit=True,\n    # llm_int8_threshold=6.0\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"&lt;MODEL_NAME&gt;\",\n    quantization_config=quantization_config,\n    device_map=\"auto\",       # or \"cuda:0\" for single GPU\n    trust_remote_code=True\n)\n</code></pre> <p>Note: Only change <code>&lt;MODEL_NAME&gt;</code> and adjust quantization parameters. Everything else remains the same for any model.</p>"},{"location":"sunflower/inference/#5-prepare-chat-prompt","title":"5. Prepare Chat Prompt","text":"<pre><code># Define system message (assistant role)\nSYSTEM_MESSAGE = \"\"\"You are Sunflower, a multilingual assistant for Ugandan languages made by Sunbird AI. You specialise in accurate translations, explanations, summaries, and other cross-lingual tasks.\"\"\"\n\n# Example user prompt\nprompt_text = \"Sunbird AI is a non-profit research organization in Kampala, Uganda. We build and deploy practical applied machine learning systems for African contexts.\"\nuser_prompt = f\"Translate to Luganda: {prompt_text}\"\n\n# Structure messages\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n    {\"role\": \"user\", \"content\": user_prompt}\n]\n\n# Prepare model input\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\ninputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n</code></pre>"},{"location":"sunflower/inference/#6-configure-text-generation","title":"6. Configure Text Generation","text":"<pre><code># Streamer outputs tokens in real-time\ntext_streamer = transformers.TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n\n# Generation parameters\nnum_beams = 5  # Adjust for higher quality\ngeneration_config = {\n    \"max_new_tokens\": 512,       # Maximum tokens to generate\n    \"temperature\": 0.3,          # Lower = more deterministic\n    \"do_sample\": True,           # Enable sampling\n    \"no_repeat_ngram_size\": 5,   # Avoid repeated phrases\n    \"num_beams\": num_beams,      # Beam search width\n}\n</code></pre> <ul> <li>Beam search improves output quality but slows generation.</li> <li><code>TextStreamer</code> works only when <code>num_beams == 1</code>.</li> </ul>"},{"location":"sunflower/inference/#7-generate-output","title":"7. Generate Output","text":"<pre><code># Generate model output\noutputs = model.generate(\n    **inputs,\n    **generation_config,\n    streamer=text_streamer if num_beams == 1 else None,\n)\n\n# Decode if using multi-beam search\nif num_beams &gt; 1:\n    response = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True)\n    print(response)\n</code></pre> <ul> <li>Single-beam: real-time streaming.</li> <li>Multi-beam: output decoded after generation.</li> </ul>"},{"location":"sunflower/inference/#8-notes-for-future-models","title":"8. Notes for Future Models","text":"<ul> <li>The workflow is identical for any Sunflower model.</li> <li> <p>Only model-specific changes:</p> </li> <li> <p><code>&lt;MODEL_NAME&gt;</code> in <code>from_pretrained</code>.</p> </li> <li>Quantization parameters (<code>load_in_4bit</code> / <code>load_in_8bit</code>).</li> <li>Everything else (chat template, streaming, generation) stays the same.</li> </ul>"},{"location":"sunflower/overview/","title":"\ud83c\udf3b Sunflower Quantized Inference","text":""},{"location":"sunflower/overview/#overview","title":"Overview","text":"<p>The Sunflower models are available in 14B and 32B sizes and support 8-bit and 4-bit quantized inference for efficient performance on GPUs with limited memory.  </p> <p>Quantization reduces memory requirements while keeping inference quality high, enabling large models to run on consumer-grade GPUs or hardware with limited VRAM.</p> Feature 8-bit 4-bit Memory Usage Higher (~16GB for 14B) Lower (~10GB for 14B) Speed \u26a1 Fast \u26a1\u26a1 Faster Accuracy Very Good Slightly Lower VRAM Efficiency Moderate High <p>\u26a0\ufe0f Important: Do not set both 8-bit and 4-bit modes at the same time.</p>"},{"location":"sunflower/overview/#sunflower-14b-models","title":"\ud83c\udf3b Sunflower 14B Models","text":"<ul> <li>14B 8-bit: Balanced memory and accuracy, suitable for most GPUs.  </li> <li>14B 4-bit: Optimized for memory-limited GPUs and faster inference, with minimal accuracy trade-off.  </li> </ul>"},{"location":"sunflower/overview/#sunflower-32b-models","title":"\ud83c\udf3b Sunflower 32B Models","text":"<ul> <li>32B 8-bit: High accuracy, requires more GPU memory.  </li> <li>32B 4-bit: Reduced memory usage, faster inference, slightly lower accuracy.  </li> </ul> <p>The usage process is identical for 14B and 32B; only model size and quantization type differ.</p>"},{"location":"sunflower/overview/#tips-best-practices","title":"Tips &amp; Best Practices","text":"<ul> <li>Use 4-bit models when GPU memory is limited or faster inference is needed.  </li> <li>8-bit models offer a good balance of memory usage and accuracy.  </li> <li>Always choose either 8-bit or 4-bit for a model.  </li> <li>For large inputs or batch processing, monitor GPU memory to avoid out-of-memory errors.  </li> <li>Adjust inference parameters (like sequence length or tokens) for optimal performance based on your hardware.</li> </ul>"},{"location":"sunflower/quantization/","title":"Converting Sunflower LoRA Fine-tuned Models to GGUF Quantizations","text":"<p>In this guide, provide a tutorial for converting LoRA fine-tuned Sunflower models to GGUF format with multiple quantization levels, including experimental ultra-low bit quantizations.</p>"},{"location":"sunflower/quantization/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prerequisites</li> <li>Environment Setup</li> <li>Model Preparation</li> <li>LoRA Merging</li> <li>GGUF Conversion</li> <li>Quantization Process</li> <li>Experimental Quantizations</li> <li>Quality Testing</li> <li>Ollama Integration</li> <li>Distribution</li> <li>Troubleshooting</li> </ul>"},{"location":"sunflower/quantization/#prerequisites","title":"Prerequisites","text":""},{"location":"sunflower/quantization/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>RAM: Minimum 32GB (64GB recommended for 14B+ models)</li> <li>Storage: 200GB+ free space for intermediate files</li> <li>GPU: Optional but recommended for faster processing</li> </ul>"},{"location":"sunflower/quantization/#software-requirements","title":"Software Requirements","text":"<ul> <li>Linux/macOS (WSL2 for Windows)</li> <li>Python 3.9+</li> <li>Git and Git LFS</li> <li>CUDA toolkit (optional, for GPU acceleration)</li> </ul>"},{"location":"sunflower/quantization/#environment-setup","title":"Environment Setup","text":""},{"location":"sunflower/quantization/#1-install-dependencies","title":"1. Install Dependencies","text":"<pre><code># Install Python packages\npip install torch transformers accelerate\npip install sentencepiece protobuf\npip install peft huggingface_hub\npip install -U huggingface_hub\n\n# Install system dependencies (Ubuntu/Debian)\nsudo apt update\nsudo apt install -y cmake build-essential git git-lfs\n</code></pre>"},{"location":"sunflower/quantization/#2-clone-and-build-llamacpp","title":"2. Clone and Build llama.cpp","text":"<pre><code># Clone llama.cpp\ngit clone https://github.com/ggml-org/llama.cpp\ncd llama.cpp\n\n# Build with CUDA support (if available)\ncmake -B build -DGGML_CUDA=ON\ncmake --build build --config Release\n\n# OR build for CPU only\nmake clean &amp;&amp; make\n\n# Create working directories\nmkdir -p models gguf_outputs\ncd ..\n</code></pre>"},{"location":"sunflower/quantization/#model-preparation","title":"Model Preparation","text":""},{"location":"sunflower/quantization/#1-download-models","title":"1. Download Models","text":"<pre><code># Download base model\nhuggingface-cli download jq/sunflower-14b-bs64-lr1e-4 --local-dir models/base_model\n\n# Download LoRA adapter\nhuggingface-cli download jq/qwen3-14b-sunflower-20250915 --local-dir models/lora_model\n\n# Download merged model (LoRA already merged)\nhuggingface-cli download Sunbird/qwen3-14b-sunflower-merged --local-dir models/merged_model\n</code></pre> <p>Example:</p> <pre><code>huggingface-cli download jq/sunflower-14b-bs64-lr1e-4 --local-dir models/base_model\nhuggingface-cli download jq/qwen3-14b-sunflower-20250915 --local-dir models/lora_model\n\n# Download merged model (LoRA already merged)\nhuggingface-cli download Sunbird/qwen3-14b-sunflower-merged --local-dir models/merged_model\n</code></pre>"},{"location":"sunflower/quantization/#lora-merging","title":"LoRA Merging","text":"<ol> <li>Create Merge Script (Skip if Using Pre-merged Model)    Note: If you downloaded Sunbird/qwen3-14b-sunflower-merged, skip this section and go directly to GGUF Conversion.    Create merge_lora.py only if using separate base model and LoRA adapter:</li> </ol> <p>Create <code>merge_lora.py</code>:</p> <pre><code>#!/usr/bin/env python3\n\"\"\"\nMerge LoRA adapter with base model\n\"\"\"\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\nimport torch\nimport sys\n\ndef merge_lora_model():\n    print(\"Loading base model...\")\n    base_model = AutoModelForCausalLM.from_pretrained(\n        \"models/base_model\",\n        torch_dtype=torch.bfloat16,  # Use bfloat16 for better precision\n        device_map=\"auto\",\n        low_cpu_mem_usage=True\n    )\n\n    print(\"Loading LoRA adapter...\")\n    model = PeftModel.from_pretrained(base_model, \"models/lora_model\")\n\n    print(\"Merging LoRA with base model...\")\n    merged_model = model.merge_and_unload()\n\n    print(\"Saving merged model...\")\n    merged_model.save_pretrained(\"models/merged_model\", safe_serialization=True)\n\n    # Save tokenizer\n    print(\"Saving tokenizer...\")\n    tokenizer = AutoTokenizer.from_pretrained(\"models/base_model\")\n    tokenizer.save_pretrained(\"models/merged_model\")\n\n    print(\"Merge completed successfully!\")\n    print(\"Merged model saved to: models/merged_model\")\n\nif __name__ == \"__main__\":\n    merge_lora_model()\n</code></pre>"},{"location":"sunflower/quantization/#2-run-merge-process","title":"2. Run Merge Process","text":"<pre><code>python merge_lora.py\n</code></pre> <p>Expected output: Merged model saved to <code>models/merged_model</code></p>"},{"location":"sunflower/quantization/#gguf-conversion","title":"GGUF Conversion","text":""},{"location":"sunflower/quantization/#1-convert-to-f16-gguf","title":"1. Convert to F16 GGUF","text":"<pre><code># Convert merged model to F16 GGUF (preserves quality for quantization)\npython3 llama.cpp/convert_hf_to_gguf.py models/merged_model \\\n  --outfile gguf_outputs/model-merged-f16.gguf \\\n  --outtype f16\n\n# Verify file size (should be ~2GB per billion parameters for F16)\nls -lh gguf_outputs/model-merged-f16.gguf\n</code></pre> <p>Expected size: ~28GB for 14B model in F16</p>"},{"location":"sunflower/quantization/#quantization-process","title":"Quantization Process","text":""},{"location":"sunflower/quantization/#1-generate-importance-matrix","title":"1. Generate Importance Matrix","text":"<p>The importance matrix (imatrix) significantly improves quantization quality by identifying which weights are most critical to model performance.</p> <pre><code># Download calibration dataset\nwget [https://raw.githubusercontent.com/ggerganov/llama.cpp/master/examples/perplexity/wiki.test.raw](https://huggingface.co/nisten/llama3-8b-instruct-32k-gguf/raw/main/wiki.test.raw)\n\n# Generate importance matrix (this takes 30-60 minutes)\n./llama.cpp/build/bin/llama-imatrix \\\n  -m gguf_outputs/model-merged-f16.gguf \\\n  -f wiki.test.raw \\\n  --chunk 512 \\\n  -o gguf_outputs/model-imatrix.dat \\\n  -ngl 32 \\\n  --verbose\n</code></pre> <p>Note: Adjust <code>-ngl</code> based on your GPU memory (0 for CPU-only)</p> <p>Note: Understanding Importance Matrix (imatrix)</p> <p>The importance matrix is a calibration technique that identifies which model weights contribute most significantly to output quality. During quantization, weights deemed \"important\" by the matrix receive higher precision allocation, while less critical weights can be more aggressively compressed. This selective approach significantly improves quantized model quality compared to uniform quantization.</p> <p>The imatrix is generated by running representative text through the model and measuring activation patterns. While general text datasets (like WikiText) work well for most models, using domain-specific calibration data (e.g., translation examples for the Sunflower model) can provide marginal quality improvements. The process adds 30-60 minutes to quantization time but is highly recommended for production models, especially when using aggressive quantizations like Q4_K_M and below.</p>"},{"location":"sunflower/quantization/#2-standard-quantizations","title":"2. Standard Quantizations","text":"<p>Create quantized models with different quality/size trade-offs:</p> <pre><code># Q8_0: Near-lossless quality (~15GB for 14B model)\n./llama.cpp/build/bin/llama-quantize \\\n  --imatrix gguf_outputs/model-imatrix.dat \\\n  gguf_outputs/model-merged-f16.gguf \\\n  gguf_outputs/model-q8_0.gguf \\\n  Q8_0\n\n# Q6_K: High quality (~12GB for 14B model)\n./llama.cpp/build/bin/llama-quantize \\\n  --imatrix gguf_outputs/model-imatrix.dat \\\n  gguf_outputs/model-merged-f16.gguf \\\n  gguf_outputs/model-q6_k.gguf \\\n  Q6_K\n\n# Q5_K_M: Balanced quality/size (~10GB for 14B model)\n./llama.cpp/build/bin/llama-quantize \\\n  --imatrix gguf_outputs/model-imatrix.dat \\\n  gguf_outputs/model-merged-f16.gguf \\\n  gguf_outputs/model-q5_k_m.gguf \\\n  Q5_K_M\n\n# Q4_K_M: Recommended for most users (~8GB for 14B model)\n./llama.cpp/build/bin/llama-quantize \\\n  --imatrix gguf_outputs/model-imatrix.dat \\\n  gguf_outputs/model-merged-f16.gguf \\\n  gguf_outputs/model-q4_k_m.gguf \\\n  Q4_K_M\n</code></pre>"},{"location":"sunflower/quantization/#3-quantization-options-reference","title":"3. Quantization Options Reference","text":"Quantization Bits per Weight Quality Use Case Q8_0 ~8.0 Highest Production, quality critical Q6_K ~6.6 High Production, balanced Q5_K_M ~5.5 Good Most users Q4_K_M ~4.3 Acceptable Resource constrained"},{"location":"sunflower/quantization/#experimental-quantizations","title":"Experimental Quantizations","text":"<p>Warning: These quantizations achieve extreme compression but may significantly impact model quality.</p>"},{"location":"sunflower/quantization/#ultra-low-bit-quantizations","title":"Ultra-Low Bit Quantizations","text":"<pre><code># IQ2_XXS: Extreme compression (~4GB for 14B model)\n./llama.cpp/build/bin/llama-quantize \\\n  --imatrix gguf_outputs/model-imatrix.dat \\\n  gguf_outputs/model-merged-f16.gguf \\\n  gguf_outputs/model-iq2_xxs.gguf \\\n  IQ2_XXS\n\n# TQ1_0: Ternary quantization (~3.7GB for 14B model)\n./llama.cpp/build/bin/llama-quantize \\\n  --imatrix gguf_outputs/model-imatrix.dat \\\n  gguf_outputs/model-merged-f16.gguf \\\n  gguf_outputs/model-tq1_0.gguf \\\n  TQ1_0\n\n# IQ1_S: Maximum compression (~3.4GB for 14B model)\n./llama.cpp/build/bin/llama-quantize \\\n  --imatrix gguf_outputs/model-imatrix.dat \\\n  gguf_outputs/model-merged-f16.gguf \\\n  gguf_outputs/model-iq1_s.gguf \\\n  IQ1_S\n</code></pre>"},{"location":"sunflower/quantization/#experimental-quantization-reference","title":"Experimental Quantization Reference","text":"Quantization Bits per Weight Compression Warning Level IQ2_XXS 2.06 85% smaller Moderate quality loss TQ1_0 1.69 87% smaller High quality loss IQ1_S 1.56 88% smaller Severe quality loss"},{"location":"sunflower/quantization/#quality-testing","title":"Quality Testing","text":""},{"location":"sunflower/quantization/#1-quick-functionality-test","title":"1. Quick Functionality Test","text":"<pre><code># Test standard quantization\n./llama.cpp/build/bin/llama-cli \\\n  -m gguf_outputs/model-q4_k_m.gguf \\\n  -p \"Your test prompt here\" \\\n  -n 100 \\\n  --verbose\n\n# Test experimental quantization\n./llama.cpp/build/bin/llama-cli \\\n  -m gguf_outputs/model-iq1_s.gguf \\\n  -p \"Your test prompt here\" \\\n  -n 100 \\\n  --verbose\n</code></pre>"},{"location":"sunflower/quantization/#2-perplexity-evaluation","title":"2. Perplexity Evaluation","text":"<pre><code># Download evaluation dataset\nwget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\nunzip wikitext-2-raw-v1.zip\n\n# Test perplexity (lower is better)\n./llama.cpp/build/bin/llama-perplexity \\\n  -m gguf_outputs/model-q4_k_m.gguf \\\n  -f wikitext-2-raw/wiki.test.raw \\\n  -ngl 32\n</code></pre>"},{"location":"sunflower/quantization/#3-size-verification","title":"3. Size Verification","text":"<pre><code># Check all quantization sizes\nls -lh gguf_outputs/*.gguf\n</code></pre> <p>Expected output (14B model):</p> <pre><code>28G  model-merged-f16.gguf\n15G  model-q8_0.gguf\n12G  model-q6_k.gguf\n10G  model-q5_k_m.gguf\n8.4G model-q4_k_m.gguf\n4.1G model-iq2_xxs.gguf\n3.7G model-tq1_0.gguf\n3.4G model-iq1_s.gguf\n</code></pre>"},{"location":"sunflower/quantization/#ollama-integration","title":"Ollama Integration","text":"<p>Ollama provides an easy way to run your quantized models locally with a simple API interface.</p>"},{"location":"sunflower/quantization/#installation-and-setup","title":"Installation and Setup","text":"<pre><code># Install Ollama (Linux/macOS)\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Or download from https://ollama.ai for Windows\n\n# Start Ollama service (runs in background)\nollama serve\n</code></pre>"},{"location":"sunflower/quantization/#creating-modelfiles-for-different-quantizations","title":"Creating Modelfiles for Different Quantizations","text":"<p>Q4_K_M (Recommended) - Modelfile:</p> <pre><code>cat &gt; Modelfile.q4 &lt;&lt; 'EOF'\nFROM ./gguf_outputs/model-q4_k_m.gguf\n\n# System prompt for your specific use case\nSYSTEM \"\"\"You are a linguist and translator specializing in Ugandan languages, made by Sunbird AI.\"\"\"\n\n# Chat template (adjust for your base model architecture)\nTEMPLATE \"\"\"&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n{{ .Response }}&lt;|im_end|&gt;\"\"\"\n\n# Stop tokens\nPARAMETER stop \"&lt;|im_start|&gt;\"\nPARAMETER stop \"&lt;|im_end|&gt;\"\n\n# Generation parameters\nPARAMETER temperature 0.3\nPARAMETER top_p 0.95\nPARAMETER top_k 40\nPARAMETER repeat_penalty 1.1\nPARAMETER num_ctx 4096\nPARAMETER num_predict 500\nEOF\n</code></pre> <p>Experimental IQ1_S - Modelfile:</p> <pre><code>cat &gt; Modelfile.iq1s &lt;&lt; 'EOF'\nFROM ./gguf_outputs/model-iq1_s.gguf\n\nSYSTEM \"\"\"You are a translator for Ugandan languages. Note: This is an experimental ultra-compressed model - quality may be limited.\"\"\"\n\n# Same template and parameters as above\nTEMPLATE \"\"\"&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n{{ .Response }}&lt;|im_end|&gt;\"\"\"\n\nPARAMETER stop \"&lt;|im_start|&gt;\"\nPARAMETER stop \"&lt;|im_end|&gt;\"\nPARAMETER temperature 0.3\nPARAMETER top_p 0.95\nPARAMETER num_ctx 2048  # Smaller context for experimental model\nEOF\n</code></pre>"},{"location":"sunflower/quantization/#importing-models-to-ollama","title":"Importing Models to Ollama","text":"<pre><code># Import Q4_K_M model (recommended)\nollama create sunflower-14b:q4 -f Modelfile.q4\n\n# Import experimental IQ1_S model\nollama create sunflower-14b:iq1s -f Modelfile.iq1s\n\n# Import other quantizations\nollama create sunflower-14b:q5 -f Modelfile.q5\nollama create sunflower-14b:q6 -f Modelfile.q6\n\n# Verify models are imported\nollama list\n</code></pre> <p>Expected output:</p> <pre><code>NAME                    ID              SIZE    MODIFIED\nsunflower-14b:q4        abc123def       8.4GB   2 minutes ago\nsunflower-14b:iq1s      def456ghi       3.4GB   1 minute ago\n</code></pre>"},{"location":"sunflower/quantization/#using-ollama-models","title":"Using Ollama Models","text":"<p>Interactive Chat:</p> <pre><code># Start interactive session with Q4 model\nollama run sunflower-14b:q4\n\n# Example conversation:\n# &gt;&gt;&gt; Translate to Luganda: Hello, how are you today?\n# &gt;&gt;&gt; Give a dictionary definition of the Samia term \"ovulwaye\" in English\n# &gt;&gt;&gt; /bye (to exit)\n\n# Start with experimental model\nollama run sunflower-14b:iq1s\n</code></pre> <p>Single Prompt Inference:</p> <pre><code># Quick translation with Q4 model\nollama run sunflower-14b:q4 \"Translate to Luganda: People in villages rarely accept new technologies.\"\n\n# Test experimental model\nollama run sunflower-14b:iq1s \"Translate to Luganda: Good morning\"\n\n# Dictionary definition\nollama run sunflower-14b:q4 'Give a dictionary definition of the Samia term \"ovulwaye\" in English'\n</code></pre>"},{"location":"sunflower/quantization/#ollama-api-usage","title":"Ollama API Usage","text":"<p>Start API Server:</p> <pre><code># Ollama automatically serves API on http://localhost:11434\n# Test API endpoint\ncurl http://localhost:11434/api/version\n</code></pre> <p>Python API Client:</p> <pre><code>import requests\nimport json\n\ndef translate_with_ollama(text, target_lang=\"Luganda\", model=\"sunflower-14b:q4\"):\n    url = \"http://localhost:11434/api/generate\"\n\n    payload = {\n        \"model\": model,\n        \"prompt\": f\"Translate to {target_lang}: {text}\",\n        \"stream\": False\n    }\n\n    response = requests.post(url, json=payload)\n    return response.json()[\"response\"]\n\n# Test translation\nresult = translate_with_ollama(\"Hello, how are you?\")\nprint(result)\n\n# Test experimental model\nresult_experimental = translate_with_ollama(\n    \"Good morning\",\n    model=\"sunflower-14b:iq1s\"\n)\nprint(\"Experimental model:\", result_experimental)\n</code></pre> <p>curl API Examples:</p> <pre><code># Basic translation\ncurl -X POST http://localhost:11434/api/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"sunflower-14b:q4\",\n    \"prompt\": \"Translate to Luganda: How are you today?\",\n    \"stream\": false\n  }'\n\n# Streaming response\ncurl -X POST http://localhost:11434/api/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"sunflower-14b:q4\",\n    \"prompt\": \"Translate to Luganda: People in villages rarely accept new technologies.\",\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"sunflower/quantization/#model-management","title":"Model Management","text":"<pre><code># List all models\nollama list\n\n# Show model details\nollama show sunflower-14b:q4\n\n# Remove a model\nollama rm sunflower-14b:iq1s\n\n# Copy model with new name\nollama cp sunflower-14b:q4 sunflower-translator\n\n# Pull/push to Ollama registry (if you publish there)\n# ollama push sunflower-14b:q4\n</code></pre>"},{"location":"sunflower/quantization/#performance-comparison-script","title":"Performance Comparison Script","text":"<p>Create <code>test_models.py</code>:</p> <pre><code>import time\nimport requests\n\nmodels = [\"sunflower-14b:q4\", \"sunflower-14b:iq1s\"]\ntest_prompt = \"Translate to Luganda: Hello, how are you today?\"\n\ndef test_model(model_name, prompt):\n    start_time = time.time()\n\n    response = requests.post(\"http://localhost:11434/api/generate\", json={\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"stream\": False\n    })\n\n    end_time = time.time()\n    result = response.json()\n\n    return {\n        \"model\": model_name,\n        \"response\": result[\"response\"],\n        \"time\": end_time - start_time,\n        \"tokens\": len(result[\"response\"].split())\n    }\n\n# Test all models\nfor model in models:\n    result = test_model(model, test_prompt)\n    print(f\"Model: {result['model']}\")\n    print(f\"Response: {result['response']}\")\n    print(f\"Time: {result['time']:.2f}s\")\n    print(f\"Tokens: {result['tokens']}\")\n    print(\"-\" * 50)\n</code></pre>"},{"location":"sunflower/quantization/#production-deployment","title":"Production Deployment","text":"<pre><code># Create production Modelfile with optimized settings\ncat &gt; Modelfile.production &lt;&lt; 'EOF'\nFROM ./gguf_outputs/model-q4_k_m.gguf\n\nSYSTEM \"\"\"You are a professional translator for Ugandan languages, made by Sunbird AI. Provide accurate, contextually appropriate translations.\"\"\"\n\nTEMPLATE \"\"\"&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n{{ .Response }}&lt;|im_end|&gt;\"\"\"\n\nPARAMETER stop \"&lt;|im_start|&gt;\"\nPARAMETER stop \"&lt;|im_end|&gt;\"\n\n# Production-optimized parameters\nPARAMETER temperature 0.1        # Lower for consistency\nPARAMETER top_p 0.9             # Slightly more focused\nPARAMETER repeat_penalty 1.05   # Minimal repetition penalty\nPARAMETER num_ctx 4096          # Full context\nPARAMETER num_predict 200       # Reasonable response length\nEOF\n\n# Create production model\nollama create sunflower-translator:production -f Modelfile.production\n</code></pre>"},{"location":"sunflower/quantization/#distribution","title":"Distribution","text":""},{"location":"sunflower/quantization/#1-hugging-face-upload","title":"1. Hugging Face Upload","text":"<p>Create upload script <code>upload_models.py</code>:</p> <pre><code>#!/usr/bin/env python3\nfrom huggingface_hub import HfApi, login, create_repo\n\ndef upload_gguf_models():\n    repo_id = \"your-org/your-model-GGUF\"  # Update this\n    local_folder = \"gguf_outputs\"\n\n    # Login and create repository\n    login()\n    api = HfApi()\n    create_repo(repo_id, exist_ok=True, repo_type=\"model\")\n\n    # Upload all files\n    api.upload_folder(\n        folder_path=local_folder,\n        repo_id=repo_id,\n        allow_patterns=[\"*.gguf\", \"*.dat\"],\n        commit_message=\"Add GGUF quantized models\"\n    )\n    print(f\"Upload complete: https://huggingface.co/{repo_id}\")\n\nif __name__ == \"__main__\":\n    upload_gguf_models()\n</code></pre> <pre><code># Login to Hugging Face\nhuggingface-cli login\n\n# Run upload\npython upload_models.py\n</code></pre>"},{"location":"sunflower/quantization/#2-ollama-integration-complete-guide","title":"2. Ollama Integration (Complete Guide)","text":""},{"location":"sunflower/quantization/#installation-and-setup_1","title":"Installation and Setup","text":"<pre><code># Install Ollama (Linux/macOS)\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# Or download from https://ollama.ai for Windows\n\n# Start Ollama service (runs in background)\nollama serve\n</code></pre>"},{"location":"sunflower/quantization/#creating-modelfiles-for-different-quantizations_1","title":"Creating Modelfiles for Different Quantizations","text":"<p>Q4_K_M (Recommended) - Modelfile:</p> <pre><code>cat &gt; Modelfile.q4 &lt;&lt; 'EOF'\nFROM ./gguf_outputs/model-q4_k_m.gguf\n\n# System prompt for your specific use case\nSYSTEM \"\"\"You are a linguist and translator specializing in Ugandan languages, made by Sunbird AI.\"\"\"\n\n# Chat template (adjust for your base model architecture)\nTEMPLATE \"\"\"&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n{{ .Response }}&lt;|im_end|&gt;\"\"\"\n\n# Stop tokens\nPARAMETER stop \"&lt;|im_start|&gt;\"\nPARAMETER stop \"&lt;|im_end|&gt;\"\n\n# Generation parameters\nPARAMETER temperature 0.3\nPARAMETER top_p 0.95\nPARAMETER top_k 40\nPARAMETER repeat_penalty 1.1\nPARAMETER num_ctx 4096\nPARAMETER num_predict 500\nEOF\n</code></pre> <p>Experimental IQ1_S - Modelfile:</p> <pre><code>cat &gt; Modelfile.iq1s &lt;&lt; 'EOF'\nFROM ./gguf_outputs/model-iq1_s.gguf\n\nSYSTEM \"\"\"You are a translator for Ugandan languages. Note: This is an experimental ultra-compressed model - quality may be limited.\"\"\"\n\n# Same template and parameters as above\nTEMPLATE \"\"\"&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n{{ .Response }}&lt;|im_end|&gt;\"\"\"\n\nPARAMETER stop \"&lt;|im_start|&gt;\"\nPARAMETER stop \"&lt;|im_end|&gt;\"\nPARAMETER temperature 0.3\nPARAMETER top_p 0.95\nPARAMETER num_ctx 2048  # Smaller context for experimental model\nEOF\n</code></pre>"},{"location":"sunflower/quantization/#importing-models-to-ollama_1","title":"Importing Models to Ollama","text":"<pre><code># Import Q4_K_M model (recommended)\nollama create sunflower-14b:q4 -f Modelfile.q4\n\n# Import experimental IQ1_S model\nollama create sunflower-14b:iq1s -f Modelfile.iq1s\n\n# Import other quantizations\nollama create sunflower-14b:q5 -f Modelfile.q5\nollama create sunflower-14b:q6 -f Modelfile.q6\n\n# Verify models are imported\nollama list\n</code></pre> <p>Expected output:</p> <pre><code>NAME                    ID              SIZE    MODIFIED\nsunflower-14b:q4        abc123def       8.4GB   2 minutes ago\nsunflower-14b:iq1s      def456ghi       3.4GB   1 minute ago\n</code></pre>"},{"location":"sunflower/quantization/#using-ollama-models_1","title":"Using Ollama Models","text":"<p>Interactive Chat:</p> <pre><code># Start interactive session with Q4 model\nollama run sunflower-14b:q4\n\n# Example conversation:\n# &gt;&gt;&gt; Translate to Luganda: Hello, how are you today?\n# &gt;&gt;&gt; Give a dictionary definition of the Samia term \"ovulwaye\" in English\n# &gt;&gt;&gt; /bye (to exit)\n\n# Start with experimental model\nollama run sunflower-14b:iq1s\n</code></pre> <p>Single Prompt Inference:</p> <pre><code># Quick translation with Q4 model\nollama run sunflower-14b:q4 \"Translate to Luganda: People in villages rarely accept new technologies.\"\n\n# Test experimental model\nollama run sunflower-14b:iq1s \"Translate to Luganda: Good morning\"\n\n# Dictionary definition\nollama run sunflower-14b:q4 'Give a dictionary definition of the Samia term \"ovulwaye\" in English'\n</code></pre>"},{"location":"sunflower/quantization/#ollama-api-usage_1","title":"Ollama API Usage","text":"<p>Start API Server:</p> <pre><code># Ollama automatically serves API on http://localhost:11434\n# Test API endpoint\ncurl http://localhost:11434/api/version\n</code></pre> <p>Python API Client:</p> <pre><code>import requests\nimport json\n\ndef translate_with_ollama(text, target_lang=\"Luganda\", model=\"sunflower-14b:q4\"):\n    url = \"http://localhost:11434/api/generate\"\n\n    payload = {\n        \"model\": model,\n        \"prompt\": f\"Translate to {target_lang}: {text}\",\n        \"stream\": False\n    }\n\n    response = requests.post(url, json=payload)\n    return response.json()[\"response\"]\n\n# Test translation\nresult = translate_with_ollama(\"Hello, how are you?\")\nprint(result)\n\n# Test experimental model\nresult_experimental = translate_with_ollama(\n    \"Good morning\",\n    model=\"sunflower-14b:iq1s\"\n)\nprint(\"Experimental model:\", result_experimental)\n</code></pre> <p>curl API Examples:</p> <pre><code># Basic translation\ncurl -X POST http://localhost:11434/api/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"sunflower-14b:q4\",\n    \"prompt\": \"Translate to Luganda: How are you today?\",\n    \"stream\": false\n  }'\n\n# Streaming response\ncurl -X POST http://localhost:11434/api/generate \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"sunflower-14b:q4\",\n    \"prompt\": \"Translate to Luganda: People in villages rarely accept new technologies.\",\n    \"stream\": true\n  }'\n</code></pre>"},{"location":"sunflower/quantization/#model-management_1","title":"Model Management","text":"<pre><code># List all models\nollama list\n\n# Show model details\nollama show sunflower-14b:q4\n\n# Remove a model\nollama rm sunflower-14b:iq1s\n\n# Copy model with new name\nollama cp sunflower-14b:q4 sunflower-translator\n\n# Pull/push to Ollama registry (if you publish there)\n# ollama push sunflower-14b:q4\n</code></pre>"},{"location":"sunflower/quantization/#performance-comparison-script_1","title":"Performance Comparison Script","text":"<p>Create <code>test_models.py</code>:</p> <pre><code>import time\nimport requests\n\nmodels = [\"sunflower-14b:q4\", \"sunflower-14b:iq1s\"]\ntest_prompt = \"Translate to Luganda: Hello, how are you today?\"\n\ndef test_model(model_name, prompt):\n    start_time = time.time()\n\n    response = requests.post(\"http://localhost:11434/api/generate\", json={\n        \"model\": model_name,\n        \"prompt\": prompt,\n        \"stream\": False\n    })\n\n    end_time = time.time()\n    result = response.json()\n\n    return {\n        \"model\": model_name,\n        \"response\": result[\"response\"],\n        \"time\": end_time - start_time,\n        \"tokens\": len(result[\"response\"].split())\n    }\n\n# Test all models\nfor model in models:\n    result = test_model(model, test_prompt)\n    print(f\"Model: {result['model']}\")\n    print(f\"Response: {result['response']}\")\n    print(f\"Time: {result['time']:.2f}s\")\n    print(f\"Tokens: {result['tokens']}\")\n    print(\"-\" * 50)\n</code></pre>"},{"location":"sunflower/quantization/#troubleshooting-ollama","title":"Troubleshooting Ollama","text":"<p>Common Issues:</p> <ol> <li>Model fails to load:</li> </ol> <pre><code># Check model file exists\nls -la gguf_outputs/model-q4_k_m.gguf\n\n# Verify Modelfile syntax\nollama create sunflower-14b:q4 -f Modelfile.q4 --verbose\n</code></pre> <ol> <li>Out of memory:</li> </ol> <pre><code># Use smaller quantization\nollama run sunflower-14b:iq1s  # Uses only 3.4GB\n\n# Or reduce context size in Modelfile\nPARAMETER num_ctx 2048\n</code></pre> <ol> <li>Poor quality with experimental models:</li> </ol> <pre><code># Compare outputs\nollama run sunflower-14b:q4 \"Your test prompt\"\nollama run sunflower-14b:iq1s \"Your test prompt\"\n\n# Expected: IQ1_S may have degraded quality\n</code></pre> <ol> <li>Ollama service not running:</li> </ol> <pre><code># Start service\nollama serve\n\n# Or check if running\nps aux | grep ollama\n</code></pre>"},{"location":"sunflower/quantization/#production-deployment_1","title":"Production Deployment","text":"<pre><code># Create production Modelfile with optimized settings\ncat &gt; Modelfile.production &lt;&lt; 'EOF'\nFROM ./gguf_outputs/model-q4_k_m.gguf\n\nSYSTEM \"\"\"You are a professional translator for Ugandan languages, made by Sunbird AI. Provide accurate, contextually appropriate translations.\"\"\"\n\nTEMPLATE \"\"\"&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n{{ .Response }}&lt;|im_end|&gt;\"\"\"\n\nPARAMETER stop \"&lt;|im_start|&gt;\"\nPARAMETER stop \"&lt;|im_end|&gt;\"\n\n# Production-optimized parameters\nPARAMETER temperature 0.1        # Lower for consistency\nPARAMETER top_p 0.9             # Slightly more focused\nPARAMETER repeat_penalty 1.05   # Minimal repetition penalty\nPARAMETER num_ctx 4096          # Full context\nPARAMETER num_predict 200       # Reasonable response length\nEOF\n\n# Create production model\nollama create sunflower-translator:production -f Modelfile.production\n</code></pre>"},{"location":"sunflower/quantization/#troubleshooting","title":"Troubleshooting","text":""},{"location":"sunflower/quantization/#common-issues","title":"Common Issues","text":"<p>1. Out of Memory During Merge</p> <pre><code># Use smaller precision or CPU offloading\nexport PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512\n</code></pre> <p>2. GGUF Conversion Fails</p> <pre><code># Check model architecture compatibility\npython3 llama.cpp/convert_hf_to_gguf.py models/merged_model --outfile test.gguf --dry-run\n</code></pre> <p>3. Quantization Too Slow</p> <pre><code># Use fewer threads or disable imatrix for testing\n./llama.cpp/build/bin/llama-quantize \\\n  gguf_outputs/model-merged-f16.gguf \\\n  gguf_outputs/model-q4_k_m.gguf \\\n  Q4_K_M \\\n  4  # Use 4 threads\n</code></pre> <p>4. Experimental Quantizations Unusable</p> <ul> <li>This is expected for extreme quantizations like IQ1_S</li> <li>Test with your specific use case</li> <li>Consider using IQ2_XXS as minimum viable quantization</li> </ul>"},{"location":"sunflower/quantization/#file-size-expectations","title":"File Size Expectations","text":"<p>For a 14B parameter model:</p> <ul> <li>Merge process: Requires 2x model size in RAM (~56GB peak)</li> <li>F16 GGUF: ~28GB final size</li> <li>Quantized models: 3GB-15GB depending on level</li> <li>Total storage needed: ~200GB for all quantizations</li> </ul>"},{"location":"sunflower/quantization/#performance-notes","title":"Performance Notes","text":"<ul> <li>Importance matrix generation: 30-60 minutes on modern hardware</li> <li>Each quantization: 5-10 minutes per model</li> <li>Upload time: Varies by connection, large files use Git LFS</li> <li>Memory usage: Peaks during merge, lower during quantization</li> </ul>"},{"location":"sunflower/quantization/#ollama-specific-issues","title":"Ollama-Specific Issues","text":"<p>1. Model fails to load:</p> <pre><code># Check model file exists\nls -la gguf_outputs/model-q4_k_m.gguf\n\n# Verify Modelfile syntax\nollama create sunflower-14b:q4 -f Modelfile.q4 --verbose\n</code></pre> <p>2. Out of memory with Ollama:</p> <pre><code># Use smaller quantization\nollama run sunflower-14b:iq1s  # Uses only 3.4GB\n\n# Or reduce context size in Modelfile\nPARAMETER num_ctx 2048\n</code></pre> <p>3. Poor quality with experimental models:</p> <pre><code># Compare outputs\nollama run sunflower-14b:q4 \"Your test prompt\"\nollama run sunflower-14b:iq1s \"Your test prompt\"\n\n# Expected: IQ1_S may have degraded quality\n</code></pre> <p>4. Ollama service not running:</p> <pre><code># Start service\nollama serve\n\n# Or check if running\nps aux | grep ollama\n</code></pre> <p>5. API connection issues:</p> <pre><code># Test API availability\ncurl http://localhost:11434/api/version\n\n# Check if port is blocked\nnetstat -tlnp | grep 11434\n</code></pre>"},{"location":"sunflower/quantization/#conclusion","title":"Conclusion","text":"<p>This tutorial demonstrates the complete pipeline for converting LoRA fine-tuned models to GGUF format with multiple quantization levels. The process enables deployment of large models on resource-constrained hardware while maintaining various quality/size trade-offs.</p> <p>The experimental ultra-low quantizations (IQ1_S, IQ2_XXS, TQ1_0) push the boundaries of model compression and should be used with appropriate quality expectations.</p> <p>For production use, Q4_K_M provides the best balance of quality and size, while Q5_K_M and Q6_K offer better quality at larger sizes. Always evaluate quantized models on your specific tasks before deployment.</p>"},{"location":"sunflower/quantization/#references","title":"References","text":"<ul> <li>llama.cpp Repository</li> <li>GGUF Format Specification</li> <li>PEFT Library Documentation</li> <li>Hugging Face Hub Documentation</li> </ul>"},{"location":"sunflower/sunflower/","title":"Sunflower API Documentation","text":"<p>Version: 1.0 Date: September 2025 Developed by: Sunbird AI  </p>"},{"location":"sunflower/sunflower/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Introduction</li> <li>Authentication</li> <li>Rate Limiting</li> <li>API Endpoints</li> <li>Error Handling</li> <li>Code Examples</li> <li>Best Practices</li> <li>Troubleshooting</li> </ol>"},{"location":"sunflower/sunflower/#introduction","title":"Introduction","text":"<p>The Sunflower API provides access to Sunbird AI's multilingual language model, specifically designed for Ugandan languages. The API specializes in:</p> <ul> <li>Multilingual conversations in Ugandan languages (Luganda, Acholi, Ateso, etc.)</li> <li>Cross-lingual translations and explanations</li> <li>Cultural context understanding</li> <li>Educational content in local languages</li> </ul>"},{"location":"sunflower/sunflower/#key-features","title":"Key Features","text":"<ul> <li>Automatic retry with exponential backoff</li> <li>Context-aware responses</li> <li>Usage tracking and monitoring</li> <li>Support for custom system messages</li> <li>Message history management</li> <li>Professional error handling</li> </ul>"},{"location":"sunflower/sunflower/#model-information","title":"Model Information","text":"<ul> <li>Primary Model: Sunbird/Sunflower-14B-FP8</li> <li>Supported Languages: Luganda, Acholi, Ateso, English, and other Ugandan languages</li> <li>Model Types: Qwen</li> </ul>"},{"location":"sunflower/sunflower/#authentication","title":"Authentication","text":"<p>All API requests require authentication using a valid API key. Include your API key in the request headers:</p> <pre><code>Authorization: Bearer YOUR_API_KEY\n</code></pre>"},{"location":"sunflower/sunflower/#obtaining-api-keys","title":"Obtaining API Keys","text":"<ol> <li>If you don't already have an account, create one at https://api.sunbird.ai/register and login.</li> <li>Go to the tokens page to get your access token which you'll use to authenticate</li> </ol>"},{"location":"sunflower/sunflower/#rate-limiting","title":"Rate Limiting","text":"<p>The API implements rate limiting based on account types:</p> <ul> <li>Free Tier: Limited requests per hour</li> <li>Professional: Higher rate limits</li> <li>Enterprise: Custom rate limits</li> </ul>"},{"location":"sunflower/sunflower/#api-endpoints","title":"API Endpoints","text":""},{"location":"sunflower/sunflower/#1-chat-completions-endpoint","title":"1. Chat Completions Endpoint","text":"<p>Endpoint: <code>POST tasks/sunflower_inference</code></p> <p>Professional endpoint for multilingual chat completions with full conversation management.</p>"},{"location":"sunflower/sunflower/#request-format","title":"Request Format","text":"<pre><code>{\n  \"messages\": [\n    {\n      \"role\": \"system|user|assistant\",\n      \"content\": \"Message content\"\n    }\n  ],\n  \"model_type\": \"qwen\",\n  \"temperature\": 0.3,\n  \"stream\": false,\n  \"system_message\": \"Optional custom system message\"\n}\n</code></pre>"},{"location":"sunflower/sunflower/#parameters","title":"Parameters","text":"Parameter Type Required Default Description <code>messages</code> Array Yes - List of conversation messages <code>model_type</code> String No \"qwen\" Model type: \"qwen\" <code>temperature</code> Float No 0.3 Sampling temperature (0.0-2.0) <code>stream</code> Boolean No false Whether to stream the response <code>system_message</code> String No null Custom system message"},{"location":"sunflower/sunflower/#message-object","title":"Message Object","text":"Field Type Required Description <code>role</code> String Yes Message role: \"system\", \"user\", or \"assistant\" <code>content</code> String Yes Message content (cannot be empty)"},{"location":"sunflower/sunflower/#response-format","title":"Response Format","text":"<pre><code>{\n  \"content\": \"AI response content\",\n  \"model_type\": \"qwen\",\n  \"usage\": {\n    \"completion_tokens\": 150,\n    \"prompt_tokens\": 100,\n    \"total_tokens\": 250\n  },\n  \"processing_time\": 2.35,\n  \"inference_time\": 1.85,\n  \"message_count\": 3\n}\n</code></pre>"},{"location":"sunflower/sunflower/#response-fields","title":"Response Fields","text":"Field Type Description <code>content</code> String The AI's response <code>model_type</code> String Model used for inference <code>usage</code> Object Token usage statistics <code>processing_time</code> Float Total processing time in seconds <code>inference_time</code> Float Model inference time in seconds <code>message_count</code> Integer Number of messages processed"},{"location":"sunflower/sunflower/#2-simple-inference-endpoint","title":"2. Simple Inference Endpoint","text":"<p>Endpoint: <code>POST /tasks/sunflower_simple</code></p> <p>Simplified interface for single instruction/response interactions.</p>"},{"location":"sunflower/sunflower/#request-format-form-data","title":"Request Format (Form Data)","text":"<pre><code>{\n  \"instruction\": \"Translate 'Good morning' to Luganda\",\n  \"model_type\": \"qwen\",\n  \"temperature\": 0.3,\n  \"system_message\": \"You are Sunflower, a multilingual assistant for Ugandan languages made by Sunbird AI.\"\n}\n</code></pre> Parameter Type Required Default Description <code>instruction</code> String Yes - The instruction or question <code>model_type</code> String No \"qwen\" Model type: \"qwen\" <code>temperature</code> Float No 0.3 Sampling temperature (0.0-2.0) <code>system_message</code> String No null Custom system message"},{"location":"sunflower/sunflower/#response-format_1","title":"Response Format","text":"<pre><code>{\n  \"response\": \"AI response content\",\n  \"model_type\": \"qwen\",\n  \"processing_time\": 1.85,\n  \"usage\": {\n    \"completion_tokens\": 120,\n    \"prompt_tokens\": 80,\n    \"total_tokens\": 200\n  },\n  \"success\": true\n}\n</code></pre>"},{"location":"sunflower/sunflower/#error-handling","title":"Error Handling","text":"<p>The API uses standard HTTP status codes and provides detailed error messages.</p>"},{"location":"sunflower/sunflower/#http-status-codes","title":"HTTP Status Codes","text":"Code Description 200 Success 400 Bad Request - Invalid input 401 Unauthorized - Invalid API key 429 Too Many Requests - Rate limit exceeded 500 Internal Server Error 502 Bad Gateway - Empty model response 503 Service Unavailable - Model loading 504 Gateway Timeout - Request timeout"},{"location":"sunflower/sunflower/#error-response-format","title":"Error Response Format","text":"<pre><code>{\n  \"detail\": \"Error description\",\n  \"status_code\": 400,\n  \"timestamp\": \"2025-09-26T10:30:00Z\"\n}\n</code></pre>"},{"location":"sunflower/sunflower/#common-error-messages","title":"Common Error Messages","text":""},{"location":"sunflower/sunflower/#model-loading-503","title":"Model Loading (503)","text":"<pre><code>{\n  \"detail\": \"The AI model is currently loading. This usually takes 2-3 minutes. Please try again shortly.\"\n}\n</code></pre>"},{"location":"sunflower/sunflower/#timeout-504","title":"Timeout (504)","text":"<pre><code>{\n  \"detail\": \"The request timed out. Please try again with a shorter prompt or check your network connection.\"\n}\n</code></pre>"},{"location":"sunflower/sunflower/#invalid-request-400","title":"Invalid Request (400)","text":"<pre><code>{\n  \"detail\": \"Message 0 content cannot be empty\"\n}\n</code></pre>"},{"location":"sunflower/sunflower/#code-examples","title":"Code Examples","text":""},{"location":"sunflower/sunflower/#python-examples","title":"Python Examples","text":""},{"location":"sunflower/sunflower/#1-basic-chat-completion","title":"1. Basic Chat Completion","text":"<pre><code>import requests\nimport json\n\ndef chat_with_sunflower(messages, api_key, base_url=\"https://api.sunbird.ai\"):\n    \"\"\"\n    Send a chat completion request to Sunflower API\n    \"\"\"\n    url = f\"{base_url}/tasks/sunflower_inference\"\n\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    payload = {\n        \"messages\": messages,\n        \"model_type\": \"qwen\",\n        \"temperature\": 0.3,\n        \"stream\": False\n    }\n\n    try:\n        response = requests.post(url, headers=headers, json=payload, timeout=30)\n        response.raise_for_status()\n        return response.json()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n# Example usage\napi_key = \"your_api_key_here\"\n\nmessages = [\n    {\n        \"role\": \"system\", \n        \"content\": \"You are Sunflower, a multilingual assistant for Ugandan languages made by Sunbird AI.\"\n    },\n    {\n        \"role\": \"user\", \n        \"content\": \"Translate 'Good morning' to Luganda\"\n    }\n]\n\nresult = chat_with_sunflower(messages, api_key)\nif result:\n    print(f\"Response: {result['content']}\")\n    print(f\"Tokens used: {result['usage']['total_tokens']}\")\n</code></pre>"},{"location":"sunflower/sunflower/#2-simple-inference","title":"2. Simple Inference","text":"<pre><code>import requests\n\ndef simple_inference(instruction, api_key, base_url=\"https://api.sunbird.ai\"):\n    \"\"\"\n    Send a simple inference request to Sunflower API\n    \"\"\"\n    url = f\"{base_url}/tasks/sunflower_simple\"\n\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\"\n    }\n\n    data = {\n        \"instruction\": instruction,\n        \"model_type\": \"qwen\",\n        \"temperature\": 0.3\n    }\n\n    try:\n        response = requests.post(url, headers=headers, data=data, timeout=30)\n        response.raise_for_status()\n        return response.json()\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Request failed: {e}\")\n        return None\n\n# Example usage\napi_key = \"your_api_key_here\"\ninstruction = \"Explain the meaning of 'Ubuntu' in Ugandan context\"\n\nresult = simple_inference(instruction, api_key)\nif result:\n    print(f\"Response: {result['response']}\")\n</code></pre>"},{"location":"sunflower/sunflower/#3-conversation-management","title":"3. Conversation Management","text":"<pre><code>class SunflowerConversation:\n    def __init__(self, api_key, base_url=\"https://api.sunbird.ai\"):\n        self.api_key = api_key\n        self.base_url = base_url\n        self.messages = []\n\n    def set_system_message(self, message):\n        \"\"\"Set or update the system message\"\"\"\n        # Remove existing system message\n        self.messages = [msg for msg in self.messages if msg[\"role\"] != \"system\"]\n        # Add new system message at the beginning\n        self.messages.insert(0, {\"role\": \"system\", \"content\": message})\n\n    def add_user_message(self, content):\n        \"\"\"Add a user message to the conversation\"\"\"\n        self.messages.append({\"role\": \"user\", \"content\": content})\n\n    def get_response(self, model_type=\"qwen\", temperature=0.3):\n        \"\"\"Get AI response and add it to conversation\"\"\"\n        url = f\"{self.base_url}/tasks/sunflower_inference\"\n\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n        payload = {\n            \"messages\": self.messages,\n            \"model_type\": model_type,\n            \"temperature\": temperature\n        }\n\n        try:\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\n            response.raise_for_status()\n            result = response.json()\n\n            # Add assistant response to conversation\n            self.messages.append({\n                \"role\": \"assistant\", \n                \"content\": result[\"content\"]\n            })\n\n            return result\n\n        except requests.exceptions.RequestException as e:\n            print(f\"Request failed: {e}\")\n            return None\n\n    def clear_conversation(self):\n        \"\"\"Clear conversation history but keep system message\"\"\"\n        system_messages = [msg for msg in self.messages if msg[\"role\"] == \"system\"]\n        self.messages = system_messages\n\n# Example usage\nconversation = SunflowerConversation(\"your_api_key_here\")\n\n# Set system message\nconversation.set_system_message(\n    \"You are Sunflower, a helpful assistant specializing in Ugandan languages and culture.\"\n)\n\n# Have a conversation\nconversation.add_user_message(\"Hello, can you greet me in Luganda?\")\nresponse1 = conversation.get_response()\nprint(f\"AI: {response1['content']}\")\n\nconversation.add_user_message(\"How do I say 'thank you' in Acholi?\")\nresponse2 = conversation.get_response()\nprint(f\"AI: {response2['content']}\")\n</code></pre>"},{"location":"sunflower/sunflower/#4-error-handling-with-retry-logic","title":"4. Error Handling with Retry Logic","text":"<pre><code>import time\nimport random\nfrom typing import Optional\n\ndef sunflower_request_with_retry(\n    messages, \n    api_key, \n    max_retries=3,\n    base_delay=2.0,\n    base_url=\"https://api.sunbird.ai\"\n) -&gt; Optional[dict]:\n    \"\"\"\n    Make a request to Sunflower API with exponential backoff retry\n    \"\"\"\n    url = f\"{base_url}/tasks/sunflower_inference\"\n\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n\n    payload = {\n        \"messages\": messages,\n        \"model_type\": \"qwen\",\n        \"temperature\": 0.3\n    }\n\n    for attempt in range(max_retries + 1):\n        try:\n            response = requests.post(url, headers=headers, json=payload, timeout=30)\n\n            if response.status_code == 200:\n                return response.json()\n\n            elif response.status_code == 503:\n                # Model loading - retry with longer delay\n                if attempt &lt; max_retries:\n                    delay = base_delay * (2 ** attempt) + random.uniform(0, 1)\n                    print(f\"Model loading, retrying in {delay:.1f} seconds...\")\n                    time.sleep(delay)\n                    continue\n\n            elif response.status_code == 429:\n                # Rate limited - retry with exponential backoff\n                if attempt &lt; max_retries:\n                    delay = base_delay * (2 ** attempt)\n                    print(f\"Rate limited, retrying in {delay:.1f} seconds...\")\n                    time.sleep(delay)\n                    continue\n\n            else:\n                print(f\"Request failed with status {response.status_code}: {response.text}\")\n                return None\n\n        except requests.exceptions.Timeout:\n            if attempt &lt; max_retries:\n                delay = base_delay * (2 ** attempt)\n                print(f\"Request timed out, retrying in {delay:.1f} seconds...\")\n                time.sleep(delay)\n                continue\n            else:\n                print(\"Request timed out after all retries\")\n                return None\n\n        except requests.exceptions.RequestException as e:\n            print(f\"Request failed: {e}\")\n            return None\n\n    print(\"All retry attempts exhausted\")\n    return None\n\n# Example usage\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is the capital of Uganda?\"}\n]\n\nresult = sunflower_request_with_retry(messages, \"your_api_key_here\")\nif result:\n    print(f\"Response: {result['content']}\")\n</code></pre>"},{"location":"sunflower/sunflower/#javascriptnodejs-examples","title":"JavaScript/Node.js Examples","text":""},{"location":"sunflower/sunflower/#1-basic-chat-completion_1","title":"1. Basic Chat Completion","text":"<pre><code>const axios = require('axios');\n\nasync function chatWithSunflower(messages, apiKey, baseUrl = 'https://api.sunbird.ai') {\n    const url = `${baseUrl}/tasks/sunflower_inference`;\n\n    const headers = {\n        'Authorization': `Bearer ${apiKey}`,\n        'Content-Type': 'application/json'\n    };\n\n    const payload = {\n        messages: messages,\n        model_type: 'qwen',\n        temperature: 0.3,\n        stream: false\n    };\n\n    try {\n        const response = await axios.post(url, payload, { \n            headers: headers,\n            timeout: 30000 \n        });\n        return response.data;\n    } catch (error) {\n        console.error('Request failed:', error.message);\n        if (error.response) {\n            console.error('Response data:', error.response.data);\n        }\n        return null;\n    }\n}\n\n// Example usage\nconst apiKey = 'your_api_key_here';\n\nconst messages = [\n    {\n        role: 'system',\n        content: 'You are Sunflower, a multilingual assistant for Ugandan languages.'\n    },\n    {\n        role: 'user',\n        content: 'How do you say \"welcome\" in Luganda?'\n    }\n];\n\nchatWithSunflower(messages, apiKey)\n    .then(result =&gt; {\n        if (result) {\n            console.log('Response:', result.content);\n            console.log('Tokens used:', result.usage.total_tokens);\n        }\n    });\n</code></pre>"},{"location":"sunflower/sunflower/#2-simple-inference_1","title":"2. Simple Inference","text":"<pre><code>const FormData = require('form-data');\nconst axios = require('axios');\n\nasync function simpleInference(instruction, apiKey, baseUrl = 'https://api.sunbird.ai') {\n    const url = `${baseUrl}/tasks/sunflower_simple`;\n\n    const formData = new FormData();\n    formData.append('instruction', instruction);\n    formData.append('model_type', 'qwen');\n    formData.append('temperature', '0.3');\n\n    const headers = {\n        'Authorization': `Bearer ${apiKey}`,\n        ...formData.getHeaders()\n    };\n\n    try {\n        const response = await axios.post(url, formData, {\n            headers: headers,\n            timeout: 30000\n        });\n        return response.data;\n    } catch (error) {\n        console.error('Request failed:', error.message);\n        return null;\n    }\n}\n\n// Example usage\nconst apiKey = 'your_api_key_here';\nconst instruction = 'Translate \"How are you?\" to Ateso';\n\nsimpleInference(instruction, apiKey)\n    .then(result =&gt; {\n        if (result) {\n            console.log('Response:', result.response);\n        }\n    });\n</code></pre>"},{"location":"sunflower/sunflower/#curl-examples","title":"cURL Examples","text":""},{"location":"sunflower/sunflower/#1-chat-completion","title":"1. Chat Completion","text":"<pre><code>curl -X POST \"https://api.sunbird.ai/tasks/sunflower_inference\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"messages\": [\n      {\n        \"role\": \"system\",\n        \"content\": \"You are Sunflower, a multilingual assistant for Ugandan languages made by Sunbird AI.\"\n      },\n      {\n        \"role\": \"user\", \n        \"content\": \"Translate \\\"Good evening\\\" to Luganda\"\n      }\n    ],\n    \"model_type\": \"qwen\",\n    \"temperature\": 0.3\n  }'\n</code></pre>"},{"location":"sunflower/sunflower/#2-simple-inference_2","title":"2. Simple Inference","text":"<pre><code>curl -X POST \"https://api.sunbird.ai/tasks/sunflower_simple\" \\\n  -H \"Authorization: Bearer YOUR_API_KEY\" \\\n  -F \"instruction=What are some traditional Ugandan foods?\" \\\n  -F \"model_type=qwen\" \\\n  -F \"temperature=0.3\"\n</code></pre>"},{"location":"sunflower/sunflower/#best-practices","title":"Best Practices","text":""},{"location":"sunflower/sunflower/#1-message-management","title":"1. Message Management","text":"<ul> <li>Always include a system message to provide context and improve response quality</li> <li>Keep conversation history relevant - trim old messages to stay within token limits</li> <li>Use clear, specific prompts for better results</li> </ul>"},{"location":"sunflower/sunflower/#2-error-handling","title":"2. Error Handling","text":"<ul> <li>Implement retry logic for transient errors (503, 504, 429)</li> <li>Handle model loading delays - allow 2-3 minutes for cold starts</li> <li>Validate input before sending requests</li> </ul>"},{"location":"sunflower/sunflower/#3-performance-optimization","title":"3. Performance Optimization","text":"<ul> <li>Use appropriate temperature settings:</li> <li>0.0-0.3: More deterministic, good for translations</li> <li>0.4-0.7: Balanced creativity</li> <li>0.8-1.0: More creative responses</li> <li>Consider using simple endpoint for single-turn interactions</li> <li>Implement caching for repeated requests</li> </ul>"},{"location":"sunflower/sunflower/#4-rate-limiting","title":"4. Rate Limiting","text":"<ul> <li>Monitor rate limit headers in responses</li> <li>Implement exponential backoff for rate limit exceeded errors</li> <li>Consider request batching where appropriate</li> </ul>"},{"location":"sunflower/sunflower/#5-security","title":"5. Security","text":"<ul> <li>Never expose API keys in client-side code</li> <li>Use environment variables to store credentials</li> <li>Implement proper authentication in your applications</li> </ul>"},{"location":"sunflower/sunflower/#troubleshooting","title":"Troubleshooting","text":""},{"location":"sunflower/sunflower/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"sunflower/sunflower/#1-model-loading-errors-503","title":"1. Model Loading Errors (503)","text":"<p>Problem: \"The AI model is currently loading\"</p> <p>Solutions: - Wait 2-3 minutes before retrying - Implement exponential backoff retry logic - Use longer timeout values during cold starts</p>"},{"location":"sunflower/sunflower/#2-empty-responses-502","title":"2. Empty Responses (502)","text":"<p>Problem: \"The model returned an empty response\"</p> <p>Solutions: - Rephrase your request to be more specific - Check if the input contains inappropriate content - Try a different temperature setting</p>"},{"location":"sunflower/sunflower/#3-timeout-errors-504","title":"3. Timeout Errors (504)","text":"<p>Problem: Request times out</p> <p>Solutions: - Reduce prompt length - Use simpler queries - Increase timeout values in your code - Check network connectivity</p>"},{"location":"sunflower/sunflower/#4-rate-limiting-429","title":"4. Rate Limiting (429)","text":"<p>Problem: Too many requests</p> <p>Solutions: - Implement request queuing - Use exponential backoff - Consider upgrading your account tier</p>"},{"location":"sunflower/sunflower/#5-invalid-message-format-400","title":"5. Invalid Message Format (400)","text":"<p>Problem: Message validation errors</p> <p>Solutions: - Ensure all messages have 'role' and 'content' fields - Check that content is not empty - Validate role values ('system', 'user', 'assistant')</p>"},{"location":"sunflower/sunflower/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Enable detailed logging to track request/response cycles</li> <li>Test with simple queries first before complex conversations</li> <li>Use the simple endpoint to isolate issues</li> <li>Check API status at Sunbird AI's status page</li> <li>Monitor token usage to avoid unexpected costs</li> </ol>"},{"location":"sunflower/sunflower/#support-and-resources","title":"Support and Resources","text":""},{"location":"sunflower/sunflower/#getting-help","title":"Getting Help","text":"<ul> <li>Email Support: info@sunbird.ai</li> <li>Documentation: https://salt.sunbird.ai/docs</li> </ul> <p>\u00a9 2025 Sunbird AI. All rights reserved.</p> <p>This documentation is subject to updates and improvements. Please check the official documentation website for the latest version.</p>"},{"location":"tutorials/01-introduction/","title":"Introduction","text":"<p>Leb, inspired by the Luo word for 'language,' is a project dedicated to the seamless integration of Sunbird AI Language Technology. Our goal is to empower developers to effortlessly create machine learning models for Neural Machine Translation (NMT), Speech-to-Text (STT), Text-to-Speech (TTS), and other language-related applications.</p> <p>By drawing inspiration from the Luo concept of 'language' itself, Project Leb is envisioned as a springboard for connecting ideas and cultures across the Africa's diverse range of tongues and dialects. Just as languages connect people, this technology would connect languages - old and new - through an inclusive platform optimized for integration, accessibility, and human-centric design.</p>"},{"location":"tutorials/02-installation/","title":"Installation","text":"<p>This part of the project documentation focuses on an understanding-oriented approach. You'll get a chance to read about the background of the project, as well as reasoning about how it was implemented.</p> <p>Note: Expand this section by considering the following points:</p> <ul> <li>Give context and background on your library</li> <li>Explain why you created it</li> <li>Provide multiple examples and approaches of how     to work with it</li> <li>Help the reader make connections</li> <li>Avoid writing instructions or technical descriptions     here</li> </ul>"},{"location":"tutorials/03-quick-tour/","title":"03 quick tour","text":"<p>This part of the project documentation focuses on a problem-oriented approach. You'll tackle common tasks that you might have, with the help of the code provided in this project.</p>"},{"location":"tutorials/04-basics/","title":"Basics","text":""},{"location":"tutorials/04-basics/#one-to-multiple-translation-english-text-to-luganda-and-acholi-text","title":"one-to-multiple translation: English text to Luganda and Acholi text","text":"<pre><code>import sys\nsys.path.append('../..')\nimport leb.dataset\nimport leb.utils\nimport yaml\n</code></pre> <p>set up the configs</p> <pre><code>yaml_config = '''\nhuggingface_load:\n  path: Sunbird/salt\n  split: train\n  name: text-all\nsource:\n  type: text\n  language: eng\n  preprocessing:\n      - prefix_target_language\ntarget:\n  type: text\n  language: [lug, ach]\n'''\n\nconfig = yaml.safe_load(yaml_config)\nds = leb.dataset.create(config)\nlist(ds.take(5))\n</code></pre> <p>output</p> <pre><code>[\n  {\n    \"source\": \"&gt;&gt;lug&lt;&lt; Eggplants always grow best under warm conditions.\",\n    \"target\": \"Bbiringanya lubeerera  asinga kukulira mu mbeera ya bugumu\"\n  },\n  {\n    \"source\": \"&gt;&gt;ach&lt;&lt; Eggplants always grow best under warm conditions.\",\n    \"target\": \"Bilinyanya pol kare dongo maber ka lyeto tye\"\n  },\n  {\n    \"source\": \"&gt;&gt;lug&lt;&lt; Farmland is sometimes a challenge to farmers.\",\n    \"target\": \"Ettaka ly'okulimirako n'okulundirako ebiseera ebimu kisoomooza abalimi\"\n  },\n  {\n    \"source\": \"&gt;&gt;ach&lt;&lt; Farmland is sometimes a challenge to farmers.\",\n    \"target\": \"Ngom me pur i kare mukene obedo peko madit bot lupur\"\n  },\n  {\n    \"source\": \"&gt;&gt;lug&lt;&lt; Farmers should be encouraged to grow more coffee.\",\n    \"target\": \"Abalimi balina okukubirizibwa okwongera okulima emmwanyi\"\n  }\n]\n</code></pre> <p>This is how a basic data loader works</p>"},{"location":"tutorials/07-speech-datasets/","title":"Sunbird African Language Technology (SALT) dataset","text":"<p>SALT is a multi-way parallel text and speech corpus of Engish and six languages widely spoken in Uganda and East Africa: <code>Luganda</code>, <code>Lugbara</code>, <code>Acholi</code>, <code>Runyankole</code>, <code>Ateso</code> and <code>Swahili</code>. The core of the dataset is a set of <code>25,000</code> sentences covering a range of topics of local relevance, such as agriculture, health and society. Each sentence is translated into all languages, to support machine translation, and speech recordings are made for approximately <code>5,000</code> of the sentences both by a variety of speakers in natural settings (suitable for ASR) and by professionals in a studio setting (suitable for text-to-speech).</p>"},{"location":"tutorials/07-speech-datasets/#subsets","title":"Subsets","text":"Subset name Contents text-all Text translations of each sentence. multispeaker-<code>{lang}</code> Speech recordings of each sentence, by a variety of speakers in natural settings. studio-<code>{lang}</code> Speech recordings in a studio setting, suitable for text-to-speech. <p>The sentence IDs map across subsets, so that for example the text of a sentence in Acholi can be mapped to the studio recording of that concept being expressed in Swahili. The subsets can therefore be combined to support the training and evaluation of several further tasks, such as speech-to-text translation and speech-to-speech translation.</p>"},{"location":"tutorials/07-speech-datasets/#language-support","title":"Language support","text":"ISO 639-3 Language Translated text Multispeaker speech Studio speech eng English (Ugandan accent) Yes Yes Yes lug Luganda Yes Yes Yes ach Acholi Yes Yes Yes lgg Lugbara Yes Yes Yes teo Ateso Yes Yes Yes nyn Runyankole Yes Yes Yes swa Swahili Yes No Yes ibo Igbo Yes No No"},{"location":"tutorials/07-speech-datasets/#helper-utilities","title":"Helper utilities","text":"<p>Code for convenient experimentation with multilingual models can be found at https://github.com/SunbirdAI/salt. See example notebooks here.</p>"},{"location":"tutorials/07-speech-datasets/#collaborators","title":"Collaborators","text":"<p>This dataset was collected in practical collaboration between Sunbird AI and the Makerere University AI Lab (Ugandan languages) and KenCorpus, Maseno University (Swahili).</p>"},{"location":"tutorials/07-speech-datasets/#reference","title":"Reference","text":"<p>Machine Translation For African Languages: Community Creation Of Datasets And Models In Uganda. Benjamin Akera, Jonathan Mukiibi, Lydia Sanyu Naggayi, Claire Babirye, Isaac Owomugisha, Solomon Nsumba, Joyce Nakatumba-Nabende, Engineer Bainomugisha, Ernest Mwebaze, John Quinn. 3<sup>rd</sup> Workshop on African Natural Language Processing, 2022.</p>"},{"location":"tutorials/08-translation-models/","title":"NLLB-Based Translation Model Training Documentation","text":""},{"location":"tutorials/08-translation-models/#overview","title":"Overview","text":"<p>This documentation describes the training process and configuration for a multilingual translation model based on Facebook's NLLB-200-1.3B architecture. The model supports bidirectional translation between English and several African languages: Acholi, Lugbara, Luganda, Runyankole, and Ateso.</p>"},{"location":"tutorials/08-translation-models/#model-architecture","title":"Model Architecture","text":"<ul> <li>Base Model: facebook/nllb-200-1.3B</li> <li>Model Type: M2M100ForConditionalGeneration</li> <li>Tokenizer: NllbTokenizer</li> <li>Special Adaptations: Custom token mappings for African languages not in the original NLLB vocabulary</li> </ul>"},{"location":"tutorials/08-translation-models/#supported-languages","title":"Supported Languages","text":"ISO 693-3 Language Name eng English ach Acholi lgg Lugbara lug Luganda nyn Runyankole teo Ateso"},{"location":"tutorials/08-translation-models/#training-data","title":"Training Data","text":"<p>The model was trained on a diverse collection of datasets:</p>"},{"location":"tutorials/08-translation-models/#primary-dataset","title":"Primary Dataset","text":"<ul> <li>SALT dataset (Sunbird/salt)</li> </ul>"},{"location":"tutorials/08-translation-models/#additional-external-resources","title":"Additional External Resources","text":"<ol> <li>AI4D dataset</li> <li>FLORES-200</li> <li>LAFAND-MT (English-Luganda and English-Luo combinations)</li> <li>Mozilla Common Voice 110</li> <li>MT560 (Acholi, Luganda, Runyankole variants)</li> <li>Back-translated data:</li> <li>Google Translate based back-translations</li> <li>Language-specific back-translations (Acholi-English, Luganda-English)</li> </ol>"},{"location":"tutorials/08-translation-models/#training-configuration","title":"Training Configuration","text":""},{"location":"tutorials/08-translation-models/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CUDA-capable GPU (recommended)</li> <li>Sufficient RAM for large batch processing</li> </ul>"},{"location":"tutorials/08-translation-models/#key-training-parameters","title":"Key Training Parameters","text":"<pre><code>Effective Batch Size: 3000\nTraining Batch Size: 25\nEvaluation Batch Size: 25\nGradient Accumulation Steps: 120\nLearning Rate: 3.0e-4\nOptimizer: Adafactor\nWeight Decay: 0.01\nMaximum Steps: 1500\nFP16 Training: Enabled\n</code></pre>"},{"location":"tutorials/08-translation-models/#data-preprocessing","title":"Data Preprocessing","text":"<p>The training pipeline includes several preprocessing steps: 1. Text cleaning 2. Target sentence format matching 3. Random case augmentation 4. Character augmentation 5. Dataset-specific tagging (MT560: '', Backtranslation: '')"},{"location":"tutorials/08-translation-models/#model-training-process","title":"Model Training Process","text":""},{"location":"tutorials/08-translation-models/#setup","title":"Setup","text":"<ol> <li> <p>Install required dependencies: <pre><code>pip install peft transformers datasets bitsandbytes accelerate sentencepiece sacremoses wandb\n</code></pre></p> </li> <li> <p>Initialize the tokenizer with custom language mappings: <pre><code>tokenizer = transformers.NllbTokenizer.from_pretrained(\n    'facebook/nllb-200-distilled-1.3B',\n    src_lang='eng_Latn',\n    tgt_lang='eng_Latn')\n</code></pre></p> </li> <li> <p>Configure language code mappings: <pre><code>code_mapping = {\n    'eng': 'eng_Latn',\n    'lug': 'lug_Latn',\n    'ach': 'luo_Latn',\n    'nyn': 'ace_Latn',\n    'teo': 'afr_Latn',\n    'lgg': 'aka_Latn'\n}\n</code></pre></p> </li> </ol>"},{"location":"tutorials/08-translation-models/#training-process","title":"Training Process","text":"<ol> <li>Data Collation</li> <li>Uses DataCollatorForSeq2Seq</li> <li>Handles language code insertion</li> <li> <p>Manages padding and truncation</p> </li> <li> <p>Evaluation Strategy</p> </li> <li>Evaluation every 100 steps</li> <li>Model checkpointing every 100 steps</li> <li>Early stopping with patience of 4</li> <li>BLEU score monitoring</li> </ol>"},{"location":"tutorials/08-translation-models/#evaluation-results","title":"Evaluation Results","text":""},{"location":"tutorials/08-translation-models/#bleu-scores-on-development-set","title":"BLEU Scores on Development Set","text":"Source Target BLEU Score ach eng 28.371 lgg eng 30.450 lug eng 41.978 nyn eng 32.296 teo eng 30.422 eng ach 20.972 eng lgg 22.362 eng lug 30.359 eng nyn 15.305 eng teo 21.391"},{"location":"tutorials/08-translation-models/#usage-example","title":"Usage Example","text":"<pre><code>import transformers\nimport torch\n\ndef translate_text(text, source_language, target_language):\n    tokenizer = transformers.NllbTokenizer.from_pretrained(\n        'Sunbird/translate-nllb-1.3b-salt')\n    model = transformers.M2M100ForConditionalGeneration.from_pretrained(\n        'Sunbird/translate-nllb-1.3b-salt')\n\n    language_tokens = {\n        'eng': 256047,\n        'ach': 256111,\n        'lgg': 256008,\n        'lug': 256110,\n        'nyn': 256002,\n        'teo': 256006,\n    }\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    inputs['input_ids'][0][0] = language_tokens[source_language]\n\n    translated_tokens = model.to(device).generate(\n        **inputs,\n        forced_bos_token_id=language_tokens[target_language],\n        max_length=100,\n        num_beams=5,\n    )\n\n    return tokenizer.batch_decode(\n        translated_tokens, skip_special_tokens=True)[0]\n</code></pre>"},{"location":"tutorials/08-translation-models/#model-limitations-and-considerations","title":"Model Limitations and Considerations","text":"<ol> <li>Performance varies significantly between language pairs</li> <li>Best results achieved when English is involved (either source or target)</li> <li>Performance may degrade for:</li> <li>Very long sentences</li> <li>Domain-specific terminology</li> <li>Informal or colloquial language</li> </ol>"},{"location":"tutorials/08-translation-models/#future-improvements","title":"Future Improvements","text":"<ol> <li>Expand training data for lower-performing language pairs</li> <li>Implement more robust data augmentation techniques</li> <li>Explore domain adaptation for specific use cases</li> <li>Fine-tune model size and architecture for optimal performance</li> </ol>"},{"location":"tutorials/08-translation-models/#references","title":"References","text":"<ul> <li>NLLB-200 Paper: No Language Left Behind</li> <li>SALT Dataset: Sunbird/salt</li> <li>Model Weights: Sunbird/translate-nllb-1.3b-salt</li> </ul>"},{"location":"tutorials/09-asr-models/","title":"ASR Models","text":""},{"location":"tutorials/09-asr-models/#whisper-large-for-ugandan-languages","title":"Whisper large for Ugandan languages","text":"<p>This model is an adaptation of whisper-large-v2 for the following languages widely spoken in Uganda: Luganda, Acholi, Lugbara, Ateso, Runyankole and English (Ugandan accent).</p>"},{"location":"tutorials/09-asr-models/#training","title":"Training","text":"<p>The model was trained with the SALT dataset, Common Voice (Luganda) and FLEURS datasets. To help with generalisation in practical settings, training used addition of random noise and random downsampling to 8kHz to simulate phone speech.</p>"},{"location":"tutorials/09-asr-models/#usage","title":"Usage","text":"<p>The model is used in a similar way to the base Whisper model. The model will attempt to auto-detect the language and provide a transcription.  However, note that language detection is not always accurate and results may be improved by specifying it instead. The languages in this model are not supported by the base Whisper model, so the format is slightly different:</p> <pre><code>import transformers\nimport datasets\nimport torch\n\nprocessor = transformers.WhisperProcessor.from_pretrained(\n    \"Sunbird/asr-whisper-large-v2-salt\")\nmodel = transformers.WhisperForConditionalGeneration.from_pretrained(\n    \"Sunbird/asr-whisper-large-v2-salt\")\n\nSALT_LANGUAGE_TOKENS_WHISPER = {\n    'eng': 50259,  # English (Ugandan)\n    'ach': 50357,  # Acholi\n    'lgg': 50356,  # Lugbara\n    'lug': 50355,  # Luganda\n    'nyn': 50354,  # Runyankole\n    'teo': 50353,  # Ateso\n}\n\n# Get some test audio\nds = datasets.load_dataset('Sunbird/salt', 'multispeaker-lug', split='test')\naudio = ds[0]['audio']\nsample_rate = ds[0]['sample_rate']\n\n# Specify a language from one of the above.\nlang = 'lug'\n\n# Apply the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_features = processor(\n    audio, sampling_rate=sample_rate, return_tensors=\"pt\").input_features\ninput_features = input_features.to(device)\npredicted_ids = model.to(device).generate(\n    input_features,\n    # Optionally set language=None here instead to auto-detect.\n    language=processor.tokenizer.decode(SALT_LANGUAGE_TOKENS_WHISPER[lang]),\n    forced_decoder_ids=None)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n\nprint(transcription)\n# Ekikoola kya kasooli kya kyenvu wabula langi yaakyo etera okuba eya kitaka wansi.\n</code></pre>"},{"location":"tutorials/09-asr-models/#mms-speech-recognition-for-ugandan-languages","title":"MMS speech recognition for Ugandan languages","text":"<p>This is a fine-tuned version of facebook/mms-1b-all for Ugandan languages, trained with the SALT dataset. The languages supported are:</p> code language lug Luganda ach Acholi lgg Lugbara teo Ateso nyn Runyankole eng English (Ugandan) <p>For each  language there are two adapters: one optimised for cases where the speech is only in that language, and another in which code-switching with English is expected.</p>"},{"location":"tutorials/09-asr-models/#usage_1","title":"Usage","text":"<p>Usage is the same as the base model, though with different adapters available.</p> <pre><code>import torch\nimport transformers\nimport datasets\n\n# Available adapters:\n# ['lug', 'lug+eng', 'ach', 'ach+eng', 'lgg', 'lgg+eng',\n#  'nyn', 'nyn+eng', 'teo', 'teo+eng']\nlanguage = 'lug'\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = transformers.Wav2Vec2ForCTC.from_pretrained(\n    'Sunbird/asr-mms-salt').to(device)\nmodel.load_adapter(language)\n\nprocessor = transformers.Wav2Vec2Processor.from_pretrained(\n    'Sunbird/asr-mms-salt')\nprocessor.tokenizer.set_target_lang(language)\n\n# Get some test audio\nds = datasets.load_dataset('Sunbird/salt', 'multispeaker-lug', split='test')\naudio = ds[0]['audio']\nsample_rate = ds[0]['sample_rate']\n\n# Apply the model\ninputs = processor(audio, sampling_rate=sample_rate, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs.to(device)).logits\n\nids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n\nprint(transcription)\n# ekikola ky'akasooli kyakyenvu wabula langi yakyo etera okuba eyaakitaka wansi\n</code></pre> <p>The output of this model is unpunctuated and lower case. For applications requiring formatted text, an alternative model is Sunbird/asr-whisper-large-v2-salt.</p>"},{"location":"tutorials/10-tts-spark-models/","title":"Spark-TTS Inference Guide","text":"<p>This documentation provides a comprehensive guide to using the Spark-TTS model for text-to-speech (TTS) inference. Spark-TTS is a multilingual TTS system capable of generating speech in various East African languages and accents, such as Acholi, Ateso, Runyankore, Lugbara, Swahili, and Luganda, using predefined speaker IDs. The model is based on a customized version hosted on Hugging Face and relies on an audio tokenizer from the original Spark-TTS repository.</p> <p>The guide is derived from the provided example script (<code>spark_tts_inference_example_new.py</code>) and Jupyter notebook (<code>spark_tts_inference_example_new.ipynb</code>), which demonstrate setup, model loading, and speech generation. This is intended for developers, researchers, or users interested in TTS applications, particularly for low-resource languages.</p>"},{"location":"tutorials/10-tts-spark-models/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python Environment: Python 3.8+ (tested with 3.12.3 in the examples).</li> <li>Hardware: GPU recommended (CUDA-enabled for faster inference). The examples use <code>torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")</code>.</li> <li>Hugging Face Account: Required for downloading models. You'll need to log in via <code>huggingface_hub.notebook_login()</code> or set up an access token.</li> <li>Dependencies: Install the required libraries using pip. No internet access is needed during inference after downloads, but initial setup requires it.</li> </ul>"},{"location":"tutorials/10-tts-spark-models/#installation","title":"Installation","text":"<ol> <li>Install Dependencies:    Run the following command to install the necessary packages:</li> </ol> <pre><code>pip install -qU xformers transformers unsloth omegaconf einx einops soundfile librosa torch torchaudio\n</code></pre> <p>These include:    - <code>transformers</code> and <code>unsloth</code> for model handling.    - <code>soundfile</code> and <code>librosa</code> for audio processing.    - <code>torch</code> and <code>torchaudio</code> for tensor operations and audio transforms.    - Others for configuration and utilities.</p> <p>Note: If you encounter dependency conflicts (e.g., with <code>pyarrow</code>), resolve them based on your environment (e.g., downgrade if needed).</p> <ol> <li>Clone the Spark-TTS Repository:    The repository provides the audio tokenizer and utility code. Clone it from GitHub:</li> </ol> <pre><code>git clone https://github.com/SparkAudio/Spark-TTS\n</code></pre> <p>Add it to your Python path:</p> <pre><code>import sys\nsys.path.append('Spark-TTS')\n</code></pre> <ol> <li> <p>Download the Models from Hugging Face:    The models are hosted on Hugging Face Hub. Download them as follows:</p> </li> <li> <p>Audio Tokenizer: From the repository <code>unsloth/Spark-TTS-0.5B</code>. This is the BiCodecTokenizer for encoding/decoding audio tokens.</p> <ul> <li>Download URL: https://huggingface.co/unsloth/Spark-TTS-0.5B</li> <li>Use <code>snapshot_download</code> to fetch only the tokenizer parts (ignore LLM files):</li> </ul> <pre><code>from huggingface_hub import snapshot_download\nsnapshot_download(\n    \"unsloth/Spark-TTS-0.5B\", \n    local_dir=\"Spark-TTS-0.5B\",\n    ignore_patterns=[\"*LLM*\"]\n)\n</code></pre> </li> <li> <p>Customized TTS Model: From the repository <code>jq/spark-tts-salt</code>. This is the core language model for TTS generation.</p> <ul> <li>Download URL: https://huggingface.co/jq/spark-tts-salt</li> <li>Load it directly with <code>transformers</code>:</li> </ul> <pre><code>import transformers\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    \"jq/spark-tts-salt\",\n    device_map='auto',\n    torch_dtype=\"auto\"\n)\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"jq/spark-tts-salt\")\n</code></pre> </li> </ol> <p>Before downloading, log in to Hugging Face:</p> <pre><code>import huggingface_hub\nhuggingface_hub.notebook_login()  # Or use CLI: huggingface-cli login\n</code></pre> <p>These models are open-source under permissive licenses (check the repository pages for details, e.g., Apache 2.0 or similar).</p>"},{"location":"tutorials/10-tts-spark-models/#usage","title":"Usage","text":""},{"location":"tutorials/10-tts-spark-models/#importing-required-modules","title":"Importing Required Modules","text":"<p>Import the necessary libraries and utilities:</p> <pre><code>import re\nimport numpy as np\nimport torch\nimport time\nfrom IPython.display import Audio, display  # For Jupyter; optional in scripts\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom sparktts.models.audio_tokenizer import BiCodecTokenizer\nfrom sparktts.utils.audio import audio_volume_normalize  # Optional for normalization\n</code></pre> <p>Load the audio tokenizer:</p> <pre><code>audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B\", \"cuda\")  # Or \"cpu\" if no GPU\n</code></pre>"},{"location":"tutorials/10-tts-spark-models/#speaker-ids","title":"Speaker IDs","text":"<p>The model uses speaker IDs prefixed to the text input to select voices/languages. Available IDs from the examples:</p> <ul> <li>241: Acholi (female)</li> <li>242: Ateso (female)</li> <li>243: Runyankore (female)</li> <li>245: Lugbara (female)</li> <li>246: Swahili (male)</li> <li>248: Luganda (female)</li> </ul> <p>Prefix the ID to your text, e.g., \"248: Hello\" for Luganda female voice.</p>"},{"location":"tutorials/10-tts-spark-models/#generating-speech","title":"Generating Speech","text":"<p>Use the provided function to generate speech from text. It handles prompt formatting, token generation, extraction of semantic/global tokens, and decoding to waveform.</p> <pre><code>@torch.inference_mode()\ndef generate_speech_from_text(\n    text: str,\n    temperature: float = 0.8,\n    top_k: int = 50,\n    top_p: float = 1.0,\n    max_new_audio_tokens: int = 2048,  # Limits audio length; increase for longer text\n    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n) -&gt; np.ndarray:\n    \"\"\"\n    Generates speech audio from text using default voice control parameters.\n\n    Args:\n        text (str): The text input to be converted to speech. Prefix with speaker ID, e.g., \"248: Hello\".\n        temperature (float): Sampling temperature for generation (higher = more diverse).\n        top_k (int): Top-k sampling parameter.\n        top_p (float): Top-p (nucleus) sampling parameter.\n        max_new_audio_tokens (int): Max number of new tokens to generate (limits audio length).\n        device (torch.device): Device to run inference on.\n\n    Returns:\n        np.ndarray: Generated waveform as a NumPy array (sample rate: 16,000 Hz).\n    \"\"\"\n    prompt = \"\".join([\n        \"&lt;|task_tts|&gt;\",\n        \"&lt;|start_content|&gt;\",\n        text,\n        \"&lt;|end_content|&gt;\",\n        \"&lt;|start_global_token|&gt;\"\n    ])\n\n    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        generated_ids = model.generate(\n            **model_inputs,\n            max_new_tokens=max_new_audio_tokens,\n            do_sample=True,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n            eos_token_id=tokenizer.eos_token_id,\n            pad_token_id=tokenizer.pad_token_id\n        )\n\n    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n    predicts_text = tokenizer.batch_decode(generated_ids_trimmed, skip_special_tokens=False)[0]\n\n    # Extract semantic token IDs\n    semantic_matches = re.findall(r\"&lt;\\|bicodec_semantic_(\\d+)\\|&gt;\", predicts_text)\n    if not semantic_matches:\n        raise ValueError(\"No semantic tokens found in the generated output.\")\n    pred_semantic_ids = torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0)\n\n    # Extract global token IDs\n    global_matches = re.findall(r\"&lt;\\|bicodec_global_(\\d+)\\|&gt;\", predicts_text)\n    if not global_matches:\n        pred_global_ids = torch.zeros((1, 1), dtype=torch.long)\n    else:\n        pred_global_ids = torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0)\n\n    # Decode to waveform\n    wav_np = audio_tokenizer.detokenize(\n        pred_global_ids.to(device),\n        pred_semantic_ids.to(device)\n    )\n    return wav_np\n</code></pre>"},{"location":"tutorials/10-tts-spark-models/#examples","title":"Examples","text":"<ol> <li> <p>Short Text:    <pre><code>short_input_text = \"248: Hello\"\nprint(\"Generating short text...\")\nstart_time = time.time()\nwav_np = generate_speech_from_text(short_input_text)\nprint(f\"Generation took {time.time() - start_time:.2f} seconds\")\ndisplay(Audio(wav_np, rate=16000))  # In Jupyter; save to file otherwise\n</code></pre></p> </li> <li> <p>Long Text (English):    <pre><code>long_input_text = \"248: Uganda is named after the Buganda kingdom, which encompasses a large portion of the south, including Kampala, and whose language Luganda is widely spoken\"\nwav_np = generate_speech_from_text(long_input_text)\ndisplay(Audio(wav_np, rate=16000))\n</code></pre></p> </li> <li> <p>Long Text (Luganda):    <pre><code>long_lug_text = \"248: Awo eizaalibwa gwe olasimba kulamusizzaako ne nkwebaza gyonna gy'otuuseeko. ...\"  # Full text from examples\nwav_np = generate_speech_from_text(long_lug_text)\ndisplay(Audio(wav_np, rate=16000))\n</code></pre></p> </li> </ol> <p>To save audio to a file: <pre><code>import soundfile as sf\nsf.write(\"output.wav\", wav_np, 16000)\n</code></pre></p>"},{"location":"tutorials/10-tts-spark-models/#performance-notes","title":"Performance Notes","text":"<ul> <li>Inference Time: Short texts take ~1-2 seconds; longer texts (200+ words) may take 10-30 seconds on GPU.</li> <li>Limitations: </li> <li>Max audio length is controlled by <code>max_new_audio_tokens</code>. Increase for longer outputs, but it may increase memory usage.</li> <li>If no tokens are extracted, check input formatting or model outputs.</li> <li>Audio is at 16 kHz sample rate.</li> <li>Customization: Adjust <code>temperature</code>, <code>top_k</code>, <code>top_p</code> for varied outputs. For volume normalization, use <code>audio_volume_normalize(wav_np)</code>.</li> </ul>"},{"location":"tutorials/10-tts-spark-models/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>CUDA Errors: Ensure CUDA is installed and compatible with PyTorch.</li> <li>Model Download Issues: Verify Hugging Face login and repository access.</li> <li>No Output Tokens: Ensure text is prefixed with a valid speaker ID.</li> <li>Dependencies: If conflicts arise, use a virtual environment (e.g., via <code>venv</code>).</li> </ul>"},{"location":"tutorials/10-tts-spark-models/#resources","title":"Resources","text":"<ul> <li>Audio Tokenizer: https://huggingface.co/unsloth/Spark-TTS-0.5B</li> <li>Customized Model: https://huggingface.co/jq/spark-tts-salt</li> <li>Example Colab: The original notebook is from This link</li> </ul> <p>For contributions or issues, refer to the GitHub repo. This model supports open-source TTS research for underrepresented languages.</p>"},{"location":"tutorials/11-data-loading/","title":"Data Loading","text":"<p>Documentation for the LEB Repository</p>"},{"location":"tutorials/11-data-loading/#introduction","title":"Introduction","text":"<p>The LEB repository provides tools and resources for working with Sunbird African Language Technology (SALT) datasets. This repository facilitates the creation of multilingual datasets, the training and evaluation of multilingual models, and data preprocessing. It includes robust utilities for model training using HuggingFace frameworks, making it a valuable resource for machine translation and natural language processing (NLP) in underrepresented languages.</p>"},{"location":"tutorials/11-data-loading/#key-features","title":"Key Features","text":"<ul> <li>Multilingual dataset handling and preprocessing.</li> <li>Metrics for evaluating NLP models.</li> <li>Utilities for training HuggingFace models.</li> <li>Jupyter notebooks demonstrating use cases.</li> <li>Documentation support via MkDocs.</li> </ul>"},{"location":"tutorials/11-data-loading/#installation","title":"Installation","text":""},{"location":"tutorials/11-data-loading/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed:</p> <ul> <li>Python 3.8 or above.</li> <li>Git for cloning the repository.</li> <li>pip for managing Python packages.</li> </ul>"},{"location":"tutorials/11-data-loading/#steps","title":"Steps","text":"<ol> <li>Clone the repository:</li> </ol> <p><pre><code>git clone https://github.com/jqug/leb.git\ncd leb\n</code></pre>    Later this will just be 'pip install leb'.</p> <ol> <li>Install the required dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Verify installation by running tests:</li> </ol> <pre><code>pytest\n</code></pre>"},{"location":"tutorials/11-data-loading/#getting-started","title":"Getting Started","text":""},{"location":"tutorials/11-data-loading/#loading-a-dataset","title":"Loading a Dataset","text":"<p>To load a dataset using the tools provided in <code>dataset.py</code>:</p> <pre><code>from leb.dataset import create\n\nconfig = {\n    'huggingface_load': [\n        {\n            'path': 'mozilla-foundation/common_voice',\n            'name': 'en'\n        }\n    ],\n    'source': {\n        'language': 'en',\n        'type': 'text',\n        'preprocessing': ['clean_text']\n    },\n    'target': {\n        'language': 'fr',\n        'type': 'text',\n        'preprocessing': ['clean_text']\n    },\n    'shuffle': True\n}\n\ndataset = create(config)\nprint(f\"Dataset created with {len(dataset)} examples.\")\n</code></pre>"},{"location":"tutorials/11-data-loading/#preprocessing-data","title":"Preprocessing Data","text":"<p>Use <code>preprocessing.py</code> for operations like cleaning, augmentation, and formatting:</p> <pre><code>from leb.preprocessing import clean_text, random_case\n\nraw_data = {\"source\": \"Hello, WORLD!\", \"target\": \"Bonjour, MONDE!\"}\ncleaned_data = clean_text(raw_data, \"source\", lower=True)\naugmented_data = random_case(cleaned_data, \"target\")\nprint(augmented_data)\n</code></pre>"},{"location":"tutorials/11-data-loading/#training-a-model","title":"Training a Model","text":"<p>Leverage HuggingFace training utilities:</p> <pre><code>from leb.utils import TrainableM2MForConditionalGeneration\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = TrainableM2MForConditionalGeneration.from_pretrained(\n    \"facebook/nllb-200-distilled-1.3B\")\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./models\",\n    evaluation_strategy=\"steps\",\n    save_steps=500,\n    learning_rate=3e-5,\n    per_device_train_batch_size=16,\n    predict_with_generate=True\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset\n)\n\ntrainer.train()\n</code></pre>"},{"location":"tutorials/11-data-loading/#modules-overview","title":"Modules Overview","text":""},{"location":"tutorials/11-data-loading/#datasetpy","title":"<code>dataset.py</code>","text":""},{"location":"tutorials/11-data-loading/#purpose","title":"Purpose","text":"<p>Handles dataset loading, validation, and conversion tasks.</p>"},{"location":"tutorials/11-data-loading/#key-functions","title":"Key Functions","text":"<ul> <li><code>create(config)</code>:</li> <li>Generates a dataset based on the provided configuration.</li> <li>Example usage:     <pre><code>dataset = create(config)\n</code></pre></li> </ul>"},{"location":"tutorials/11-data-loading/#preprocessingpy","title":"<code>preprocessing.py</code>","text":""},{"location":"tutorials/11-data-loading/#purpose_1","title":"Purpose","text":"<p>Provides tools for cleaning and formatting text and audio data.</p>"},{"location":"tutorials/11-data-loading/#key-functions_1","title":"Key Functions","text":"<ul> <li><code>clean_text</code>:</li> <li>Cleans text by removing noise and standardizing formatting.</li> <li>Example usage:     <pre><code>clean_text({\"source\": \"Noisy DATA!!\"}, \"source\")\n</code></pre></li> <li><code>random_case</code>:</li> <li> <p>Randomizes casing to simulate realistic variability in text data.</p> </li> <li> <p><code>augment_audio_noise</code>:</p> </li> <li>Adds controlled noise to audio samples for robustness.</li> </ul>"},{"location":"tutorials/11-data-loading/#metricspy","title":"<code>metrics.py</code>","text":""},{"location":"tutorials/11-data-loading/#purpose_2","title":"Purpose","text":"<p>Defines evaluation metrics for NLP tasks.</p>"},{"location":"tutorials/11-data-loading/#key-functions_2","title":"Key Functions","text":"<ul> <li><code>multilingual_eval</code>:</li> <li>Computes BLEU and other metrics for multilingual tasks.</li> <li>Example usage:     <pre><code>results = multilingual_eval(preds, \"en\", \"fr\", [metric_bleu], tokenizer)\n</code></pre></li> </ul>"},{"location":"tutorials/11-data-loading/#utilspy","title":"<code>utils.py</code>","text":""},{"location":"tutorials/11-data-loading/#purpose_3","title":"Purpose","text":"<p>Provides utilities for model training, evaluation, and debugging.</p>"},{"location":"tutorials/11-data-loading/#key-classes-and-functions","title":"Key Classes and Functions","text":"<ul> <li><code>TrainableM2MForConditionalGeneration</code>:</li> <li>Customizes training for multilingual translation models.</li> <li> <p>Example usage:     <pre><code>model = TrainableM2MForConditionalGeneration.from_pretrained(checkpoint)\n</code></pre></p> </li> <li> <p><code>ForcedVariableBOSTokenLogitsProcessor</code>:</p> </li> <li>Allows dynamic BOS token adjustments.</li> </ul>"},{"location":"tutorials/11-data-loading/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"tutorials/11-data-loading/#example-1-custom-audio-augmentation","title":"Example 1: Custom Audio Augmentation","text":"<pre><code>from leb.preprocessing import augment_audio_noise\n\naudio_data = {\"source\": np.zeros(16000), \"source.sample_rate\": 16000}\naugmented_audio = augment_audio_noise(audio_data, \"source\")\n</code></pre>"},{"location":"tutorials/11-data-loading/#example-2-evaluation-with-multiple-metrics","title":"Example 2: Evaluation with Multiple Metrics","text":"<pre><code>from leb.metrics import multilingual_eval_fn\n\nmetrics = [evaluate.load(\"sacrebleu\")]\neval_fn = multilingual_eval_fn(eval_dataset, metrics, tokenizer)\nresults = eval_fn(predictions)\n</code></pre>"},{"location":"tutorials/13-diarization/","title":"Speaker Diarization","text":"<p>Speaker Diarization is the process of partitioning an audio stream into homogeneous segments according to the identity of the speaker. It answers the question \"who spoke when?\" in a given audio or video recording. This is a crucial step in many speech processing applications, such as transcription, speaker recognition, and meeting analysis.</p> <p>Speaker Diarization at Sunbird is performed using pyannote's speaker-diarization-3.0 as the main tool for identifying speakers and the text that corresponds to them along with the Sunbird mms that aids in transcription of the text in the audio.</p>"},{"location":"tutorials/13-diarization/#framework","title":"Framework","text":"<p>Setup and Installation</p> <p>The necessary libraries to perform speaker diarization required for efficient execution of the pipeline and determine various metrics are installed and imported.</p> <pre><code>!pip install pyctcdecode\n!pip install kenlm\n!pip install jiwer\n!pip install huggingface-hub\n!pip install transformers\n!pip install pandas\n!pip install pyannote.audio\n!pip install onnxruntime\n</code></pre> <pre><code>import torch\nfrom huggingface_hub import hf_hub_download\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2CTCTokenizer,\n    Wav2Vec2FeatureExtractor,\n    Wav2Vec2Processor,\n    Wav2Vec2ProcessorWithLM,\n    AutomaticSpeechRecognitionPipeline,\n    AutoProcessor\n)\nfrom pyctcdecode import build_ctcdecoder\nfrom jiwer import wer\n\nimport os\nimport csv\nimport pandas as pd\n</code></pre> <p>Loading Models and LM Heads</p> <p>The Sunbird mms is a huggingface repository with a variety of models and adapters optimized for transcription and translation of languages. Currently, the Diarization developed caters for three languages English, Luganda and Acholi.</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlang_config = {\n    \"ach\": \"Sunbird/sunbird-mms\",\n    \"lug\": \"Sunbird/sunbird-mms\",\n    \"eng\": \"Sunbird/sunbird-mms\",\n}\nmodel_id = \"Sunbird/sunbird-mms\"\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id).to(device)\n</code></pre>"},{"location":"tutorials/13-diarization/#processor-setup","title":"Processor Setup","text":"<pre><code>processor = AutoProcessor.from_pretrained(model_id)\ntokenizer = Wav2Vec2CTCTokenizer.from_pretrained(model_id)\n</code></pre>"},{"location":"tutorials/13-diarization/#tokenizer-setup","title":"Tokenizer setup","text":"<pre><code>tokenizer.set_target_lang(\"eng\")\nmodel.load_adapter(\"eng_meta\")\n</code></pre>"},{"location":"tutorials/13-diarization/#feature-extractor-setup","title":"Feature extractor setup","text":"<pre><code>feature_extractor = Wav2Vec2FeatureExtractor(\n    feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True\n)\nprocessor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\nvocab_dict = processor.tokenizer.get_vocab()\nsorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n</code></pre>"},{"location":"tutorials/13-diarization/#language-model-file-setup","title":"Language model file setup","text":"<p>Within the <code>Sunbird/sunbird-mms</code> huggingface repository is a subfolder named <code>language_model</code> containing various language models capable of efficient transcription.</p> <pre><code>lm_file_name = \"eng_3gram.bin\"\nlm_file_subfolder = \"language_model\"\nlm_file = hf_hub_download(\n    repo_id=lang_config[\"eng\"],\n    filename=lm_file_name,\n    subfolder=lm_file_subfolder,\n)\n</code></pre>"},{"location":"tutorials/13-diarization/#decoder-setup-use-kenlm-as-decoder","title":"Decoder setup -&gt; Use KenLM as decoder","text":"<pre><code>decoder = build_ctcdecoder(\n    labels=list(sorted_vocab_dict.keys()),\n    kenlm_model_path=lm_file,\n)\n</code></pre>"},{"location":"tutorials/13-diarization/#use-the-lm-as-the-processor","title":"Use the lm as the Processor","text":"<pre><code>processor_with_lm = Wav2Vec2ProcessorWithLM(\n    feature_extractor=feature_extractor,\n    tokenizer=tokenizer,\n    decoder=decoder,\n)\nfeature_extractor._set_processor_class(\"Wav2Vec2ProcessorWithLM\")\n</code></pre>"},{"location":"tutorials/13-diarization/#asr-pipeline-with-a-chunk-and-stride","title":"ASR Pipeline, with a chunk and stride","text":"<p>The ASR pipeline is initialized with the pretrained <code>Sunbird-mms</code> model, <code>processor_with_lm</code> attributes <code>tokenizer</code>, <code>feature_extractor</code> and <code>decoder</code>, respective device, <code>chunch_length_s</code>, <code>stride_length_s</code> and <code>return_timestamps</code></p> <pre><code>pipe = AutomaticSpeechRecognitionPipeline(\n    model=model,\n    tokenizer=processor_with_lm.tokenizer,    feature_extractor=processor_with_lm.feature_extractor,\n    decoder=processor_with_lm.decoder,\n    device=device,\n    chunk_length_s=10,\n    stride_length_s=(4, 2),\n    return_timestamps=\"word\"\n)\n</code></pre> <p>Performing a transcription</p> <pre><code> transcription = pipe(\"/content/Kibuuka_eng.mp3\")\n</code></pre> <p>The resulting dictionary <code>transcription</code> will contain a <code>text</code> key containing all the transcribed text as well as a <code>chunks</code> containing individual texts along with their time stamps of the format below:</p> <pre><code> {\n    'text' : 'Hello world',\n    'chunks': [\n        {'text': 'Hello','timestamp': (0.5, 1.0)},\n        {'text': 'world','timestamp': (1.5, 2.0)}\n        ]\n}\n</code></pre>"},{"location":"tutorials/13-diarization/#diarization","title":"Diarization","text":"<p>Imports</p> <pre><code>from typing import Optional, Union\nimport numpy as np\nfrom pyannote.audio import Pipeline\nimport librosa\n</code></pre> <p>Loading an audio file</p> <pre><code>SAMPLE_RATE = 16000\n\ndef load_audio(file: str, sr: int = SAMPLE_RATE) -&gt; np.ndarray:\n\n    try:\n        # librosa automatically resamples to the given sample rate (if necessary)\n        # and converts the signal to mono (by averaging channels)\n        audio, _ = librosa.load(file, sr=sr, mono=True, dtype=np.float32)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load audio with librosa: {e}\") from e\n\n    return audio\n</code></pre> <p>The <code>load_audio</code> functions takes an audio file and sampling rate as one of its parameters. The sampling rate used for this Speaker Diarization is 16000. This sampling rate should be the same sampling rate used to transcribe the audio from using the Sunbird mms to ensure consistency with the output.</p> <p>Diarization Pipeline</p> <p>The class <code>Diarization Pipeline</code> is a custom class created to facilitate the diarization task. It initializes with a pretrained model and can be called with an audio file or waveform to perform diarization.</p> <p>It returns a pandas DataFrame with with columns for the segment, label, speaker, start time, and end time of each speaker segment.</p> <pre><code>class DiarizationPipeline:\n    def __init__(\n        self,\n        model_name=\"pyannote/speaker-diarization-3.0\",\n        use_auth_token=None,\n        device: Optional[Union[str, torch.device]] = \"cpu\",\n    ):\n        if isinstance(device, str):\n            device = torch.device(device)\n        self.model = Pipeline.from_pretrained(model_name,\n        use_auth_token=use_auth_token).to(device)\n\n    def __call__(\n        self,\n        audio: Union[str, np.ndarray],\n        min_speakers: Optional[int] = None,\n        max_speakers: Optional[int] = None\n    ) -&gt; pd.DataFrame:\n\n        if isinstance(audio, str):\n            audio = load_audio(audio)\n        audio_data = {\n            'waveform': torch.from_numpy(audio[None, :]),\n            'sample_rate': SAMPLE_RATE\n        }\n        segments = self.model(audio_data, min_speakers=min_speakers, max_speakers=max_speakers)\n        diarize_df = pd.DataFrame(segments.itertracks(yield_label=True), columns=['segment', 'label', 'speaker'])\n        diarize_df['start'] = diarize_df['segment'].apply(lambda x: x.start)\n        diarize_df['end'] = diarize_df['segment'].apply(lambda x: x.end)\n        return diarize_df\n</code></pre> <p>Segment</p> <p>A class to represent a single segment of an audio with start time, end time and speaker label.</p> <p>This class to encapsulates the information about a segment of audio that has been identified during a speaker diarization process, including the time the segment starts, when it ends, and which speaker is speaking.</p> <pre><code>class Segment:\n    def __init__(self, start, end, speaker=None):\n        self.start = start\n        self.end = end\n        self.speaker = speaker\n</code></pre> <p>Assigning Speakers</p> <p>This is the process that involves taking the transcribed chunks and assigning them to the speakers discovered by the Speaker Diarization Pipeline.</p> <p>In this function, timestamps of the different chunks are compared against the start and end times of speakers in the DataFrame returned by the <code>SpeakerDiarization</code> pipeline segments of a transcript are assigned speaker labels based on the overlap between the speech segments and diarization data.</p> <p>The function iterates through segments of a transcript and assigns the speaker labels based on the overlap between the speech segments and the diarization data.</p> <p>In case of no overlap, a the fill_nearest parameter can be set to <code>True</code>, then the function will assign the speakers to segments by finding the closest speaker in time.</p> <p>The function takes parameters:</p> <p><code>diarize_df</code>: a pandas DataFrame returned by the DiarizationPipeline containing the diarization information with columns like <code>start</code>, <code>end</code> and <code>speaker</code></p> <p><code>transcript_result</code>: A dictionary with a key <code>chunks</code> that contains a list of trancript <code>Segments</code> obtained from the ASR pipeline.</p> <p><code>fill_nearest</code>: Default is <code>False</code></p> <p><code>Returns:</code> An updated <code>transcript_result</code> with speakers assigned to each segment in the form:</p> <pre><code>{\n    'text':'Hello World',\n    'chunks':[\n        {'text': 'Hello', 'timestamp': (0.5, 1.0), 'speaker': 0},\n        {'text': 'world', 'timestamp': (1.5, 2.0), 'speaker': 1}\n    ]\n}\n</code></pre> <pre><code>def assign_word_speakers(diarize_df, transcript_result, fill_nearest=False):\n\n    transcript_segments = transcript_result[\"chunks\"]\n\n    for seg in transcript_segments:\n        # Calculate intersection and union between diarization segments and transcript segment\n        diarize_df['intersection'] = np.minimum(diarize_df['end'], seg[\"timestamp\"][1]) - np.maximum(diarize_df['start'], seg[\"timestamp\"][0])\n        diarize_df['union'] = np.maximum(diarize_df['end'], seg[\"timestamp\"][1]) - np.minimum(diarize_df['start'], seg[\"timestamp\"][0])\n\n        # Filter out diarization segments with no overlap if fill_nearest is False\n        if not fill_nearest:\n            dia_tmp = diarize_df[diarize_df['intersection'] &gt; 0]\n        else:\n            dia_tmp = diarize_df\n\n        # If there are overlapping segments, assign the speaker with the greatest overlap\n        if len(dia_tmp) &gt; 0:\n            speaker = dia_tmp.groupby(\"speaker\")[\"intersection\"].sum().sort_values(ascending=False).index[0]\n            seg[\"speaker\"] = speaker\n\n    return transcript_result\n</code></pre> <p>Running the diarization model</p> <pre><code>diarize_model = DiarizationPipeline(use_auth_token=hf_token, device=device)\ndiarize_segments = diarize_model(\"/content/Kibuuka_eng.mp3\", min_speakers=1, max_speakers=2)\n\ndiarize_segments\n</code></pre> <p>Sample Output</p> <p></p> <pre><code>output = assign_word_speakers(diarize_segments, transcription)\noutput\n</code></pre> <p>Sample Output after Assigning Speakers</p> <pre><code>{'text': \"this is the chitaka's podcast my husband and i will be letting in honor life as a couple husband and helper husband and wife as husband and wife marriage is not a new wild you enter into you don't become a new person you come with what you been working on it's easy to go through the first year of your marriage trying to knit pick the shortcomings of your partner now this is our first episode and it's a series of random reflections from our one year in marriage now we hope that as we share experiences and insights on our journey the you will be inspired to pursue the potion and purpose to your marriage so this is the chitaka'spodcast and these are random reflections when you are married\",\n 'chunks': [{'text': 'this',\n   'timestamp': (2.42, 2.58),\n   'speaker': 'SPEAKER_01'},\n  {'text': 'is', 'timestamp': (2.68, 2.72), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (2.78, 2.84), 'speaker': 'SPEAKER_01'},\n  {'text': \"chitaka's\", 'timestamp': (2.9, 3.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'podcast', 'timestamp': (3.38, 3.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'my', 'timestamp': (4.4, 4.48), 'speaker': 'SPEAKER_01'},\n  {'text': 'husband', 'timestamp': (4.52, 4.72), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (4.8, 4.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'i', 'timestamp': (4.96, 4.98), 'speaker': 'SPEAKER_01'},\n  {'text': 'will', 'timestamp': (5.1, 5.22), 'speaker': 'SPEAKER_01'},\n  {'text': 'be', 'timestamp': (5.28, 5.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'letting', 'timestamp': (5.38, 5.64), 'speaker': 'SPEAKER_01'},\n  {'text': 'in', 'timestamp': (5.82, 5.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'honor', 'timestamp': (6.06, 6.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'life', 'timestamp': (6.42, 6.7), 'speaker': 'SPEAKER_01'},\n  {'text': 'as', 'timestamp': (6.82, 6.9), 'speaker': 'SPEAKER_01'},\n  {'text': 'a', 'timestamp': (6.98, 7.0), 'speaker': 'SPEAKER_01'},\n  {'text': 'couple', 'timestamp': (7.14, 7.52), 'speaker': 'SPEAKER_01'},\n  {'text': 'husband', 'timestamp': (8.06, 8.36), 'speaker': 'SPEAKER_00'},\n  {'text': 'and', 'timestamp': (8.44, 8.5), 'speaker': 'SPEAKER_00'},\n  {'text': 'helper', 'timestamp': (8.64, 9.02), 'speaker': 'SPEAKER_00'},\n  {'text': 'husband', 'timestamp': (9.36, 9.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (9.76, 9.84), 'speaker': 'SPEAKER_01'},\n  {'text': 'wife', 'timestamp': (9.94, 10.3), 'speaker': 'SPEAKER_01'},\n  {'text': 'as', 'timestamp': (11.06, 11.14), 'speaker': 'SPEAKER_01'},\n  {'text': 'husband', 'timestamp': (11.24, 11.56), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (11.62, 11.7), 'speaker': 'SPEAKER_01'},\n  {'text': 'wife', 'timestamp': (11.76, 12.04), 'speaker': 'SPEAKER_01'},\n  {'text': 'marriage', 'timestamp': (12.48, 12.82), 'speaker': 'SPEAKER_00'},\n  {'text': 'is', 'timestamp': (12.88, 12.94), 'speaker': 'SPEAKER_00'},\n  {'text': 'not', 'timestamp': (13.12, 13.48), 'speaker': 'SPEAKER_00'},\n  {'text': 'a', 'timestamp': (13.78, 13.8), 'speaker': 'SPEAKER_00'},\n  {'text': 'new', 'timestamp': (13.92, 14.06), 'speaker': 'SPEAKER_00'},\n  {'text': 'wild', 'timestamp': (14.16, 14.42), 'speaker': 'SPEAKER_00'},\n  {'text': 'you', 'timestamp': (14.5, 14.56), 'speaker': 'SPEAKER_00'},\n  {'text': 'enter', 'timestamp': (14.64, 14.82), 'speaker': 'SPEAKER_00'},\n  {'text': 'into', 'timestamp': (14.94, 15.2), 'speaker': 'SPEAKER_00'},\n  {'text': 'you', 'timestamp': (15.38, 15.44), 'speaker': 'SPEAKER_00'},\n  {'text': \"don't\", 'timestamp': (15.5, 15.64), 'speaker': 'SPEAKER_00'},\n  {'text': 'become', 'timestamp': (15.74, 15.98), 'speaker': 'SPEAKER_00'},\n  {'text': 'a', 'timestamp': (16.06, 16.08), 'speaker': 'SPEAKER_00'},\n  {'text': 'new', 'timestamp': (16.18, 16.28), 'speaker': 'SPEAKER_00'},\n  {'text': 'person', 'timestamp': (16.42, 16.86), 'speaker': 'SPEAKER_00'},\n  {'text': 'you', 'timestamp': (17.2, 17.26), 'speaker': 'SPEAKER_00'},\n  {'text': 'come', 'timestamp': (17.44, 17.64), 'speaker': 'SPEAKER_00'},\n  {'text': 'with', 'timestamp': (17.72, 17.82), 'speaker': 'SPEAKER_00'},\n  {'text': 'what', 'timestamp': (17.92, 18.02), 'speaker': 'SPEAKER_00'},\n  {'text': 'you', 'timestamp': (18.12, 18.18), 'speaker': 'SPEAKER_00'},\n  {'text': 'been', 'timestamp': (18.34, 18.46), 'speaker': 'SPEAKER_00'},\n  {'text': 'working', 'timestamp': (18.54, 18.86), 'speaker': 'SPEAKER_00'},\n  {'text': 'on', 'timestamp': (18.96, 19.12), 'speaker': 'SPEAKER_00'},\n  {'text': \"it's\", 'timestamp': (19.42, 19.52), 'speaker': 'SPEAKER_01'},\n  {'text': 'easy', 'timestamp': (19.64, 19.78), 'speaker': 'SPEAKER_01'},\n  {'text': 'to', 'timestamp': (19.9, 19.96), 'speaker': 'SPEAKER_01'},\n  {'text': 'go', 'timestamp': (20.12, 20.16), 'speaker': 'SPEAKER_01'},\n  {'text': 'through', 'timestamp': (20.36, 20.62), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (21.32, 21.38), 'speaker': 'SPEAKER_01'},\n  {'text': 'first', 'timestamp': (21.44, 21.64), 'speaker': 'SPEAKER_01'},\n  {'text': 'year', 'timestamp': (21.7, 21.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'of', 'timestamp': (21.86, 21.9), 'speaker': 'SPEAKER_01'},\n  {'text': 'your', 'timestamp': (21.96, 22.08), 'speaker': 'SPEAKER_01'},\n  {'text': 'marriage', 'timestamp': (22.14, 22.42), 'speaker': 'SPEAKER_01'},\n  {'text': 'trying', 'timestamp': (22.54, 22.74), 'speaker': 'SPEAKER_01'},\n  {'text': 'to', 'timestamp': (22.84, 22.88), 'speaker': 'SPEAKER_01'},\n  {'text': 'knit', 'timestamp': (23.2, 23.42), 'speaker': 'SPEAKER_01'},\n  {'text': 'pick', 'timestamp': (23.6, 23.78), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (24.58, 24.64), 'speaker': 'SPEAKER_01'},\n  {'text': 'shortcomings', 'timestamp': (24.7, 25.2), 'speaker': 'SPEAKER_01'},\n  {'text': 'of', 'timestamp': (25.26, 25.3), 'speaker': 'SPEAKER_01'},\n  {'text': 'your', 'timestamp': (25.36, 25.46), 'speaker': 'SPEAKER_01'},\n  {'text': 'partner', 'timestamp': (25.52, 25.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'now', 'timestamp': (26.28, 26.38), 'speaker': 'SPEAKER_01'},\n  {'text': 'this', 'timestamp': (26.46, 26.54), 'speaker': 'SPEAKER_01'},\n  {'text': 'is', 'timestamp': (26.62, 26.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'our', 'timestamp': (26.74, 26.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'first', 'timestamp': (26.92, 27.12), 'speaker': 'SPEAKER_01'},\n  {'text': 'episode', 'timestamp': (27.24, 27.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (27.82, 28.04), 'speaker': 'SPEAKER_01'},\n  {'text': \"it's\", 'timestamp': (28.48, 28.6), 'speaker': 'SPEAKER_01'},\n  {'text': 'a', 'timestamp': (28.66, 28.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'series', 'timestamp': (28.74, 28.96), 'speaker': 'SPEAKER_01'},\n  {'text': 'of', 'timestamp': (29.0, 29.04), 'speaker': 'SPEAKER_01'},\n  {'text': 'random', 'timestamp': (29.14, 29.4), 'speaker': 'SPEAKER_01'},\n  {'text': 'reflections', 'timestamp': (29.5, 30.04), 'speaker': 'SPEAKER_01'},\n  {'text': 'from', 'timestamp': (30.2, 30.3), 'speaker': 'SPEAKER_01'},\n  {'text': 'our', 'timestamp': (30.38, 30.52), 'speaker': 'SPEAKER_01'},\n  {'text': 'one', 'timestamp': (30.7, 30.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'year', 'timestamp': (30.9, 31.08), 'speaker': 'SPEAKER_01'},\n  {'text': 'in', 'timestamp': (31.26, 31.34), 'speaker': 'SPEAKER_01'},\n  {'text': 'marriage', 'timestamp': (31.44, 31.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'now', 'timestamp': (31.92, 32.02), 'speaker': 'SPEAKER_01'},\n  {'text': 'we', 'timestamp': (32.14, 32.22), 'speaker': 'SPEAKER_01'},\n  {'text': 'hope', 'timestamp': (32.36, 32.54), 'speaker': 'SPEAKER_01'},\n  {'text': 'that', 'timestamp': (32.66, 32.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'as', 'timestamp': (32.96, 33.02), 'speaker': 'SPEAKER_01'},\n  {'text': 'we', 'timestamp': (33.08, 33.14), 'speaker': 'SPEAKER_01'},\n  {'text': 'share', 'timestamp': (33.24, 33.44), 'speaker': 'SPEAKER_01'},\n  {'text': 'experiences',\n   'timestamp': (33.58, 34.14),\n   'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (34.2, 34.26), 'speaker': 'SPEAKER_01'},\n  {'text': 'insights', 'timestamp': (34.34, 34.74), 'speaker': 'SPEAKER_01'},\n  {'text': 'on', 'timestamp': (34.9, 34.98), 'speaker': 'SPEAKER_01'},\n  {'text': 'our', 'timestamp': (35.06, 35.16), 'speaker': 'SPEAKER_01'},\n  {'text': 'journey', 'timestamp': (35.22, 35.54), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (36.0, 36.08), 'speaker': 'SPEAKER_01'},\n  {'text': 'you', 'timestamp': (36.22, 36.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'will', 'timestamp': (36.44, 36.56), 'speaker': 'SPEAKER_01'},\n  {'text': 'be', 'timestamp': (36.64, 36.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'inspired', 'timestamp': (36.76, 37.24), 'speaker': 'SPEAKER_01'},\n  {'text': 'to', 'timestamp': (37.6, 37.64), 'speaker': 'SPEAKER_01'},\n  {'text': 'pursue', 'timestamp': (37.7, 37.94), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (38.0, 38.06), 'speaker': 'SPEAKER_01'},\n  {'text': 'potion', 'timestamp': (38.14, 38.46), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (38.5, 38.58), 'speaker': 'SPEAKER_01'},\n  {'text': 'purpose', 'timestamp': (38.66, 39.06), 'speaker': 'SPEAKER_01'},\n  {'text': 'to', 'timestamp': (39.4, 39.46), 'speaker': 'SPEAKER_01'},\n  {'text': 'your', 'timestamp': (39.54, 39.66), 'speaker': 'SPEAKER_01'},\n  {'text': 'marriage', 'timestamp': (39.86, 40.24), 'speaker': 'SPEAKER_01'},\n  {'text': 'so', 'timestamp': (40.82, 40.9), 'speaker': 'SPEAKER_01'},\n  {'text': 'this', 'timestamp': (41.42, 41.6), 'speaker': 'SPEAKER_01'},\n  {'text': 'is', 'timestamp': (41.78, 41.84), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (41.94, 42.0), 'speaker': 'SPEAKER_01'},\n  {'text': \"chitaka'spodcast\",\n   'timestamp': (42.12, 43.16),\n   'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (43.54, 43.62), 'speaker': 'SPEAKER_01'},\n  {'text': 'these', 'timestamp': (43.7, 43.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'are', 'timestamp': (43.94, 44.02), 'speaker': 'SPEAKER_01'},\n  {'text': 'random', 'timestamp': (44.1, 44.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'reflections', 'timestamp': (44.4, 44.88), 'speaker': 'SPEAKER_01'},\n  {'text': 'when', 'timestamp': (45.28, 45.42), 'speaker': 'SPEAKER_01'},\n  {'text': 'you', 'timestamp': (45.48, 45.54), 'speaker': 'SPEAKER_01'},\n  {'text': 'are', 'timestamp': (45.56, 45.62), 'speaker': 'SPEAKER_01'},\n  {'text': 'married', 'timestamp': (45.68, 45.92), 'speaker': 'SPEAKER_01'}]}\n</code></pre>"},{"location":"tutorials/14-diarization-training/","title":"PyanNet Model Training Speaker Diarization","text":"<p>This process highlights the steps taken for Model Training on the CallHome Dataset. For this particular dataset we used the English version of the CallHome Dataset. The Model Training Architecture, Loss Functions, Optimisation Techniques, Data Augmentation and Metrics Used.</p>"},{"location":"tutorials/14-diarization-training/#segmentation-model-configuration-explained","title":"Segmentation Model Configuration Explained","text":""},{"location":"tutorials/14-diarization-training/#overview","title":"Overview","text":""},{"location":"tutorials/14-diarization-training/#model-architecture","title":"Model Architecture","text":"<ul> <li>SegmentationModel: This is a wrapper for the PyanNet segmentation model used for speaker diarization tasks. Inherits from Pretrained model to be compatible with the HF Trainer. Can be used to train segmentation models to be used for the \"SpeakerDiarisation Task\" in pyannote.</li> </ul> <p>Forward</p> <p><code>forward</code>: Forward pass function of the Pretrained Model.</p> <p>Parameters:</p> <p><code>waveforms(torch.tensor)</code> : A tensor containing audio data to be processed by the model and ensures the waveforms parameter is a PyTorch tensor.</p> <p><code>labels</code>: Ground truth labels for Training. Defaults to None.</p> <p><code>nb_speakers</code>: Number of speakers. Defaults to <code>None</code></p> <p>Returns: A dictionary with loss(if predicted) and predictions.</p> <p>Setup loss function</p> <p><code>setup_loss_func</code>: Sets up the loss function especially when using the powerset classes. ie <code>self.specifications.powerset=True</code></p> <p>Segmentation Loss Function</p> <p><code>segmentation_loss</code>: Defines the permutation-invariant segmentation loss. Computes the loss using either <code>nll_loss</code>(negative log likelihood) for <code>powerset</code> or <code>binary_cross_entropy</code></p> <p>Parameters:</p> <p><code>permutated_prediction</code>: Prediction after permutation. Type: <code>torch.Tensor</code></p> <p><code>target</code>: Ground truth labels. Type: <code>torch.Tensor</code></p> <p><code>weight</code>: Type: <code>Optional[torch.Tensor]</code></p> <p>Returns: Permutation-invariant segmentation loss. <code>torch.Tensor</code></p> <p>To pyannote</p> <p><code>to_pyannote_model</code>: Converts the current model to a pyannote segmentation model for use in pyannote pipelines</p> <pre><code>class SegmentationModel(PreTrainedModel):\n    config_class = SegmentationModelConfig\n\n    def __init__(\n        self,\n        config=SegmentationModelConfig(),\n    ):\n        super().__init__(config)\n\n        self.model = PyanNet_nn(sincnet={\"stride\": 10})\n\n        self.weigh_by_cardinality = config.weigh_by_cardinality\n        self.max_speakers_per_frame = config.max_speakers_per_frame\n        self.chunk_duration = config.chunk_duration\n        self.min_duration = config.min_duration\n        self.warm_up = config.warm_up\n        self.max_speakers_per_chunk = config.max_speakers_per_chunk\n\n        self.specifications = Specifications(\n            problem=Problem.MULTI_LABEL_CLASSIFICATION\n            if self.max_speakers_per_frame is None\n            else Problem.MONO_LABEL_CLASSIFICATION,\n            resolution=Resolution.FRAME,\n            duration=self.chunk_duration,\n            min_duration=self.min_duration,\n            warm_up=self.warm_up,\n            classes=[f\"speaker#{i+1}\" for i in range(self.max_speakers_per_chunk)],\n            powerset_max_classes=self.max_speakers_per_frame,\n            permutation_invariant=True,\n        )\n        self.model.specifications = self.specifications\n        self.model.build()\n        self.setup_loss_func()\n\n    def forward(self, waveforms, labels=None, nb_speakers=None):\n\n        prediction = self.model(waveforms.unsqueeze(1))\n        batch_size, num_frames, _ = prediction.shape\n\n        if labels is not None:\n            weight = torch.ones(batch_size, num_frames, 1, device=waveforms.device)\n            warm_up_left = round(self.specifications.warm_up[0] / self.specifications.duration * num_frames)\n            weight[:, :warm_up_left] = 0.0\n            warm_up_right = round(self.specifications.warm_up[1] / self.specifications.duration * num_frames)\n            weight[:, num_frames - warm_up_right :] = 0.0\n\n            if self.specifications.powerset:\n                multilabel = self.model.powerset.to_multilabel(prediction)\n                permutated_target, _ = permutate(multilabel, labels)\n\n                permutated_target_powerset = self.model.powerset.to_powerset(permutated_target.float())\n                loss = self.segmentation_loss(prediction, permutated_target_powerset, weight=weight)\n\n            else:\n                permutated_prediction, _ = permutate(labels, prediction)\n                loss = self.segmentation_loss(permutated_prediction, labels, weight=weight)\n\n            return {\"loss\": loss, \"logits\": prediction}\n\n        return {\"logits\": prediction}\n\n    def setup_loss_func(self):\n        if self.specifications.powerset:\n            self.model.powerset = Powerset(\n                len(self.specifications.classes),\n                self.specifications.powerset_max_classes,\n            )\n\n    def segmentation_loss(\n        self,\n        permutated_prediction: torch.Tensor,\n        target: torch.Tensor,\n        weight: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n\n        if self.specifications.powerset:\n            # `clamp_min` is needed to set non-speech weight to 1.\n            class_weight = torch.clamp_min(self.model.powerset.cardinality, 1.0) if self.weigh_by_cardinality else None\n            seg_loss = nll_loss(\n                permutated_prediction,\n                torch.argmax(target, dim=-1),\n                class_weight=class_weight,\n                weight=weight,\n            )\n        else:\n            seg_loss = binary_cross_entropy(permutated_prediction, target.float(), weight=weight)\n\n        return seg_loss\n\n    @classmethod\n    def from_pyannote_model(cls, pretrained):\n\n        # Initialize model:\n        specifications = copy.deepcopy(pretrained.specifications)\n\n        # Copy pretrained model hyperparameters:\n        chunk_duration = specifications.duration\n        max_speakers_per_frame = specifications.powerset_max_classes\n        weigh_by_cardinality = False\n        min_duration = specifications.min_duration\n        warm_up = specifications.warm_up\n        max_speakers_per_chunk = len(specifications.classes)\n\n        config = SegmentationModelConfig(\n            chunk_duration=chunk_duration,\n            max_speakers_per_frame=max_speakers_per_frame,\n            weigh_by_cardinality=weigh_by_cardinality,\n            min_duration=min_duration,\n            warm_up=warm_up,\n            max_speakers_per_chunk=max_speakers_per_chunk,\n        )\n\n        model = cls(config)\n\n        # Copy pretrained model weights:\n        model.model.hparams = copy.deepcopy(pretrained.hparams)\n        model.model.sincnet = copy.deepcopy(pretrained.sincnet)\n        model.model.sincnet.load_state_dict(pretrained.sincnet.state_dict())\n        model.model.lstm = copy.deepcopy(pretrained.lstm)\n        model.model.lstm.load_state_dict(pretrained.lstm.state_dict())\n        model.model.linear = copy.deepcopy(pretrained.linear)\n        model.model.linear.load_state_dict(pretrained.linear.state_dict())\n        model.model.classifier = copy.deepcopy(pretrained.classifier)\n        model.model.classifier.load_state_dict(pretrained.classifier.state_dict())\n        model.model.activation = copy.deepcopy(pretrained.activation)\n        model.model.activation.load_state_dict(pretrained.activation.state_dict())\n\n        return model\n\n    def to_pyannote_model(self):\n\n        seg_model = PyanNet(sincnet={\"stride\": 10})\n        seg_model.hparams.update(self.model.hparams)\n\n        seg_model.sincnet = copy.deepcopy(self.model.sincnet)\n        seg_model.sincnet.load_state_dict(self.model.sincnet.state_dict())\n\n        seg_model.lstm = copy.deepcopy(self.model.lstm)\n        seg_model.lstm.load_state_dict(self.model.lstm.state_dict())\n\n        seg_model.linear = copy.deepcopy(self.model.linear)\n        seg_model.linear.load_state_dict(self.model.linear.state_dict())\n\n        seg_model.classifier = copy.deepcopy(self.model.classifier)\n        seg_model.classifier.load_state_dict(self.model.classifier.state_dict())\n\n        seg_model.activation = copy.deepcopy(self.model.activation)\n        seg_model.activation.load_state_dict(self.model.activation.state_dict())\n\n        seg_model.specifications = self.specifications\n\n        return seg_model\n</code></pre> <p>Segmentation Model Configuration</p> <ul> <li><code>SegmentationModelConfig</code>Configuration class for the segmentation model, specifying various parameters like chunk duration, maximum speakers per frame, etc.</li> <li>Configuration parameters like chunk duration, number of speakers per chunk/frame, minimum duration, warm-up period, etc.</li> </ul> <pre><code>class SegmentationModelConfig(PretrainedConfig):\n\n    model_type = \"pyannet\"\n\n    def __init__(\n        self,\n        chunk_duration=10,\n        max_speakers_per_frame=2,\n        max_speakers_per_chunk=3,\n        min_duration=None,\n        warm_up=(0.0, 0.0),\n        weigh_by_cardinality=False,\n        **kwargs,\n    ):\n\n        super().__init__(**kwargs)\n        self.chunk_duration = chunk_duration\n        self.max_speakers_per_frame = max_speakers_per_frame\n        self.max_speakers_per_chunk = max_speakers_per_chunk\n        self.min_duration = min_duration\n        self.warm_up = warm_up\n        self.weigh_by_cardinality = weigh_by_cardinality\n        # For now, the model handles only 16000 Hz sampling rate\n        self.sample_rate = 16000\n</code></pre>"},{"location":"tutorials/14-diarization-training/#loss-functions","title":"Loss Functions","text":""},{"location":"tutorials/14-diarization-training/#binary-cross-entropy","title":"Binary Cross-Entropy","text":"<ul> <li>Used when the model does not use the powerset approach.</li> <li>Computes the binary cross-entropy loss between the predicted and actual speaker activity.</li> </ul>"},{"location":"tutorials/14-diarization-training/#negative-log-likelihood-nll-loss","title":"Negative Log-Likelihood (NLL) Loss","text":"<ul> <li>Used when the model uses the powerset approach.</li> <li>Computes the NLL loss considering class weights if specified.</li> </ul>"},{"location":"tutorials/14-diarization-training/#optimization-techniques","title":"Optimization Techniques","text":""},{"location":"tutorials/14-diarization-training/#batch-size","title":"Batch Size","text":"<ul> <li>This refers to the number of samples that you feed into your model at each iteration of the training process. This can be adjusted accordingly to optimise the performance of your model</li> </ul>"},{"location":"tutorials/14-diarization-training/#learning-rate","title":"Learning Rate","text":"<ul> <li>This is an optimization tunning parameter that determines the step-size at each iteration while moving towards a minimum loss function</li> </ul>"},{"location":"tutorials/14-diarization-training/#training-epochs","title":"Training Epochs","text":"<ul> <li>An epoch refers to a complete pass through the entire training dataset. A model is exposed to all the training examples and updates its parametrs basd on the patterns it learns. In our case, we try and iterate and test with 5, 10 and 20 epochs and find that the Diarisation Error Rate remains constant at \"'der': 0.23994926057695026\"</li> </ul>"},{"location":"tutorials/14-diarization-training/#warm-up","title":"Warm-up","text":"<ul> <li>The warm-up period allows the model to adjust at the beginning of each chunk, ensuring the central part of the chunk is more accurate.</li> <li>The warm-up is applied to both the left and right parts of each chunk.</li> </ul>"},{"location":"tutorials/14-diarization-training/#permutation-invariant-training","title":"Permutation-Invariant Training","text":"<ul> <li>This technique permutes predictions and targets to find the optimal alignment, ensuring the loss computation is invariant to the order of speakers.</li> </ul>"},{"location":"tutorials/14-diarization-training/#data-augmentation-methods","title":"Data Augmentation Methods","text":"<ul> <li>For our case this is done using the the DataCollator class. This class is responsible for collecting data and ensuring that the target labels are dynamically padded.</li> <li>Pads the target labels to ensure they have the same shape.</li> <li>Pads with zeros if the number of speakers in a chunk is less than the maximum number of speakers per chunk</li> </ul>"},{"location":"tutorials/14-diarization-training/#preprocessing-steps","title":"Preprocessing Steps","text":"<ul> <li>Preprocessing steps like random overlap and fixed overlap during chunking can be considered a form of augmentation as they provide varied inputs to the model.</li> <li><code>Preprocess</code> class used to handle these preprocessing steps is not detailed here, but it's responsible for preparing the input data.</li> </ul> <pre><code>class Preprocess:\n    def __init__(\n        self,\n        config,\n    ):\n\n        self.chunk_duration = config.chunk_duration\n        self.max_speakers_per_frame = config.max_speakers_per_frame\n        self.max_speakers_per_chunk = config.max_speakers_per_chunk\n        self.min_duration = config.min_duration\n        self.warm_up = config.warm_up\n\n        self.sample_rate = config.sample_rate\n        self.model = SegmentationModel(config).to_pyannote_model()\n\n        # Get the number of frames associated to a chunk:\n        _, self.num_frames_per_chunk, _ = self.model(\n            torch.rand((1, int(self.chunk_duration * self.sample_rate)))\n        ).shape\n\n    def get_labels_in_file(self, file):\n\n\n        file_labels = []\n        for i in range(len(file[\"speakers\"][0])):\n            if file[\"speakers\"][0][i] not in file_labels:\n                file_labels.append(file[\"speakers\"][0][i])\n\n        return file_labels\n\n    def get_segments_in_file(self, file, labels):\n\n\n        file_annotations = []\n\n        for i in range(len(file[\"timestamps_start\"][0])):\n            start_segment = file[\"timestamps_start\"][0][i]\n            end_segment = file[\"timestamps_end\"][0][i]\n            label = labels.index(file[\"speakers\"][0][i])\n            file_annotations.append((start_segment, end_segment, label))\n\n        dtype = [(\"start\", \"&lt;f4\"), (\"end\", \"&lt;f4\"), (\"labels\", \"i1\")]\n\n        annotations = np.array(file_annotations, dtype)\n\n        return annotations\n\n    def get_chunk(self, file, start_time):\n\n\n        sample_rate = file[\"audio\"][0][\"sampling_rate\"]\n\n        assert sample_rate == self.sample_rate\n\n        end_time = start_time + self.chunk_duration\n        start_frame = math.floor(start_time * sample_rate)\n        num_frames_waveform = math.floor(self.chunk_duration * sample_rate)\n        end_frame = start_frame + num_frames_waveform\n\n        waveform = file[\"audio\"][0][\"array\"][start_frame:end_frame]\n\n        labels = self.get_labels_in_file(file)\n\n        file_segments = self.get_segments_in_file(file, labels)\n\n        chunk_segments = file_segments[(file_segments[\"start\"] &lt; end_time) &amp; (file_segments[\"end\"] &gt; start_time)]\n\n        # compute frame resolution:\n        # resolution = self.chunk_duration / self.num_frames_per_chunk\n\n        # discretize chunk annotations at model output resolution\n        step = self.model.receptive_field.step\n        half = 0.5 * self.model.receptive_field.duration\n\n        # discretize chunk annotations at model output resolution\n        start = np.maximum(chunk_segments[\"start\"], start_time) - start_time - half\n        start_idx = np.maximum(0, np.round(start / step)).astype(int)\n\n        # start_idx = np.floor(start / resolution).astype(int)\n        end = np.minimum(chunk_segments[\"end\"], end_time) - start_time - half\n        end_idx = np.round(end / step).astype(int)\n\n        # end_idx = np.ceil(end / resolution).astype(int)\n\n        # get list and number of labels for current scope\n        labels = list(np.unique(chunk_segments[\"labels\"]))\n        num_labels = len(labels)\n        # initial frame-level targets\n        y = np.zeros((self.num_frames_per_chunk, num_labels), dtype=np.uint8)\n\n        # map labels to indices\n        mapping = {label: idx for idx, label in enumerate(labels)}\n\n        for start, end, label in zip(start_idx, end_idx, chunk_segments[\"labels\"]):\n            mapped_label = mapping[label]\n            y[start : end + 1, mapped_label] = 1\n\n        return waveform, y, labels\n\n    def get_start_positions(self, file, overlap, random=False):\n\n        sample_rate = file[\"audio\"][0][\"sampling_rate\"]\n\n        assert sample_rate == self.sample_rate\n\n        file_duration = len(file[\"audio\"][0][\"array\"]) / sample_rate\n        start_positions = np.arange(0, file_duration - self.chunk_duration, self.chunk_duration * (1 - overlap))\n\n        if random:\n            nb_samples = int(file_duration / self.chunk_duration)\n            start_positions = np.random.uniform(0, file_duration, nb_samples)\n\n        return start_positions\n\n    def __call__(self, file, random=False, overlap=0.0):\n\n        new_batch = {\"waveforms\": [], \"labels\": [], \"nb_speakers\": []}\n\n        if random:\n            start_positions = self.get_start_positions(file, overlap, random=True)\n        else:\n            start_positions = self.get_start_positions(file, overlap)\n\n        for start_time in start_positions:\n            waveform, target, label = self.get_chunk(file, start_time)\n\n            new_batch[\"waveforms\"].append(waveform)\n            new_batch[\"labels\"].append(target)\n            new_batch[\"nb_speakers\"].append(label)\n\n        return new_batch\n</code></pre>"},{"location":"tutorials/14-diarization-training/#metrics-and-trainer","title":"Metrics and Trainer","text":"<ul> <li>Initializes the Metrics class for evaluation.</li> <li>Configures the Trainer with the model, training arguments, datasets, data collator, and metrics.</li> <li>For the metrics we have the Diarisation Error Rate(DER), FalseAlarm Rate, MissedDetectionRate and the SpeakerConfusionRate with the implementation in the metrics class below.</li> </ul> <pre><code>import numpy as np\nimport torch\nfrom pyannote.audio.torchmetrics import (DiarizationErrorRate, FalseAlarmRate,\n                                         MissedDetectionRate,\n                                         SpeakerConfusionRate)\nfrom pyannote.audio.utils.powerset import Powerset\n\n\nclass Metrics:\n    \"\"\"Metric class used by the HF trainer to compute speaker diarization metrics.\"\"\"\n\n    def __init__(self, specifications) -&gt; None:\n        \"\"\"init method\n\n        Args:\n            specifications (_type_): specifications attribute from a SegmentationModel.\n        \"\"\"\n        self.powerset = specifications.powerset\n        self.classes = specifications.classes\n        self.powerset_max_classes = specifications.powerset_max_classes\n\n        self.model_powerset = Powerset(\n            len(self.classes),\n            self.powerset_max_classes,\n        )\n\n        self.metrics = {\n            \"der\": DiarizationErrorRate(0.5),\n            \"confusion\": SpeakerConfusionRate(0.5),\n            \"missed_detection\": MissedDetectionRate(0.5),\n            \"false_alarm\": FalseAlarmRate(0.5),\n        }\n\n    def __call__(self, eval_pred):\n\n        logits, labels = eval_pred\n\n        if self.powerset:\n            predictions = self.model_powerset.to_multilabel(torch.tensor(logits))\n        else:\n            predictions = torch.tensor(logits)\n\n        labels = torch.tensor(labels)\n\n        predictions = torch.transpose(predictions, 1, 2)\n        labels = torch.transpose(labels, 1, 2)\n\n        metrics = {\"der\": 0, \"false_alarm\": 0, \"missed_detection\": 0, \"confusion\": 0}\n\n        metrics[\"der\"] += self.metrics[\"der\"](predictions, labels).cpu().numpy()\n        metrics[\"false_alarm\"] += self.metrics[\"false_alarm\"](predictions, labels).cpu().numpy()\n        metrics[\"missed_detection\"] += self.metrics[\"missed_detection\"](predictions, labels).cpu().numpy()\n        metrics[\"confusion\"] += self.metrics[\"confusion\"](predictions, labels).cpu().numpy()\n\n        return metrics\n\n\nclass DataCollator:\n    \"\"\"Data collator that will dynamically pad the target labels to have max_speakers_per_chunk\"\"\"\n\n    def __init__(self, max_speakers_per_chunk) -&gt; None:\n        self.max_speakers_per_chunk = max_speakers_per_chunk\n\n    def __call__(self, features):\n        \"\"\"_summary_\n\n        Args:\n            features (_type_): _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n\n        batch = {}\n\n        speakers = [f[\"nb_speakers\"] for f in features]\n        labels = [f[\"labels\"] for f in features]\n\n        batch[\"labels\"] = self.pad_targets(labels, speakers)\n\n        batch[\"waveforms\"] = torch.stack([f[\"waveforms\"] for f in features])\n\n        return batch\n\n    def pad_targets(self, labels, speakers):\n        \"\"\"\n        labels:\n        speakers:\n\n        Returns:\n            _type_:\n                Collated target tensor of shape (num_frames, self.max_speakers_per_chunk)\n                If one chunk has more than max_speakers_per_chunk speakers, we keep\n                the max_speakers_per_chunk most talkative ones. If it has less, we pad with\n                zeros (artificial inactive speakers).\n        \"\"\"\n\n        targets = []\n\n        for i in range(len(labels)):\n            label = speakers[i]\n            target = labels[i].numpy()\n            num_speakers = len(label)\n\n            if num_speakers &gt; self.max_speakers_per_chunk:\n                indices = np.argsort(-np.sum(target, axis=0), axis=0)\n                target = target[:, indices[: self.max_speakers_per_chunk]]\n\n            elif num_speakers &lt; self.max_speakers_per_chunk:\n                target = np.pad(\n                    target,\n                    ((0, 0), (0, self.max_speakers_per_chunk - num_speakers)),\n                    mode=\"constant\",\n                )\n\n            targets.append(target)\n\n        return torch.from_numpy(np.stack(targets))\n</code></pre>"},{"location":"tutorials/14-diarization-training/#training-script","title":"Training Script","text":"<ul> <li>The script train_segmentation.py   can be used to pre-process a diarization dataset and subsequently fine-tune the pyannote segmentation model. In the following example, we fine-tuned the segmentation model on the English subset of the CallHome dataset, a conversational dataset between native speakers:</li> </ul> <pre><code>!python3 train_segmentation.py \\\n    --dataset_name=diarizers-community/callhome \\\n    --dataset_config_name=eng \\\n    --split_on_subset=data \\\n    --model_name_or_path=pyannote/segmentation-3.0 \\\n    --output_dir=./speaker-segmentation-fine-tuned-callhome-eng \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate=1e-3 \\\n    --num_train_epochs=20 \\\n    --lr_scheduler_type=cosine \\\n    --per_device_train_batch_size=32 \\\n    --per_device_eval_batch_size=32 \\\n    --evaluation_strategy=epoch \\\n    --save_strategy=epoch \\\n    --preprocessing_num_workers=2 \\\n    --dataloader_num_workers=2 \\\n    --logging_steps=100 \\\n    --load_best_model_at_end \\\n    --push_to_hub\n</code></pre>"},{"location":"tutorials/14-diarization-training/#evaluation-script","title":"Evaluation Script","text":"<p>The script test_segmentation.pycan be used to evaluate a fine-tuned model on a diarization dataset. In the following example, we evaluate the fine-tuned model from the previous step on the test split of the CallHome English dataset:</p> <pre><code>!python3 test_segmentation.py \\\n    --dataset_name=diarizers-community/callhome \\\n    --dataset_config_name=eng \\\n    --split_on_subset=data \\\n    --test_split_name=test \\\n    --model_name_or_path=diarizers-community/speaker-segmentation-fine-tuned-callhome-eng \\\n    --preprocessing_num_workers=2 \\\n    --evaluate_with_pipeline\n</code></pre> <p>Sample Output</p> <p></p>"},{"location":"tutorials/14-diarization-training/#inference-with-pyannote","title":"Inference with Pyannote","text":"<ul> <li>The fine-tuned segmentation model can easily be loaded into the pyannote speaker diarization pipeline for inference. To do so, we load the pre-trained speaker diarization pipeline, and subsequently override the segmentation model with our fine-tuned checkpoint:</li> </ul> <pre><code>from diarizers import SegmentationModel\nfrom pyannote.audio import Pipeline\nfrom datasets import load_dataset\nimport torch\n\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# load the pre-trained pyannote pipeline\npipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\")\npipeline.to(device)\n\n# replace the segmentation model with your fine-tuned one\nmodel = SegmentationModel().from_pretrained(\"diarizers-community/speaker-segmentation-fine-tuned-callhome-jpn\")\nmodel = model.to_pyannote_model()\npipeline._segmentation.model = model.to(device)\n\n# load dataset example\ndataset = load_dataset(\"diarizers-community/callhome\", \"jpn\", split=\"data\")\nsample = dataset[0][\"audio\"]\n\n# pre-process inputs\nsample[\"waveform\"] = torch.from_numpy(sample.pop(\"array\")[None, :]).to(device, dtype=model.dtype)\nsample[\"sample_rate\"] = sample.pop(\"sampling_rate\")\n\n# perform inference\ndiarization = pipeline(sample)\n\n# dump the diarization output to disk using RTTM format\nwith open(\"audio.rttm\", \"w\") as rttm:\n    diarization.write_rttm(rttm)\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/","title":"Evaluating the Fine-Tuned Model","text":"<p>The script <code>test_segmentation.py</code> can be used to evaluate a fine-tuned model on a diarization dataset.</p> <p>In the following example, we evaluate the fine-tuned model from the test split of the CallHome English Dataset.</p> <pre><code>python3 test_segmentation.py \\\n    --dataset_name=diarizers-community/callhome \\\n    --dataset_config_name=eng \\\n    --split_on_subset=data \\\n    --test_split_name=test \\\n    --model_name_or_path=diarizers-community/speaker-segmentation-fine-tuned-callhome-eng \\\n    --preprocessing_num_workers=2 \\\n    --evaluate_with_pipeline \\\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/#sample-output","title":"Sample Output","text":"<p>The output above is the default output that can be obtained using the default Evaluation Script.</p> <p>This documentation further explores the evaluation process adding more to the metrics that can be measured during this process and highlighting the editing.</p> <p>Considering there are many metrics that can be obtained throughout the diarization process as documented in the <code>pyannote.audio.metrics</code> documentation.</p> <p></p> <p>In this documentation, we'll focus on the Segmentation Precision, Segmentation Recall and Identification F1 Score.</p>"},{"location":"tutorials/15-diarization-evaluation/#segmentation-precision-and-recall","title":"Segmentation Precision and Recall","text":"<p>Imports</p> <p><code>Segment</code>: Speaker segmentation is the process of dividing an audio recording into segments based on the changing speakers\u2019 identities. The goal of speaker segmentation is to determine the time boundaries where the speaker changes occur, effectively identifying the points at which one speaker\u2019s speech ends, and another\u2019s begins. That said, a <code>Segment</code> is a data structure with <code>start</code> and <code>end</code> time that will then be placed in a <code>Timeline</code></p> <p><code>Timeline</code>: A data structure containing various segments. Reference timelines are provided in the ground truth and are compared against the predicted timelines to calculate <code>segmentation precision</code> and <code>segmentation recall</code></p> <pre><code>from pyannote.core import SlidingWindow, SlidingWindowFeature, Timeline, Segment\nfrom pyannote.metrics import segmentation, identification\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/#testing-the-segmentation-model","title":"Testing the Segmentation Model","text":"<p>Initialization</p> <p><code>class Test</code> : The Segmentation Model test implementation is carried out within the Test Class found in the <code>Test.py</code> file in <code>src/diarizers</code></p> <p>Parameters</p> <p><code>test_dataset</code>: The test dataset to be used. In this example, it will be the test split on the Callhome English dataset.</p> <p><code>model (SegmentationModel)</code>: The model is the finetuned model trained by the <code>train_segmentation.py</code> script.</p> <p><code>step (float, optional)</code>: Steps between successive generated audio chunks. Defaults to 2.5.</p> <p><code>metrics</code>: For this example, the metrics <code>segmentation_precision</code>,<code>segmentation_recall</code>,<code>recall_value</code>,<code>precision_value</code> and <code>count</code> have been added for the purpose of calculating the segmentation recall and precision of the Segmentation Model.</p> <pre><code>class Test:\n\n    def __init__(self, test_dataset, model, step=2.5):\n\n        self.test_dataset = test_dataset\n        self.model = model\n        (self.device,) = get_devices(needs=1)\n        self.inference = Inference(self.model, step=step, device=self.device)\n\n        self.sample_rate = test_dataset[0][\"audio\"][\"sampling_rate\"]\n\n        # Get the number of frames associated to a chunk:\n        _, self.num_frames, _ = self.inference.model(\n            torch.rand((1, int(self.inference.duration * self.sample_rate))).to(self.device)\n        ).shape\n        # compute frame resolution:\n        self.resolution = self.inference.duration / self.num_frames\n\n        self.metrics = {\n            \"der\": DiarizationErrorRate(0.5).to(self.device),\n            \"confusion\": SpeakerConfusionRate(0.5).to(self.device),\n            \"missed_detection\": MissedDetectionRate(0.5).to(self.device),\n            \"false_alarm\": FalseAlarmRate(0.5).to(self.device),\n            \"segmentation_precision\": segmentation.SegmentationPrecision(),\n            \"segmentation_recall\": segmentation.SegmentationRecall(),\n            \"recall_value\":0,\n            \"precision_value\": 0,\n            \"count\": 0,\n        }\n</code></pre> <p>Predict function</p> <p>This function makes a prediction on a dataset row using pyannote inference object.</p> <pre><code>    def predict(self, file):\n        audio = torch.tensor(file[\"audio\"][\"array\"]).unsqueeze(0).to(torch.float32).to(self.device)\n        sample_rate = file[\"audio\"][\"sampling_rate\"]\n\n        input = {\"waveform\": audio, \"sample_rate\": sample_rate}\n\n        prediction = self.inference(input)\n\n        return prediction\n</code></pre> <p>Compute Ground Truth Function</p> <p>This function converts a dataset row into the suitable format for evaluation as the ground truth.</p> <p><code>Returns</code>: numpy array with shape (num_frames, num_speakers).</p> <pre><code>def compute_gt(self, file):\n\n    audio = torch.tensor(file[\"audio\"][\"array\"]).unsqueeze(0).to(torch.float32)\n    sample_rate = file[\"audio\"][\"sampling_rate\"]\n\n    audio_duration = len(audio[0]) / sample_rate\n    num_frames = int(round(audio_duration / self.resolution))\n\n    labels = list(set(file[\"speakers\"]))\n\n    gt = np.zeros((num_frames, len(labels)), dtype=np.uint8)\n\n    for i in range(len(file[\"timestamps_start\"])):\n        start = file[\"timestamps_start\"][i]\n        end = file[\"timestamps_end\"][i]\n        speaker = file[\"speakers\"][i]\n        start_frame = int(round(start / self.resolution))\n        end_frame = int(round(end / self.resolution))\n        speaker_index = labels.index(speaker)\n\n        gt[start_frame:end_frame, speaker_index] += 1\n\n    return gt\n</code></pre> <p>Convert to Timeline</p> <p>This function creates a <code>Timeline</code> using data and labels passed as parameters and converted into <code>Segments</code>. Required in order to calculate Segmentation Precision and Recall.</p> <pre><code>    def convert_to_timeline(self, data, labels):\n        timeline = Timeline()\n        for speaker_index, label in enumerate(labels):\n            segments = np.where(data[:, speaker_index] == 1)[0]\n            if len(segments) &gt; 0:\n                start = segments[0] * self.resolution\n                end = segments[0] * self.resolution\n                for frame in segments[1:]:\n                    if frame == end / self.resolution + 1:\n                        end += self.resolution\n                    else:\n                        timeline.add(Segment(start, end + self.resolution))\n                        start = frame * self.resolution\n                        end = frame * self.resolution\n                timeline.add(Segment(start, end + self.resolution))\n        return timeline\n</code></pre> <p>Compute Metrics on File</p> <p>Function that computes metrics for a dataset row passed into it. This function is run iteratively until the entire dataset has been processed.</p> <pre><code>    def compute_metrics_on_file(self, file):\n        gt = self.compute_gt(file)\n        prediction = self.predict(file)\n\n        sliding_window = SlidingWindow(start=0, step=self.resolution, duration=self.resolution)\n        labels = list(set(file[\"speakers\"]))\n\n        reference = SlidingWindowFeature(data=gt, labels=labels, sliding_window=sliding_window)\n\n        # Convert to Timeline for SegmentationPrecision\n        reference_timeline = self.convert_to_timeline(gt, labels)\n        prediction_timeline = self.convert_to_timeline(prediction.data, labels)\n\n\n        for window, pred in prediction:\n            reference_window = reference.crop(window, mode=\"center\")\n            common_num_frames = min(self.num_frames, reference_window.shape[0])\n\n            _, ref_num_speakers = reference_window.shape\n            _, pred_num_speakers = pred.shape\n\n            if pred_num_speakers &gt; ref_num_speakers:\n                reference_window = np.pad(reference_window, ((0, 0), (0, pred_num_speakers - ref_num_speakers)))\n            elif ref_num_speakers &gt; pred_num_speakers:\n                pred = np.pad(pred, ((0, 0), (0, ref_num_speakers - pred_num_speakers)))\n\n            pred = torch.tensor(pred[:common_num_frames]).unsqueeze(0).permute(0, 2, 1).to(self.device)\n            target = (torch.tensor(reference_window[:common_num_frames]).unsqueeze(0).permute(0, 2, 1)).to(self.device)\n\n            self.metrics[\"der\"](pred, target)\n            self.metrics[\"false_alarm\"](pred, target)\n            self.metrics[\"missed_detection\"](pred, target)\n            self.metrics[\"confusion\"](pred, target)\n\n\n        # Compute precision\n        self.metrics[\"precision_value\"] += self.metrics[\"segmentation_precision\"](reference_timeline, prediction_timeline)\n        self.metrics[\"recall_value\"] += self.metrics[\"segmentation_recall\"](reference_timeline, prediction_timeline)\n        self.metrics[\"count\"] += 1\n</code></pre> <p>Compute Metrics</p> <p>Using all the functions above, the metrics for the Segmentation Model can then be computed and returned at once as shown below.</p> <p>Further information on metrics that extracted from the Segmentation model can be found here</p> <pre><code>    def compute_metrics(self):\n        \"\"\"Main method, used to compute speaker diarization metrics on test_dataset.\n        Returns:\n            dict: metric values.\n        \"\"\"\n\n        for file in tqdm(self.test_dataset):\n            self.compute_metrics_on_file(file)\n        if self.metrics[\"count\"] != 0:\n            self.metrics[\"precision_value\"] /= self.metrics[\"count\"]\n            self.metrics[\"recall_value\"] /= self.metrics[\"count\"]\n\n        return {\n            \"der\": self.metrics[\"der\"].compute(),\n            \"false_alarm\": self.metrics[\"false_alarm\"].compute(),\n            \"missed_detection\": self.metrics[\"missed_detection\"].compute(),\n            \"confusion\": self.metrics[\"confusion\"].compute(),\n            \"segmentation_precision\": self.metrics[\"precision_value\"],\n            \"segmentation_recall\": self.metrics[\"recall_value\"],\n        }\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/#testing-the-speaker-diarization-with-the-fine-tuned-segmentation-model","title":"Testing the Speaker Diarization (With the Fine-tuned Segmentation Model)","text":"<p>The Fine-tuned segmentation model can be run in the Speaker Diarization Pipeline by calling <code>from_pretrained</code> and overwriting the segmentation model with the fine-tuned model. Code can be found in the <code>Test.py</code> script.</p> <pre><code>pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\")\n        pipeline._segmentation.model = model\n</code></pre> <p>Initialization</p> <p>The class <code>TestPipeline</code> will be implementing and testing the Speaker Diarization Pipeline with the finetuned segmentation model.</p> <p>Parameters</p> <p><code>pipeline</code>: Speaker Diarization pipeline</p> <p><code>test_dataset</code>: Data to be tested. In this example, it is data from the Callhome English dataset.</p> <p><code>metrics</code>: Since <code>pyannote.metrics</code> does not offer Identification F1-score, we'll use the Precision and Recall to calculate the <code>identificationF1Score</code></p> <pre><code>class TestPipeline:\n    def __init__(self, test_dataset, pipeline) -&gt; None:\n\n        self.test_dataset = test_dataset\n\n        (self.device,) = get_devices(needs=1)\n        self.pipeline = pipeline.to(self.device)\n        self.sample_rate = test_dataset[0][\"audio\"][\"sampling_rate\"]\n\n        # Get the number of frames associated to a chunk:\n        _, self.num_frames, _ = self.pipeline._segmentation.model(\n            torch.rand((1, int(self.pipeline._segmentation.duration * self.sample_rate))).to(self.device)\n        ).shape\n        # compute frame resolution:\n        self.resolution = self.pipeline._segmentation.duration / self.num_frames\n\n        self.metrics = {\n            \"der\": diarization.DiarizationErrorRate(),\n            \"identification_precision\": identification.IdentificationPrecision(),\n            \"identification_recall\": identification.IdentificationRecall(),\n            \"identification_f1\": 0,\n\n        }\n</code></pre> <p>Compute Ground Truth</p> <p>Function that reformats the Dataset Row to return the ground truth to be used for evaluation.</p> <p>Parameters</p> <p><code>file</code>: A single Dataset Row</p> <pre><code>def compute_gt(self, file):\n\n    \"\"\"\n    Args:\n        file (_type_): dataset row.\n\n    Returns:\n        gt: numpy array with shape (num_frames, num_speakers).\n    \"\"\"\n\n    audio = torch.tensor(file[\"audio\"][\"array\"]).unsqueeze(0).to(torch.float32)\n    sample_rate = file[\"audio\"][\"sampling_rate\"]\n\n    audio_duration = len(audio[0]) / sample_rate\n    num_frames = int(round(audio_duration / self.resolution))\n\n    labels = list(set(file[\"speakers\"]))\n\n    gt = np.zeros((num_frames, len(labels)), dtype=np.uint8)\n\n    for i in range(len(file[\"timestamps_start\"])):\n        start = file[\"timestamps_start\"][i]\n        end = file[\"timestamps_end\"][i]\n        speaker = file[\"speakers\"][i]\n        start_frame = int(round(start / self.resolution))\n        end_frame = int(round(end / self.resolution))\n        speaker_index = labels.index(speaker)\n\n        gt[start_frame:end_frame, speaker_index] += 1\n\n    return gt\n</code></pre> <p>Predict Function</p> <pre><code>def predict(self, file):\n\n    sample = {}\n    sample[\"waveform\"] = (\n        torch.from_numpy(file[\"audio\"][\"array\"])\n        .to(self.device, dtype=self.pipeline._segmentation.model.dtype)\n        .unsqueeze(0)\n    )\n    sample[\"sample_rate\"] = file[\"audio\"][\"sampling_rate\"]\n\n    prediction = self.pipeline(sample)\n    # print(\"Prediction data: \", prediction.data )\n\n    return prediction\n</code></pre> <p>Compute on File</p> <p>Function that calculates the f1 score of a <code>file</code>(Dataset Row) using the <code>precision</code> and <code>recall</code>. It also calculates the <code>der</code>(Diarization Error rate) and can be edited to extract more evaluation metrics such as <code>Segmentation Purity</code> and <code>Segmentation Coverage</code>.</p> <p>For the purpose of this demonstration, the latter two were not obtained. Details about Segmentation Coverage and Segmentation Purity can be obtained here.</p> <pre><code>def compute_metrics_on_file(self, file):\n\n    pred = self.predict(file)\n    gt = self.compute_gt(file)\n\n    sliding_window = SlidingWindow(start=0, step=self.resolution, duration=self.resolution)\n    gt = SlidingWindowFeature(data=gt, sliding_window=sliding_window)\n\n    gt = self.pipeline.to_annotation(\n        gt,\n        min_duration_on=0.0,\n        min_duration_off=self.pipeline.segmentation.min_duration_off,\n    )\n\n    mapping = {label: expected_label for label, expected_label in zip(gt.labels(), self.pipeline.classes())}\n\n    gt = gt.rename_labels(mapping=mapping)\n\n\n    der = self.metrics[\"der\"](pred, gt)\n    identificationPrecision = self.metrics[\"identification_precision\"](pred, gt)\n    identificationRecall = self.metrics[\"identification_recall\"](pred, gt)\n    identificationF1 = (2 * identificationPrecision * identificationRecall) / (identificationRecall + identificationPrecision)\n\n    return {\"der\": der, \"identificationF1\": identificationF1}\n</code></pre> <p>Compute Metrics</p> <p>This function iteratively calls the <code>compute_metrics_on_file</code> function to perform computation on all the files in the dataset.</p> <p><code>Returns</code>: The average values of the <code>der</code>(diarization error rate) and <code>f1</code>(F1 Score).</p> <pre><code>def compute_metrics(self):\n\n    der = 0\n    f1 = 0\n    for file in tqdm(self.test_dataset):\n        met = self.compute_metrics_on_file(file)\n        der += met[\"der\"]\n        f1 += met[\"identificationF1\"]\n\n    der /= len(self.test_dataset)\n    f1 /= len(self.test_dataset)\n\n    return {\"der\": der, \"identificationF1Score\": f1}\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/#sample-output_1","title":"Sample Output","text":"<p>An example of the output as expected from the edited script.</p> <p></p>"}]}