{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SALT Documentation","text":""},{"location":"#welcome-to-the-salt-project-documentation","title":"Welcome to the SALT project documentation!","text":"<p>This documentation serves as the official guide for the SALT project, which is part of the Sunbird AI Language Projects. The goal of this documentation is to provide you with comprehensive information on how to use the Leb project effectively.</p>"},{"location":"reference/","title":"Reference","text":"<p>This part of the project documentation focuses on an information-oriented approach. Use it as a reference for the technical implementation of the <code>SALT</code> project code.</p>"},{"location":"API/","title":"SUNBIRDAI API","text":"<p>Welcome to the Sunbird AI API documentation. The Sunbird AI API provides you access to Sunbird's language models. The currently supported models are: </p> <ul> <li> <p>Translation (English to Multiple): translate from English to Acholi, Ateso, Luganda, Lugbara and Runyankole.</p> </li> <li> <p>Translation (Multiple to English): translate from the 5 local language above to English.</p> </li> <li> <p>Speech To Text: Convert speech audio to text. Currently the supported languages are (English, Acholi, Ateso, Luganda, Lugbara and Runyankole)</p> </li> </ul>"},{"location":"API/#login-and-signup","title":"Login and Signup","text":"<p>If you don't already have an account, visit the sunbird AI API page here. If You already have an account just proceed by logging in.</p>"},{"location":"API/#logging-in-and-getting-an-access-token","title":"Logging in and getting an access token.","text":"<p>Authentication is done via a Bearer token. After you have created an account and you are logged in just visit the tokens page to get your access token. This is the <code>auth token</code> that is required when making calls to the sunbird AI api.</p> <p>To see the full api endpoint documentations, visit the api docs here.</p>"},{"location":"API/#ai-tasks","title":"AI Tasks","text":"<ul> <li>Use the <code>/tasks/stt</code> endpoint for speech to text inference for one audio file.</li> <li>Use the <code>tasks/nllb-translate</code> endpoint for translation of text input with the NLLB model.</li> <li>Use the <code>/tasks/language_id</code> endpoint for auto language detection of text input.  This endpoint identifies the language of a given text. It supports a limited set  of local languages including Acholi (ach), Ateso (teo), English (eng),Luganda (lug),  Lugbara (lgg), and Runyankole (nyn).</li> <li>Use the <code>/tasks/summarise</code> endpoint for anonymised summarization of text input.  This endpoint does anonymised summarisation of a given text. The text languages supported for now are English (eng) and Luganda (lug).</li> </ul>"},{"location":"API/#getting-started","title":"Getting started","text":"<p>The guides below demonstrate how to make endpoint calls to the api programmatically. Select your programming language of choice to see the example usage.</p>"},{"location":"API/#sunbird-ai-api-tutorial","title":"Sunbird AI API Tutorial","text":"<p>This page describes how to use the Sunbird AI API and includes code samples in Python and Javascript.</p>"},{"location":"API/#part-1-how-to-authenticate","title":"Part 1: How to authenticate","text":"<ol> <li>If you don't already have an account, create one at https://api.sunbird.ai/register and login.</li> <li>Go to the tokens page to get your access token which you'll use to authenticate</li> </ol> <p>Add an <code>.env</code> file in the same directory as the script and define <code>AUTH_TOKEN</code> in it:</p> <pre><code>AUTH_TOKEN=your_token_here\n</code></pre>"},{"location":"API/#part-2-how-to-call-the-translation-endpoint","title":"Part 2: How to call the translation endpoint","text":"<p>Refer to the sample code below. Replace <code>{access_token}</code> with the token you received above.</p> PythonJavascript <p>Install the required dependencies:</p> <pre><code>pip install requests python-dotenv\n</code></pre> <pre><code>import os\nimport requests\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nurl = \"https://api.sunbird.ai/tasks/nllb_translate\"\naccess_token = os.getenv(\"AUTH_TOKEN\")\nheaders = {\n    \"accept\": \"application/json\",\n    \"Authorization\": f\"Bearer {access_token}\",\n    \"Content-Type\": \"application/json\",\n}\n\ndata = {\n    \"source_language\": \"lug\",\n    \"target_language\": \"eng\",\n    \"text\": \"Ekibiina ekiddukanya omuzannyo gw\u2019emisinde mu ggwanga ekya Uganda Athletics Federation kivuddeyo nekitegeeza nga lawundi esooka eyemisinde egisunsulamu abaddusi abanakiika mu mpaka ezenjawulo ebweru w\u2019eggwanga egya National Athletics Trials nga bwegisaziddwamu.\",\n}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(response.json())\n</code></pre> <p>Install the required dependencies:</p> <pre><code>npm install axios dotenv\n</code></pre> <pre><code>const axios = require('axios');\nrequire('dotenv').config();\n\nconst url = \"https://api.sunbird.ai/tasks/nllb_translate\";\nconst accessToken = process.env.AUTH_TOKEN;\n\nconst headers = {\n    \"accept\": \"application/json\",\n    \"Authorization\": `Bearer ${accessToken}`,\n    \"Content-Type\": \"application/json\"\n};\n\nconst data = {\n    source_language: \"lug\",\n    target_language: \"eng\",\n    text: \"Ekibiina ekiddukanya omuzannyo gw\u2019emisinde mu ggwanga ekya Uganda Athletics Federation kivuddeyo nekitegeeza nga lawundi esooka eyemisinde egisunsulamu abaddusi abanakiika mu mpaka ezenjawulo ebweru w\u2019eggwanga egya National Athletics Trials nga bwegisaziddwamu.\"\n};\n\naxios.post(url, data, { headers })\n    .then(response =&gt; {\n        console.log(response.data);\n    })\n    .catch(error =&gt; {\n        console.error(error.response ? error.response.data : error.message);\n    });\n</code></pre> <p>The dictionary below represents the language codes available now for the translate endpoint</p> PythonJavascript <pre><code>language_codes: {\n    \"English\": \"eng\",\n    \"Luganda\": \"lug\",\n    \"Runyankole\": \"nyn\",\n    \"Acholi\": \"ach\",\n    \"Ateso\": \"teo\",\n    \"Lugbara\": \"lgg\"\n}\n</code></pre> <pre><code>const languageCodes = {\n    English: \"eng\",\n    Luganda: \"lug\",\n    Runyankole: \"nyn\",\n    Acholi: \"ach\",\n    Ateso: \"teo\",\n    Lugbara: \"lgg\"\n};\n</code></pre>"},{"location":"API/#part-3-how-to-call-the-speech-to-text-asr-endpoint","title":"Part 3: How to call the speech-to-text (ASR) endpoint","text":"<p>Refer to the sample code below. Replace <code>{access_token}</code> with the token you got from the <code>/auth/token</code> endpoint. And replace <code>/path/to/audio_file</code> with the path to the audio file you want to transcribe and <code>FILE_NAME</code> with audio filename. </p> PythonJavascript <pre><code>import os\nimport requests\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nurl = \"https://api.sunbird.ai/tasks/stt\"\naccess_token = os.getenv(\"AUTH_TOKEN\")\nheaders = {\n    \"accept\": \"application/json\",\n    \"Authorization\": f\"Bearer {access_token}\",\n}\n\nfiles = {\n    \"audio\": (\n        \"FILE_NAME\",\n        open(\"/path/to/audio_file\", \"rb\"),\n        \"audio/mpeg\",\n    ),\n}\ndata = {\n    \"language\": \"lug\",\n    \"adapter\": \"lug\",\n    \"whisper\": True,\n}\n\nresponse = requests.post(url, headers=headers, files=files, data=data)\n\nprint(response.json())\n</code></pre> <p>Install the required dependencies:</p> <pre><code>npm install axios form-data dotenv\n</code></pre> <pre><code>const axios = require('axios');\nconst FormData = require('form-data');\nconst fs = require('fs');\nrequire('dotenv').config();\n\nconst url = \"https://api.sunbird.ai/tasks/stt\";\nconst accessToken = process.env.AUTH_TOKEN;\n\nconst headers = {\n    \"accept\": \"application/json\",\n    \"Authorization\": `Bearer ${accessToken}`\n};\n\n// Create FormData\nconst formData = new FormData();\nformData.append(\"audio\", fs.createReadStream(\"/path/to/audio_file\"), {\n    filename: \"FILE_NAME\",\n    contentType: \"audio/mpeg\"\n});\nformData.append(\"language\", \"lug\");\nformData.append(\"adapter\", \"lug\");\nformData.append(\"whisper\", true);\n\n// Merge headers\nconst requestHeaders = {\n    ...headers,\n    ...formData.getHeaders()\n};\n\n// Send POST request\naxios.post(url, formData, { headers: requestHeaders })\n    .then(response =&gt; {\n        console.log(response.data);\n    })\n    .catch(error =&gt; {\n        console.error(error.response ? error.response.data : error.message);\n    });\n</code></pre>"},{"location":"API/#part-4-how-to-call-the-summary-endpoint","title":"Part 4: How to call the summary endpoint","text":"PythonJavascript <pre><code>import os\n\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nurl = \"https://api.sunbird.ai/tasks/summarise\"\ntoken = os.getenv(\"AUTH_TOKEN\")\nheaders = {\n    \"accept\": \"application/json\",\n    \"Authorization\": f\"Bearer {token}\",\n    \"Content-Type\": \"application/json\",\n}\n\ntext = (\n    \"ndowooza yange ku baana bano abato abatalina tufuna funa ya uganda butuufu \"\n    \"eserbamby omwana oyo bingi bye yeegomba okuva mu buto bwe ate by'atasobola \"\n    \"kwetuusaako bw'afuna mu naawumuwaamagezi nti ekya mazima nze kaboyiaadeyaatei \"\n    \"ebintu kati bisusse mu uganda wano ebyegombebw'omwana by'atasobola kwetuusaako \"\n    \"ng'ate abazadde nabo bambi bwe beetunulamubamufuna mpola tebasobola kulabirira \"\n    \"mwana oyo bintu by'ayagala ekivaamu omwana akemererwan'ayagala omulenzi omulenzi \"\n    \"naye n'atoba okuatejukira ba mbi ba tannategeera bigambo bya kufuna famire fulani \"\n    \"bakola kyagenda layivu n'afuna embuto eky'amazima nze mbadde nsaba be kikwata \"\n    \"govenment sembera embeera etuyisa nnyo abaana ne tubafaako embeera gwe nyiga gwa \"\n    \"omuzadde olina olabirira maama we olina olabirira n'abato kati kano akasuumuseemu \"\n    \"bwe ka kubulako ne keegulirayooba kapalaobakakioba tokyabisobola ne keyiiyabatuyambe \"\n    \"buduufuembeera bagikyusa mu tulemye\"\n)\n\ndata = {\"text\": text}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(response.json())\n</code></pre> <pre><code>const axios = require('axios');\nrequire('dotenv').config();\n\nconst url = \"https://api.sunbird.ai/tasks/summarise\";\nconst token = process.env.AUTH_TOKEN;\n\nconst headers = {\n    \"accept\": \"application/json\",\n    \"Authorization\": `Bearer ${token}`,\n    \"Content-Type\": \"application/json\"\n};\n\nconst text = \n    \"ndowooza yange ku baana bano abato abatalina tufuna funa ya uganda butuufu \" +\n    \"eserbamby omwana oyo bingi bye yeegomba okuva mu buto bwe ate by'atasobola \" +\n    \"kwetuusaako bw'afuna mu naawumuwaamagezi nti ekya mazima nze kaboyiaadeyaatei \" +\n    \"ebintu kati bisusse mu uganda wano ebyegombebw'omwana by'atasobola kwetuusaako \" +\n    \"ng'ate abazadde nabo bambi bwe beetunulamubamufuna mpola tebasobola kulabirira \" +\n    \"mwana oyo bintu by'ayagala ekivaamu omwana akemererwan'ayagala omulenzi omulenzi \" +\n    \"naye n'atoba okuatejukira ba mbi ba tannategeera bigambo bya kufuna famire fulani \" +\n    \"bakola kyagenda layivu n'afuna embuto eky'amazima nze mbadde nsaba be kikwata \" +\n    \"govenment sembera embeera etuyisa nnyo abaana ne tubafaako embeera gwe nyiga gwa \" +\n    \"omuzadde olina olabirira maama we olina olabirira n'abato kati kano akasuumuseemu \" +\n    \"bwe ka kubulako ne keegulirayooba kapalaobakakioba tokyabisobola ne keyiiyabatuyambe \" +\n    \"buduufuembeera bagikyusa mu tulemye\";\n\nconst data = { text };\n\naxios.post(url, data, { headers })\n    .then(response =&gt; {\n        console.log(response.data);\n    })\n    .catch(error =&gt; {\n        console.error(error.response ? error.response.data : error.message);\n    });\n</code></pre>"},{"location":"API/#part-5-how-to-call-the-language_id-endpoint","title":"Part 5: How to call the language_id endpoint","text":"PythonJavascript <pre><code>import os\n\nimport requests\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nurl = \"https://api.sunbird.ai/tasks/language_id\"\ntoken = os.getenv(\"AUTH_TOKEN\")\nheaders = {\n    \"accept\": \"application/json\",\n    \"Authorization\": f\"Bearer {token}\",\n    \"Content-Type\": \"application/json\",\n}\n\ntext = \"ndowooza yange ku baana bano abato abatalina tufuna funa ya uganda butuufu\"\n\ndata = {\"text\": text}\n\nresponse = requests.post(url, headers=headers, json=data)\n\nprint(response.json())\n</code></pre> <pre><code>const axios = require('axios');\nrequire('dotenv').config();\n\nconst url = \"https://api.sunbird.ai/tasks/language_id\";\nconst token = process.env.AUTH_TOKEN;\n\nconst headers = {\n    \"accept\": \"application/json\",\n    \"Authorization\": `Bearer ${token}`,\n    \"Content-Type\": \"application/json\"\n};\n\nconst text = \"ndowooza yange ku baana bano abato abatalina tufuna funa ya uganda butuufu\";\n\nconst data = { text };\n\naxios.post(url, data, { headers })\n    .then(response =&gt; {\n        console.log(response.data);\n    })\n    .catch(error =&gt; {\n        console.error(error.response ? error.response.data : error.message);\n    });\n</code></pre> <p>You can refer to the docs for more info about the endpoints.</p>"},{"location":"API/#feedback-and-questions","title":"Feedback and Questions.","text":"<p>Don't hesitate to leave us any feedback or questions you have by opening an issue in this repo.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"tutorials/01-introduction/","title":"Introduction","text":"<p>Leb, inspired by the Luo word for 'language,' is a project dedicated to the seamless integration of Sunbird AI Language Technology. Our goal is to empower developers to effortlessly create machine learning models for Neural Machine Translation (NMT), Speech-to-Text (STT), Text-to-Speech (TTS), and other language-related applications.</p> <p>By drawing inspiration from the Luo concept of 'language' itself, Project Leb is envisioned as a springboard for connecting ideas and cultures across the Africa's diverse range of tongues and dialects. Just as languages connect people, this technology would connect languages - old and new - through an inclusive platform optimized for integration, accessibility, and human-centric design.</p>"},{"location":"tutorials/02-installation/","title":"Installation","text":"<p>This part of the project documentation focuses on an understanding-oriented approach. You'll get a chance to read about the background of the project, as well as reasoning about how it was implemented.</p> <p>Note: Expand this section by considering the following points:</p> <ul> <li>Give context and background on your library</li> <li>Explain why you created it</li> <li>Provide multiple examples and approaches of how     to work with it</li> <li>Help the reader make connections</li> <li>Avoid writing instructions or technical descriptions     here</li> </ul>"},{"location":"tutorials/03-quick-tour/","title":"03 quick tour","text":"<p>This part of the project documentation focuses on a problem-oriented approach. You'll tackle common tasks that you might have, with the help of the code provided in this project.</p>"},{"location":"tutorials/04-basics/","title":"Basics","text":""},{"location":"tutorials/04-basics/#one-to-multiple-translation-english-text-to-luganda-and-acholi-text","title":"one-to-multiple translation: English text to Luganda and Acholi text","text":"<pre><code>import sys\nsys.path.append('../..')\nimport leb.dataset\nimport leb.utils\nimport yaml\n</code></pre> <p>set up the configs</p> <pre><code>yaml_config = '''\nhuggingface_load:\n  path: Sunbird/salt\n  split: train\n  name: text-all\nsource:\n  type: text\n  language: eng\n  preprocessing:\n      - prefix_target_language\ntarget:\n  type: text\n  language: [lug, ach]\n'''\n\nconfig = yaml.safe_load(yaml_config)\nds = leb.dataset.create(config)\nlist(ds.take(5))\n</code></pre> <p>output</p> <pre><code>[\n  {\n    \"source\": \"&gt;&gt;lug&lt;&lt; Eggplants always grow best under warm conditions.\",\n    \"target\": \"Bbiringanya lubeerera  asinga kukulira mu mbeera ya bugumu\"\n  },\n  {\n    \"source\": \"&gt;&gt;ach&lt;&lt; Eggplants always grow best under warm conditions.\",\n    \"target\": \"Bilinyanya pol kare dongo maber ka lyeto tye\"\n  },\n  {\n    \"source\": \"&gt;&gt;lug&lt;&lt; Farmland is sometimes a challenge to farmers.\",\n    \"target\": \"Ettaka ly'okulimirako n'okulundirako ebiseera ebimu kisoomooza abalimi\"\n  },\n  {\n    \"source\": \"&gt;&gt;ach&lt;&lt; Farmland is sometimes a challenge to farmers.\",\n    \"target\": \"Ngom me pur i kare mukene obedo peko madit bot lupur\"\n  },\n  {\n    \"source\": \"&gt;&gt;lug&lt;&lt; Farmers should be encouraged to grow more coffee.\",\n    \"target\": \"Abalimi balina okukubirizibwa okwongera okulima emmwanyi\"\n  }\n]\n</code></pre> <p>This is how a basic data loader works</p>"},{"location":"tutorials/07-speech-datasets/","title":"Sunbird African Language Technology (SALT) dataset","text":"<p>SALT is a multi-way parallel text and speech corpus of Engish and six languages widely spoken in Uganda and East Africa: <code>Luganda</code>, <code>Lugbara</code>, <code>Acholi</code>, <code>Runyankole</code>, <code>Ateso</code> and <code>Swahili</code>. The core of the dataset is a set of <code>25,000</code> sentences covering a range of topics of local relevance, such as agriculture, health and society. Each sentence is translated into all languages, to support machine translation, and speech recordings are made for approximately <code>5,000</code> of the sentences both by a variety of speakers in natural settings (suitable for ASR) and by professionals in a studio setting (suitable for text-to-speech).</p>"},{"location":"tutorials/07-speech-datasets/#subsets","title":"Subsets","text":"Subset name Contents text-all Text translations of each sentence. multispeaker-<code>{lang}</code> Speech recordings of each sentence, by a variety of speakers in natural settings. studio-<code>{lang}</code> Speech recordings in a studio setting, suitable for text-to-speech. <p>The sentence IDs map across subsets, so that for example the text of a sentence in Acholi can be mapped to the studio recording of that concept being expressed in Swahili. The subsets can therefore be combined to support the training and evaluation of several further tasks, such as speech-to-text translation and speech-to-speech translation.</p>"},{"location":"tutorials/07-speech-datasets/#language-support","title":"Language support","text":"ISO 639-3 Language Translated text Multispeaker speech Studio speech eng English (Ugandan accent) Yes Yes Yes lug Luganda Yes Yes Yes ach Acholi Yes Yes Yes lgg Lugbara Yes Yes Yes teo Ateso Yes Yes Yes nyn Runyankole Yes Yes Yes swa Swahili Yes No Yes ibo Igbo Yes No No"},{"location":"tutorials/07-speech-datasets/#helper-utilities","title":"Helper utilities","text":"<p>Code for convenient experimentation with multilingual models can be found at https://github.com/SunbirdAI/salt. See example notebooks here.</p>"},{"location":"tutorials/07-speech-datasets/#collaborators","title":"Collaborators","text":"<p>This dataset was collected in practical collaboration between Sunbird AI and the Makerere University AI Lab (Ugandan languages) and KenCorpus, Maseno University (Swahili).</p>"},{"location":"tutorials/07-speech-datasets/#reference","title":"Reference","text":"<p>Machine Translation For African Languages: Community Creation Of Datasets And Models In Uganda. Benjamin Akera, Jonathan Mukiibi, Lydia Sanyu Naggayi, Claire Babirye, Isaac Owomugisha, Solomon Nsumba, Joyce Nakatumba-Nabende, Engineer Bainomugisha, Ernest Mwebaze, John Quinn. 3<sup>rd</sup> Workshop on African Natural Language Processing, 2022.</p>"},{"location":"tutorials/08-translation-models/","title":"NLLB-Based Translation Model Training Documentation","text":""},{"location":"tutorials/08-translation-models/#overview","title":"Overview","text":"<p>This documentation describes the training process and configuration for a multilingual translation model based on Facebook's NLLB-200-1.3B architecture. The model supports bidirectional translation between English and several African languages: Acholi, Lugbara, Luganda, Runyankole, and Ateso.</p>"},{"location":"tutorials/08-translation-models/#model-architecture","title":"Model Architecture","text":"<ul> <li>Base Model: facebook/nllb-200-1.3B</li> <li>Model Type: M2M100ForConditionalGeneration</li> <li>Tokenizer: NllbTokenizer</li> <li>Special Adaptations: Custom token mappings for African languages not in the original NLLB vocabulary</li> </ul>"},{"location":"tutorials/08-translation-models/#supported-languages","title":"Supported Languages","text":"ISO 693-3 Language Name eng English ach Acholi lgg Lugbara lug Luganda nyn Runyankole teo Ateso"},{"location":"tutorials/08-translation-models/#training-data","title":"Training Data","text":"<p>The model was trained on a diverse collection of datasets:</p>"},{"location":"tutorials/08-translation-models/#primary-dataset","title":"Primary Dataset","text":"<ul> <li>SALT dataset (Sunbird/salt)</li> </ul>"},{"location":"tutorials/08-translation-models/#additional-external-resources","title":"Additional External Resources","text":"<ol> <li>AI4D dataset</li> <li>FLORES-200</li> <li>LAFAND-MT (English-Luganda and English-Luo combinations)</li> <li>Mozilla Common Voice 110</li> <li>MT560 (Acholi, Luganda, Runyankole variants)</li> <li>Back-translated data:</li> <li>Google Translate based back-translations</li> <li>Language-specific back-translations (Acholi-English, Luganda-English)</li> </ol>"},{"location":"tutorials/08-translation-models/#training-configuration","title":"Training Configuration","text":""},{"location":"tutorials/08-translation-models/#hardware-requirements","title":"Hardware Requirements","text":"<ul> <li>CUDA-capable GPU (recommended)</li> <li>Sufficient RAM for large batch processing</li> </ul>"},{"location":"tutorials/08-translation-models/#key-training-parameters","title":"Key Training Parameters","text":"<pre><code>Effective Batch Size: 3000\nTraining Batch Size: 25\nEvaluation Batch Size: 25\nGradient Accumulation Steps: 120\nLearning Rate: 3.0e-4\nOptimizer: Adafactor\nWeight Decay: 0.01\nMaximum Steps: 1500\nFP16 Training: Enabled\n</code></pre>"},{"location":"tutorials/08-translation-models/#data-preprocessing","title":"Data Preprocessing","text":"<p>The training pipeline includes several preprocessing steps: 1. Text cleaning 2. Target sentence format matching 3. Random case augmentation 4. Character augmentation 5. Dataset-specific tagging (MT560: '', Backtranslation: '')"},{"location":"tutorials/08-translation-models/#model-training-process","title":"Model Training Process","text":""},{"location":"tutorials/08-translation-models/#setup","title":"Setup","text":"<ol> <li> <p>Install required dependencies: <pre><code>pip install peft transformers datasets bitsandbytes accelerate sentencepiece sacremoses wandb\n</code></pre></p> </li> <li> <p>Initialize the tokenizer with custom language mappings: <pre><code>tokenizer = transformers.NllbTokenizer.from_pretrained(\n    'facebook/nllb-200-distilled-1.3B',\n    src_lang='eng_Latn',\n    tgt_lang='eng_Latn')\n</code></pre></p> </li> <li> <p>Configure language code mappings: <pre><code>code_mapping = {\n    'eng': 'eng_Latn',\n    'lug': 'lug_Latn',\n    'ach': 'luo_Latn',\n    'nyn': 'ace_Latn',\n    'teo': 'afr_Latn',\n    'lgg': 'aka_Latn'\n}\n</code></pre></p> </li> </ol>"},{"location":"tutorials/08-translation-models/#training-process","title":"Training Process","text":"<ol> <li>Data Collation</li> <li>Uses DataCollatorForSeq2Seq</li> <li>Handles language code insertion</li> <li> <p>Manages padding and truncation</p> </li> <li> <p>Evaluation Strategy</p> </li> <li>Evaluation every 100 steps</li> <li>Model checkpointing every 100 steps</li> <li>Early stopping with patience of 4</li> <li>BLEU score monitoring</li> </ol>"},{"location":"tutorials/08-translation-models/#evaluation-results","title":"Evaluation Results","text":""},{"location":"tutorials/08-translation-models/#bleu-scores-on-development-set","title":"BLEU Scores on Development Set","text":"Source Target BLEU Score ach eng 28.371 lgg eng 30.450 lug eng 41.978 nyn eng 32.296 teo eng 30.422 eng ach 20.972 eng lgg 22.362 eng lug 30.359 eng nyn 15.305 eng teo 21.391"},{"location":"tutorials/08-translation-models/#usage-example","title":"Usage Example","text":"<pre><code>import transformers\nimport torch\n\ndef translate_text(text, source_language, target_language):\n    tokenizer = transformers.NllbTokenizer.from_pretrained(\n        'Sunbird/translate-nllb-1.3b-salt')\n    model = transformers.M2M100ForConditionalGeneration.from_pretrained(\n        'Sunbird/translate-nllb-1.3b-salt')\n\n    language_tokens = {\n        'eng': 256047,\n        'ach': 256111,\n        'lgg': 256008,\n        'lug': 256110,\n        'nyn': 256002,\n        'teo': 256006,\n    }\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n    inputs['input_ids'][0][0] = language_tokens[source_language]\n\n    translated_tokens = model.to(device).generate(\n        **inputs,\n        forced_bos_token_id=language_tokens[target_language],\n        max_length=100,\n        num_beams=5,\n    )\n\n    return tokenizer.batch_decode(\n        translated_tokens, skip_special_tokens=True)[0]\n</code></pre>"},{"location":"tutorials/08-translation-models/#model-limitations-and-considerations","title":"Model Limitations and Considerations","text":"<ol> <li>Performance varies significantly between language pairs</li> <li>Best results achieved when English is involved (either source or target)</li> <li>Performance may degrade for:</li> <li>Very long sentences</li> <li>Domain-specific terminology</li> <li>Informal or colloquial language</li> </ol>"},{"location":"tutorials/08-translation-models/#future-improvements","title":"Future Improvements","text":"<ol> <li>Expand training data for lower-performing language pairs</li> <li>Implement more robust data augmentation techniques</li> <li>Explore domain adaptation for specific use cases</li> <li>Fine-tune model size and architecture for optimal performance</li> </ol>"},{"location":"tutorials/08-translation-models/#references","title":"References","text":"<ul> <li>NLLB-200 Paper: No Language Left Behind</li> <li>SALT Dataset: Sunbird/salt</li> <li>Model Weights: Sunbird/translate-nllb-1.3b-salt</li> </ul>"},{"location":"tutorials/09-asr-models/","title":"ASR Models","text":""},{"location":"tutorials/09-asr-models/#whisper-large-for-ugandan-languages","title":"Whisper large for Ugandan languages","text":"<p>This model is an adaptation of whisper-large-v2 for the following languages widely spoken in Uganda: Luganda, Acholi, Lugbara, Ateso, Runyankole and English (Ugandan accent).</p>"},{"location":"tutorials/09-asr-models/#training","title":"Training","text":"<p>The model was trained with the SALT dataset, Common Voice (Luganda) and FLEURS datasets. To help with generalisation in practical settings, training used addition of random noise and random downsampling to 8kHz to simulate phone speech.</p>"},{"location":"tutorials/09-asr-models/#usage","title":"Usage","text":"<p>The model is used in a similar way to the base Whisper model. The model will attempt to auto-detect the language and provide a transcription.  However, note that language detection is not always accurate and results may be improved by specifying it instead. The languages in this model are not supported by the base Whisper model, so the format is slightly different:</p> <pre><code>import transformers\nimport datasets\nimport torch\n\nprocessor = transformers.WhisperProcessor.from_pretrained(\n    \"Sunbird/asr-whisper-large-v2-salt\")\nmodel = transformers.WhisperForConditionalGeneration.from_pretrained(\n    \"Sunbird/asr-whisper-large-v2-salt\")\n\nSALT_LANGUAGE_TOKENS_WHISPER = {\n    'eng': 50259,  # English (Ugandan)\n    'ach': 50357,  # Acholi\n    'lgg': 50356,  # Lugbara\n    'lug': 50355,  # Luganda\n    'nyn': 50354,  # Runyankole\n    'teo': 50353,  # Ateso\n}\n\n# Get some test audio\nds = datasets.load_dataset('Sunbird/salt', 'multispeaker-lug', split='test')\naudio = ds[0]['audio']\nsample_rate = ds[0]['sample_rate']\n\n# Specify a language from one of the above.\nlang = 'lug'\n\n# Apply the model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_features = processor(\n    audio, sampling_rate=sample_rate, return_tensors=\"pt\").input_features\ninput_features = input_features.to(device)\npredicted_ids = model.to(device).generate(\n    input_features,\n    # Optionally set language=None here instead to auto-detect.\n    language=processor.tokenizer.decode(SALT_LANGUAGE_TOKENS_WHISPER[lang]),\n    forced_decoder_ids=None)\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n\nprint(transcription)\n# Ekikoola kya kasooli kya kyenvu wabula langi yaakyo etera okuba eya kitaka wansi.\n</code></pre>"},{"location":"tutorials/09-asr-models/#mms-speech-recognition-for-ugandan-languages","title":"MMS speech recognition for Ugandan languages","text":"<p>This is a fine-tuned version of facebook/mms-1b-all for Ugandan languages, trained with the SALT dataset. The languages supported are:</p> code language lug Luganda ach Acholi lgg Lugbara teo Ateso nyn Runyankole eng English (Ugandan) <p>For each  language there are two adapters: one optimised for cases where the speech is only in that language, and another in which code-switching with English is expected.</p>"},{"location":"tutorials/09-asr-models/#usage_1","title":"Usage","text":"<p>Usage is the same as the base model, though with different adapters available.</p> <pre><code>import torch\nimport transformers\nimport datasets\n\n# Available adapters:\n# ['lug', 'lug+eng', 'ach', 'ach+eng', 'lgg', 'lgg+eng',\n#  'nyn', 'nyn+eng', 'teo', 'teo+eng']\nlanguage = 'lug'\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = transformers.Wav2Vec2ForCTC.from_pretrained(\n    'Sunbird/asr-mms-salt').to(device)\nmodel.load_adapter(language)\n\nprocessor = transformers.Wav2Vec2Processor.from_pretrained(\n    'Sunbird/asr-mms-salt')\nprocessor.tokenizer.set_target_lang(language)\n\n# Get some test audio\nds = datasets.load_dataset('Sunbird/salt', 'multispeaker-lug', split='test')\naudio = ds[0]['audio']\nsample_rate = ds[0]['sample_rate']\n\n# Apply the model\ninputs = processor(audio, sampling_rate=sample_rate, return_tensors=\"pt\")\n\nwith torch.no_grad():\n    outputs = model(**inputs.to(device)).logits\n\nids = torch.argmax(outputs, dim=-1)[0]\ntranscription = processor.decode(ids)\n\nprint(transcription)\n# ekikola ky'akasooli kyakyenvu wabula langi yakyo etera okuba eyaakitaka wansi\n</code></pre> <p>The output of this model is unpunctuated and lower case. For applications requiring formatted text, an alternative model is Sunbird/asr-whisper-large-v2-salt.</p>"},{"location":"tutorials/11-data-loading/","title":"Data Loading","text":"<p>Documentation for the LEB Repository</p>"},{"location":"tutorials/11-data-loading/#introduction","title":"Introduction","text":"<p>The LEB repository provides tools and resources for working with Sunbird African Language Technology (SALT) datasets. This repository facilitates the creation of multilingual datasets, the training and evaluation of multilingual models, and data preprocessing. It includes robust utilities for model training using HuggingFace frameworks, making it a valuable resource for machine translation and natural language processing (NLP) in underrepresented languages.</p>"},{"location":"tutorials/11-data-loading/#key-features","title":"Key Features","text":"<ul> <li>Multilingual dataset handling and preprocessing.</li> <li>Metrics for evaluating NLP models.</li> <li>Utilities for training HuggingFace models.</li> <li>Jupyter notebooks demonstrating use cases.</li> <li>Documentation support via MkDocs.</li> </ul>"},{"location":"tutorials/11-data-loading/#installation","title":"Installation","text":""},{"location":"tutorials/11-data-loading/#prerequisites","title":"Prerequisites","text":"<p>Ensure you have the following installed:</p> <ul> <li>Python 3.8 or above.</li> <li>Git for cloning the repository.</li> <li>pip for managing Python packages.</li> </ul>"},{"location":"tutorials/11-data-loading/#steps","title":"Steps","text":"<ol> <li>Clone the repository:</li> </ol> <p><pre><code>git clone https://github.com/jqug/leb.git\ncd leb\n</code></pre>    Later this will just be 'pip install leb'.</p> <ol> <li>Install the required dependencies:</li> </ol> <pre><code>pip install -r requirements.txt\n</code></pre> <ol> <li>Verify installation by running tests:</li> </ol> <pre><code>pytest\n</code></pre>"},{"location":"tutorials/11-data-loading/#getting-started","title":"Getting Started","text":""},{"location":"tutorials/11-data-loading/#loading-a-dataset","title":"Loading a Dataset","text":"<p>To load a dataset using the tools provided in <code>dataset.py</code>:</p> <pre><code>from leb.dataset import create\n\nconfig = {\n    'huggingface_load': [\n        {\n            'path': 'mozilla-foundation/common_voice',\n            'name': 'en'\n        }\n    ],\n    'source': {\n        'language': 'en',\n        'type': 'text',\n        'preprocessing': ['clean_text']\n    },\n    'target': {\n        'language': 'fr',\n        'type': 'text',\n        'preprocessing': ['clean_text']\n    },\n    'shuffle': True\n}\n\ndataset = create(config)\nprint(f\"Dataset created with {len(dataset)} examples.\")\n</code></pre>"},{"location":"tutorials/11-data-loading/#preprocessing-data","title":"Preprocessing Data","text":"<p>Use <code>preprocessing.py</code> for operations like cleaning, augmentation, and formatting:</p> <pre><code>from leb.preprocessing import clean_text, random_case\n\nraw_data = {\"source\": \"Hello, WORLD!\", \"target\": \"Bonjour, MONDE!\"}\ncleaned_data = clean_text(raw_data, \"source\", lower=True)\naugmented_data = random_case(cleaned_data, \"target\")\nprint(augmented_data)\n</code></pre>"},{"location":"tutorials/11-data-loading/#training-a-model","title":"Training a Model","text":"<p>Leverage HuggingFace training utilities:</p> <pre><code>from leb.utils import TrainableM2MForConditionalGeneration\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n\nmodel = TrainableM2MForConditionalGeneration.from_pretrained(\n    \"facebook/nllb-200-distilled-1.3B\")\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./models\",\n    evaluation_strategy=\"steps\",\n    save_steps=500,\n    learning_rate=3e-5,\n    per_device_train_batch_size=16,\n    predict_with_generate=True\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset\n)\n\ntrainer.train()\n</code></pre>"},{"location":"tutorials/11-data-loading/#modules-overview","title":"Modules Overview","text":""},{"location":"tutorials/11-data-loading/#datasetpy","title":"<code>dataset.py</code>","text":""},{"location":"tutorials/11-data-loading/#purpose","title":"Purpose","text":"<p>Handles dataset loading, validation, and conversion tasks.</p>"},{"location":"tutorials/11-data-loading/#key-functions","title":"Key Functions","text":"<ul> <li><code>create(config)</code>:</li> <li>Generates a dataset based on the provided configuration.</li> <li>Example usage:     <pre><code>dataset = create(config)\n</code></pre></li> </ul>"},{"location":"tutorials/11-data-loading/#preprocessingpy","title":"<code>preprocessing.py</code>","text":""},{"location":"tutorials/11-data-loading/#purpose_1","title":"Purpose","text":"<p>Provides tools for cleaning and formatting text and audio data.</p>"},{"location":"tutorials/11-data-loading/#key-functions_1","title":"Key Functions","text":"<ul> <li><code>clean_text</code>:</li> <li>Cleans text by removing noise and standardizing formatting.</li> <li>Example usage:     <pre><code>clean_text({\"source\": \"Noisy DATA!!\"}, \"source\")\n</code></pre></li> <li><code>random_case</code>:</li> <li> <p>Randomizes casing to simulate realistic variability in text data.</p> </li> <li> <p><code>augment_audio_noise</code>:</p> </li> <li>Adds controlled noise to audio samples for robustness.</li> </ul>"},{"location":"tutorials/11-data-loading/#metricspy","title":"<code>metrics.py</code>","text":""},{"location":"tutorials/11-data-loading/#purpose_2","title":"Purpose","text":"<p>Defines evaluation metrics for NLP tasks.</p>"},{"location":"tutorials/11-data-loading/#key-functions_2","title":"Key Functions","text":"<ul> <li><code>multilingual_eval</code>:</li> <li>Computes BLEU and other metrics for multilingual tasks.</li> <li>Example usage:     <pre><code>results = multilingual_eval(preds, \"en\", \"fr\", [metric_bleu], tokenizer)\n</code></pre></li> </ul>"},{"location":"tutorials/11-data-loading/#utilspy","title":"<code>utils.py</code>","text":""},{"location":"tutorials/11-data-loading/#purpose_3","title":"Purpose","text":"<p>Provides utilities for model training, evaluation, and debugging.</p>"},{"location":"tutorials/11-data-loading/#key-classes-and-functions","title":"Key Classes and Functions","text":"<ul> <li><code>TrainableM2MForConditionalGeneration</code>:</li> <li>Customizes training for multilingual translation models.</li> <li> <p>Example usage:     <pre><code>model = TrainableM2MForConditionalGeneration.from_pretrained(checkpoint)\n</code></pre></p> </li> <li> <p><code>ForcedVariableBOSTokenLogitsProcessor</code>:</p> </li> <li>Allows dynamic BOS token adjustments.</li> </ul>"},{"location":"tutorials/11-data-loading/#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"tutorials/11-data-loading/#example-1-custom-audio-augmentation","title":"Example 1: Custom Audio Augmentation","text":"<pre><code>from leb.preprocessing import augment_audio_noise\n\naudio_data = {\"source\": np.zeros(16000), \"source.sample_rate\": 16000}\naugmented_audio = augment_audio_noise(audio_data, \"source\")\n</code></pre>"},{"location":"tutorials/11-data-loading/#example-2-evaluation-with-multiple-metrics","title":"Example 2: Evaluation with Multiple Metrics","text":"<pre><code>from leb.metrics import multilingual_eval_fn\n\nmetrics = [evaluate.load(\"sacrebleu\")]\neval_fn = multilingual_eval_fn(eval_dataset, metrics, tokenizer)\nresults = eval_fn(predictions)\n</code></pre>"},{"location":"tutorials/13-diarization/","title":"Speaker Diarization","text":"<p>Speaker Diarization is the process of partitioning an audio stream into homogeneous segments according to the identity of the speaker. It answers the question \"who spoke when?\" in a given audio or video recording. This is a crucial step in many speech processing applications, such as transcription, speaker recognition, and meeting analysis.</p> <p>Speaker Diarization at Sunbird is performed using pyannote's speaker-diarization-3.0 as the main tool for identifying speakers and the text that corresponds to them along with the Sunbird mms that aids in transcription of the text in the audio.</p>"},{"location":"tutorials/13-diarization/#framework","title":"Framework","text":"<p>Setup and Installation</p> <p>The necessary libraries to perform speaker diarization required for efficient execution of the pipeline and determine various metrics are installed and imported.</p> <pre><code>!pip install pyctcdecode\n!pip install kenlm\n!pip install jiwer\n!pip install huggingface-hub\n!pip install transformers\n!pip install pandas\n!pip install pyannote.audio\n!pip install onnxruntime\n</code></pre> <pre><code>import torch\nfrom huggingface_hub import hf_hub_download\nfrom transformers import (\n    Wav2Vec2ForCTC,\n    Wav2Vec2CTCTokenizer,\n    Wav2Vec2FeatureExtractor,\n    Wav2Vec2Processor,\n    Wav2Vec2ProcessorWithLM,\n    AutomaticSpeechRecognitionPipeline,\n    AutoProcessor\n)\nfrom pyctcdecode import build_ctcdecoder\nfrom jiwer import wer\n\nimport os\nimport csv\nimport pandas as pd\n</code></pre> <p>Loading Models and LM Heads</p> <p>The Sunbird mms is a huggingface repository with a variety of models and adapters optimized for transcription and translation of languages. Currently, the Diarization developed caters for three languages English, Luganda and Acholi.</p> <pre><code>device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nlang_config = {\n    \"ach\": \"Sunbird/sunbird-mms\",\n    \"lug\": \"Sunbird/sunbird-mms\",\n    \"eng\": \"Sunbird/sunbird-mms\",\n}\nmodel_id = \"Sunbird/sunbird-mms\"\nmodel = Wav2Vec2ForCTC.from_pretrained(model_id).to(device)\n</code></pre>"},{"location":"tutorials/13-diarization/#processor-setup","title":"Processor Setup","text":"<pre><code>processor = AutoProcessor.from_pretrained(model_id)\ntokenizer = Wav2Vec2CTCTokenizer.from_pretrained(model_id)\n</code></pre>"},{"location":"tutorials/13-diarization/#tokenizer-setup","title":"Tokenizer setup","text":"<pre><code>tokenizer.set_target_lang(\"eng\")\nmodel.load_adapter(\"eng_meta\")\n</code></pre>"},{"location":"tutorials/13-diarization/#feature-extractor-setup","title":"Feature extractor setup","text":"<pre><code>feature_extractor = Wav2Vec2FeatureExtractor(\n    feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True\n)\nprocessor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\nvocab_dict = processor.tokenizer.get_vocab()\nsorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n</code></pre>"},{"location":"tutorials/13-diarization/#language-model-file-setup","title":"Language model file setup","text":"<p>Within the <code>Sunbird/sunbird-mms</code> huggingface repository is a subfolder named <code>language_model</code> containing various language models capable of efficient transcription.</p> <pre><code>lm_file_name = \"eng_3gram.bin\"\nlm_file_subfolder = \"language_model\"\nlm_file = hf_hub_download(\n    repo_id=lang_config[\"eng\"],\n    filename=lm_file_name,\n    subfolder=lm_file_subfolder,\n)\n</code></pre>"},{"location":"tutorials/13-diarization/#decoder-setup-use-kenlm-as-decoder","title":"Decoder setup -&gt; Use KenLM as decoder","text":"<pre><code>decoder = build_ctcdecoder(\n    labels=list(sorted_vocab_dict.keys()),\n    kenlm_model_path=lm_file,\n)\n</code></pre>"},{"location":"tutorials/13-diarization/#use-the-lm-as-the-processor","title":"Use the lm as the Processor","text":"<pre><code>processor_with_lm = Wav2Vec2ProcessorWithLM(\n    feature_extractor=feature_extractor,\n    tokenizer=tokenizer,\n    decoder=decoder,\n)\nfeature_extractor._set_processor_class(\"Wav2Vec2ProcessorWithLM\")\n</code></pre>"},{"location":"tutorials/13-diarization/#asr-pipeline-with-a-chunk-and-stride","title":"ASR Pipeline, with a chunk and stride","text":"<p>The ASR pipeline is initialized with the pretrained <code>Sunbird-mms</code> model, <code>processor_with_lm</code> attributes <code>tokenizer</code>, <code>feature_extractor</code> and <code>decoder</code>, respective device, <code>chunch_length_s</code>, <code>stride_length_s</code> and <code>return_timestamps</code></p> <pre><code>pipe = AutomaticSpeechRecognitionPipeline(\n    model=model,\n    tokenizer=processor_with_lm.tokenizer,    feature_extractor=processor_with_lm.feature_extractor,\n    decoder=processor_with_lm.decoder,\n    device=device,\n    chunk_length_s=10,\n    stride_length_s=(4, 2),\n    return_timestamps=\"word\"\n)\n</code></pre> <p>Performing a transcription</p> <pre><code> transcription = pipe(\"/content/Kibuuka_eng.mp3\")\n</code></pre> <p>The resulting dictionary <code>transcription</code> will contain a <code>text</code> key containing all the transcribed text as well as a <code>chunks</code> containing individual texts along with their time stamps of the format below:</p> <pre><code> {\n    'text' : 'Hello world',\n    'chunks': [\n        {'text': 'Hello','timestamp': (0.5, 1.0)},\n        {'text': 'world','timestamp': (1.5, 2.0)}\n        ]\n}\n</code></pre>"},{"location":"tutorials/13-diarization/#diarization","title":"Diarization","text":"<p>Imports</p> <pre><code>from typing import Optional, Union\nimport numpy as np\nfrom pyannote.audio import Pipeline\nimport librosa\n</code></pre> <p>Loading an audio file</p> <pre><code>SAMPLE_RATE = 16000\n\ndef load_audio(file: str, sr: int = SAMPLE_RATE) -&gt; np.ndarray:\n\n    try:\n        # librosa automatically resamples to the given sample rate (if necessary)\n        # and converts the signal to mono (by averaging channels)\n        audio, _ = librosa.load(file, sr=sr, mono=True, dtype=np.float32)\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load audio with librosa: {e}\") from e\n\n    return audio\n</code></pre> <p>The <code>load_audio</code> functions takes an audio file and sampling rate as one of its parameters. The sampling rate used for this Speaker Diarization is 16000. This sampling rate should be the same sampling rate used to transcribe the audio from using the Sunbird mms to ensure consistency with the output.</p> <p>Diarization Pipeline</p> <p>The class <code>Diarization Pipeline</code> is a custom class created to facilitate the diarization task. It initializes with a pretrained model and can be called with an audio file or waveform to perform diarization.</p> <p>It returns a pandas DataFrame with with columns for the segment, label, speaker, start time, and end time of each speaker segment.</p> <pre><code>class DiarizationPipeline:\n    def __init__(\n        self,\n        model_name=\"pyannote/speaker-diarization-3.0\",\n        use_auth_token=None,\n        device: Optional[Union[str, torch.device]] = \"cpu\",\n    ):\n        if isinstance(device, str):\n            device = torch.device(device)\n        self.model = Pipeline.from_pretrained(model_name,\n        use_auth_token=use_auth_token).to(device)\n\n    def __call__(\n        self,\n        audio: Union[str, np.ndarray],\n        min_speakers: Optional[int] = None,\n        max_speakers: Optional[int] = None\n    ) -&gt; pd.DataFrame:\n\n        if isinstance(audio, str):\n            audio = load_audio(audio)\n        audio_data = {\n            'waveform': torch.from_numpy(audio[None, :]),\n            'sample_rate': SAMPLE_RATE\n        }\n        segments = self.model(audio_data, min_speakers=min_speakers, max_speakers=max_speakers)\n        diarize_df = pd.DataFrame(segments.itertracks(yield_label=True), columns=['segment', 'label', 'speaker'])\n        diarize_df['start'] = diarize_df['segment'].apply(lambda x: x.start)\n        diarize_df['end'] = diarize_df['segment'].apply(lambda x: x.end)\n        return diarize_df\n</code></pre> <p>Segment</p> <p>A class to represent a single segment of an audio with start time, end time and speaker label.</p> <p>This class to encapsulates the information about a segment of audio that has been identified during a speaker diarization process, including the time the segment starts, when it ends, and which speaker is speaking.</p> <pre><code>class Segment:\n    def __init__(self, start, end, speaker=None):\n        self.start = start\n        self.end = end\n        self.speaker = speaker\n</code></pre> <p>Assigning Speakers</p> <p>This is the process that involves taking the transcribed chunks and assigning them to the speakers discovered by the Speaker Diarization Pipeline.</p> <p>In this function, timestamps of the different chunks are compared against the start and end times of speakers in the DataFrame returned by the <code>SpeakerDiarization</code> pipeline segments of a transcript are assigned speaker labels based on the overlap between the speech segments and diarization data.</p> <p>The function iterates through segments of a transcript and assigns the speaker labels based on the overlap between the speech segments and the diarization data.</p> <p>In case of no overlap, a the fill_nearest parameter can be set to <code>True</code>, then the function will assign the speakers to segments by finding the closest speaker in time.</p> <p>The function takes parameters:</p> <p><code>diarize_df</code>: a pandas DataFrame returned by the DiarizationPipeline containing the diarization information with columns like <code>start</code>, <code>end</code> and <code>speaker</code></p> <p><code>transcript_result</code>: A dictionary with a key <code>chunks</code> that contains a list of trancript <code>Segments</code> obtained from the ASR pipeline.</p> <p><code>fill_nearest</code>: Default is <code>False</code></p> <p><code>Returns:</code> An updated <code>transcript_result</code> with speakers assigned to each segment in the form:</p> <pre><code>{\n    'text':'Hello World',\n    'chunks':[\n        {'text': 'Hello', 'timestamp': (0.5, 1.0), 'speaker': 0},\n        {'text': 'world', 'timestamp': (1.5, 2.0), 'speaker': 1}\n    ]\n}\n</code></pre> <pre><code>def assign_word_speakers(diarize_df, transcript_result, fill_nearest=False):\n\n    transcript_segments = transcript_result[\"chunks\"]\n\n    for seg in transcript_segments:\n        # Calculate intersection and union between diarization segments and transcript segment\n        diarize_df['intersection'] = np.minimum(diarize_df['end'], seg[\"timestamp\"][1]) - np.maximum(diarize_df['start'], seg[\"timestamp\"][0])\n        diarize_df['union'] = np.maximum(diarize_df['end'], seg[\"timestamp\"][1]) - np.minimum(diarize_df['start'], seg[\"timestamp\"][0])\n\n        # Filter out diarization segments with no overlap if fill_nearest is False\n        if not fill_nearest:\n            dia_tmp = diarize_df[diarize_df['intersection'] &gt; 0]\n        else:\n            dia_tmp = diarize_df\n\n        # If there are overlapping segments, assign the speaker with the greatest overlap\n        if len(dia_tmp) &gt; 0:\n            speaker = dia_tmp.groupby(\"speaker\")[\"intersection\"].sum().sort_values(ascending=False).index[0]\n            seg[\"speaker\"] = speaker\n\n    return transcript_result\n</code></pre> <p>Running the diarization model</p> <pre><code>diarize_model = DiarizationPipeline(use_auth_token=hf_token, device=device)\ndiarize_segments = diarize_model(\"/content/Kibuuka_eng.mp3\", min_speakers=1, max_speakers=2)\n\ndiarize_segments\n</code></pre> <p>Sample Output</p> <p></p> <pre><code>output = assign_word_speakers(diarize_segments, transcription)\noutput\n</code></pre> <p>Sample Output after Assigning Speakers</p> <pre><code>{'text': \"this is the chitaka's podcast my husband and i will be letting in honor life as a couple husband and helper husband and wife as husband and wife marriage is not a new wild you enter into you don't become a new person you come with what you been working on it's easy to go through the first year of your marriage trying to knit pick the shortcomings of your partner now this is our first episode and it's a series of random reflections from our one year in marriage now we hope that as we share experiences and insights on our journey the you will be inspired to pursue the potion and purpose to your marriage so this is the chitaka'spodcast and these are random reflections when you are married\",\n 'chunks': [{'text': 'this',\n   'timestamp': (2.42, 2.58),\n   'speaker': 'SPEAKER_01'},\n  {'text': 'is', 'timestamp': (2.68, 2.72), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (2.78, 2.84), 'speaker': 'SPEAKER_01'},\n  {'text': \"chitaka's\", 'timestamp': (2.9, 3.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'podcast', 'timestamp': (3.38, 3.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'my', 'timestamp': (4.4, 4.48), 'speaker': 'SPEAKER_01'},\n  {'text': 'husband', 'timestamp': (4.52, 4.72), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (4.8, 4.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'i', 'timestamp': (4.96, 4.98), 'speaker': 'SPEAKER_01'},\n  {'text': 'will', 'timestamp': (5.1, 5.22), 'speaker': 'SPEAKER_01'},\n  {'text': 'be', 'timestamp': (5.28, 5.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'letting', 'timestamp': (5.38, 5.64), 'speaker': 'SPEAKER_01'},\n  {'text': 'in', 'timestamp': (5.82, 5.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'honor', 'timestamp': (6.06, 6.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'life', 'timestamp': (6.42, 6.7), 'speaker': 'SPEAKER_01'},\n  {'text': 'as', 'timestamp': (6.82, 6.9), 'speaker': 'SPEAKER_01'},\n  {'text': 'a', 'timestamp': (6.98, 7.0), 'speaker': 'SPEAKER_01'},\n  {'text': 'couple', 'timestamp': (7.14, 7.52), 'speaker': 'SPEAKER_01'},\n  {'text': 'husband', 'timestamp': (8.06, 8.36), 'speaker': 'SPEAKER_00'},\n  {'text': 'and', 'timestamp': (8.44, 8.5), 'speaker': 'SPEAKER_00'},\n  {'text': 'helper', 'timestamp': (8.64, 9.02), 'speaker': 'SPEAKER_00'},\n  {'text': 'husband', 'timestamp': (9.36, 9.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (9.76, 9.84), 'speaker': 'SPEAKER_01'},\n  {'text': 'wife', 'timestamp': (9.94, 10.3), 'speaker': 'SPEAKER_01'},\n  {'text': 'as', 'timestamp': (11.06, 11.14), 'speaker': 'SPEAKER_01'},\n  {'text': 'husband', 'timestamp': (11.24, 11.56), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (11.62, 11.7), 'speaker': 'SPEAKER_01'},\n  {'text': 'wife', 'timestamp': (11.76, 12.04), 'speaker': 'SPEAKER_01'},\n  {'text': 'marriage', 'timestamp': (12.48, 12.82), 'speaker': 'SPEAKER_00'},\n  {'text': 'is', 'timestamp': (12.88, 12.94), 'speaker': 'SPEAKER_00'},\n  {'text': 'not', 'timestamp': (13.12, 13.48), 'speaker': 'SPEAKER_00'},\n  {'text': 'a', 'timestamp': (13.78, 13.8), 'speaker': 'SPEAKER_00'},\n  {'text': 'new', 'timestamp': (13.92, 14.06), 'speaker': 'SPEAKER_00'},\n  {'text': 'wild', 'timestamp': (14.16, 14.42), 'speaker': 'SPEAKER_00'},\n  {'text': 'you', 'timestamp': (14.5, 14.56), 'speaker': 'SPEAKER_00'},\n  {'text': 'enter', 'timestamp': (14.64, 14.82), 'speaker': 'SPEAKER_00'},\n  {'text': 'into', 'timestamp': (14.94, 15.2), 'speaker': 'SPEAKER_00'},\n  {'text': 'you', 'timestamp': (15.38, 15.44), 'speaker': 'SPEAKER_00'},\n  {'text': \"don't\", 'timestamp': (15.5, 15.64), 'speaker': 'SPEAKER_00'},\n  {'text': 'become', 'timestamp': (15.74, 15.98), 'speaker': 'SPEAKER_00'},\n  {'text': 'a', 'timestamp': (16.06, 16.08), 'speaker': 'SPEAKER_00'},\n  {'text': 'new', 'timestamp': (16.18, 16.28), 'speaker': 'SPEAKER_00'},\n  {'text': 'person', 'timestamp': (16.42, 16.86), 'speaker': 'SPEAKER_00'},\n  {'text': 'you', 'timestamp': (17.2, 17.26), 'speaker': 'SPEAKER_00'},\n  {'text': 'come', 'timestamp': (17.44, 17.64), 'speaker': 'SPEAKER_00'},\n  {'text': 'with', 'timestamp': (17.72, 17.82), 'speaker': 'SPEAKER_00'},\n  {'text': 'what', 'timestamp': (17.92, 18.02), 'speaker': 'SPEAKER_00'},\n  {'text': 'you', 'timestamp': (18.12, 18.18), 'speaker': 'SPEAKER_00'},\n  {'text': 'been', 'timestamp': (18.34, 18.46), 'speaker': 'SPEAKER_00'},\n  {'text': 'working', 'timestamp': (18.54, 18.86), 'speaker': 'SPEAKER_00'},\n  {'text': 'on', 'timestamp': (18.96, 19.12), 'speaker': 'SPEAKER_00'},\n  {'text': \"it's\", 'timestamp': (19.42, 19.52), 'speaker': 'SPEAKER_01'},\n  {'text': 'easy', 'timestamp': (19.64, 19.78), 'speaker': 'SPEAKER_01'},\n  {'text': 'to', 'timestamp': (19.9, 19.96), 'speaker': 'SPEAKER_01'},\n  {'text': 'go', 'timestamp': (20.12, 20.16), 'speaker': 'SPEAKER_01'},\n  {'text': 'through', 'timestamp': (20.36, 20.62), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (21.32, 21.38), 'speaker': 'SPEAKER_01'},\n  {'text': 'first', 'timestamp': (21.44, 21.64), 'speaker': 'SPEAKER_01'},\n  {'text': 'year', 'timestamp': (21.7, 21.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'of', 'timestamp': (21.86, 21.9), 'speaker': 'SPEAKER_01'},\n  {'text': 'your', 'timestamp': (21.96, 22.08), 'speaker': 'SPEAKER_01'},\n  {'text': 'marriage', 'timestamp': (22.14, 22.42), 'speaker': 'SPEAKER_01'},\n  {'text': 'trying', 'timestamp': (22.54, 22.74), 'speaker': 'SPEAKER_01'},\n  {'text': 'to', 'timestamp': (22.84, 22.88), 'speaker': 'SPEAKER_01'},\n  {'text': 'knit', 'timestamp': (23.2, 23.42), 'speaker': 'SPEAKER_01'},\n  {'text': 'pick', 'timestamp': (23.6, 23.78), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (24.58, 24.64), 'speaker': 'SPEAKER_01'},\n  {'text': 'shortcomings', 'timestamp': (24.7, 25.2), 'speaker': 'SPEAKER_01'},\n  {'text': 'of', 'timestamp': (25.26, 25.3), 'speaker': 'SPEAKER_01'},\n  {'text': 'your', 'timestamp': (25.36, 25.46), 'speaker': 'SPEAKER_01'},\n  {'text': 'partner', 'timestamp': (25.52, 25.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'now', 'timestamp': (26.28, 26.38), 'speaker': 'SPEAKER_01'},\n  {'text': 'this', 'timestamp': (26.46, 26.54), 'speaker': 'SPEAKER_01'},\n  {'text': 'is', 'timestamp': (26.62, 26.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'our', 'timestamp': (26.74, 26.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'first', 'timestamp': (26.92, 27.12), 'speaker': 'SPEAKER_01'},\n  {'text': 'episode', 'timestamp': (27.24, 27.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (27.82, 28.04), 'speaker': 'SPEAKER_01'},\n  {'text': \"it's\", 'timestamp': (28.48, 28.6), 'speaker': 'SPEAKER_01'},\n  {'text': 'a', 'timestamp': (28.66, 28.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'series', 'timestamp': (28.74, 28.96), 'speaker': 'SPEAKER_01'},\n  {'text': 'of', 'timestamp': (29.0, 29.04), 'speaker': 'SPEAKER_01'},\n  {'text': 'random', 'timestamp': (29.14, 29.4), 'speaker': 'SPEAKER_01'},\n  {'text': 'reflections', 'timestamp': (29.5, 30.04), 'speaker': 'SPEAKER_01'},\n  {'text': 'from', 'timestamp': (30.2, 30.3), 'speaker': 'SPEAKER_01'},\n  {'text': 'our', 'timestamp': (30.38, 30.52), 'speaker': 'SPEAKER_01'},\n  {'text': 'one', 'timestamp': (30.7, 30.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'year', 'timestamp': (30.9, 31.08), 'speaker': 'SPEAKER_01'},\n  {'text': 'in', 'timestamp': (31.26, 31.34), 'speaker': 'SPEAKER_01'},\n  {'text': 'marriage', 'timestamp': (31.44, 31.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'now', 'timestamp': (31.92, 32.02), 'speaker': 'SPEAKER_01'},\n  {'text': 'we', 'timestamp': (32.14, 32.22), 'speaker': 'SPEAKER_01'},\n  {'text': 'hope', 'timestamp': (32.36, 32.54), 'speaker': 'SPEAKER_01'},\n  {'text': 'that', 'timestamp': (32.66, 32.82), 'speaker': 'SPEAKER_01'},\n  {'text': 'as', 'timestamp': (32.96, 33.02), 'speaker': 'SPEAKER_01'},\n  {'text': 'we', 'timestamp': (33.08, 33.14), 'speaker': 'SPEAKER_01'},\n  {'text': 'share', 'timestamp': (33.24, 33.44), 'speaker': 'SPEAKER_01'},\n  {'text': 'experiences',\n   'timestamp': (33.58, 34.14),\n   'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (34.2, 34.26), 'speaker': 'SPEAKER_01'},\n  {'text': 'insights', 'timestamp': (34.34, 34.74), 'speaker': 'SPEAKER_01'},\n  {'text': 'on', 'timestamp': (34.9, 34.98), 'speaker': 'SPEAKER_01'},\n  {'text': 'our', 'timestamp': (35.06, 35.16), 'speaker': 'SPEAKER_01'},\n  {'text': 'journey', 'timestamp': (35.22, 35.54), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (36.0, 36.08), 'speaker': 'SPEAKER_01'},\n  {'text': 'you', 'timestamp': (36.22, 36.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'will', 'timestamp': (36.44, 36.56), 'speaker': 'SPEAKER_01'},\n  {'text': 'be', 'timestamp': (36.64, 36.68), 'speaker': 'SPEAKER_01'},\n  {'text': 'inspired', 'timestamp': (36.76, 37.24), 'speaker': 'SPEAKER_01'},\n  {'text': 'to', 'timestamp': (37.6, 37.64), 'speaker': 'SPEAKER_01'},\n  {'text': 'pursue', 'timestamp': (37.7, 37.94), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (38.0, 38.06), 'speaker': 'SPEAKER_01'},\n  {'text': 'potion', 'timestamp': (38.14, 38.46), 'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (38.5, 38.58), 'speaker': 'SPEAKER_01'},\n  {'text': 'purpose', 'timestamp': (38.66, 39.06), 'speaker': 'SPEAKER_01'},\n  {'text': 'to', 'timestamp': (39.4, 39.46), 'speaker': 'SPEAKER_01'},\n  {'text': 'your', 'timestamp': (39.54, 39.66), 'speaker': 'SPEAKER_01'},\n  {'text': 'marriage', 'timestamp': (39.86, 40.24), 'speaker': 'SPEAKER_01'},\n  {'text': 'so', 'timestamp': (40.82, 40.9), 'speaker': 'SPEAKER_01'},\n  {'text': 'this', 'timestamp': (41.42, 41.6), 'speaker': 'SPEAKER_01'},\n  {'text': 'is', 'timestamp': (41.78, 41.84), 'speaker': 'SPEAKER_01'},\n  {'text': 'the', 'timestamp': (41.94, 42.0), 'speaker': 'SPEAKER_01'},\n  {'text': \"chitaka'spodcast\",\n   'timestamp': (42.12, 43.16),\n   'speaker': 'SPEAKER_01'},\n  {'text': 'and', 'timestamp': (43.54, 43.62), 'speaker': 'SPEAKER_01'},\n  {'text': 'these', 'timestamp': (43.7, 43.86), 'speaker': 'SPEAKER_01'},\n  {'text': 'are', 'timestamp': (43.94, 44.02), 'speaker': 'SPEAKER_01'},\n  {'text': 'random', 'timestamp': (44.1, 44.32), 'speaker': 'SPEAKER_01'},\n  {'text': 'reflections', 'timestamp': (44.4, 44.88), 'speaker': 'SPEAKER_01'},\n  {'text': 'when', 'timestamp': (45.28, 45.42), 'speaker': 'SPEAKER_01'},\n  {'text': 'you', 'timestamp': (45.48, 45.54), 'speaker': 'SPEAKER_01'},\n  {'text': 'are', 'timestamp': (45.56, 45.62), 'speaker': 'SPEAKER_01'},\n  {'text': 'married', 'timestamp': (45.68, 45.92), 'speaker': 'SPEAKER_01'}]}\n</code></pre>"},{"location":"tutorials/14-diarization-training/","title":"PyanNet Model Training Speaker Diarization","text":"<p>This process highlights the steps taken for Model Training on the CallHome Dataset. For this particular dataset we used the English version of the CallHome Dataset. The Model Training Architecture, Loss Functions, Optimisation Techniques, Data Augmentation and Metrics Used.</p>"},{"location":"tutorials/14-diarization-training/#segmentation-model-configuration-explained","title":"Segmentation Model Configuration Explained","text":""},{"location":"tutorials/14-diarization-training/#overview","title":"Overview","text":""},{"location":"tutorials/14-diarization-training/#model-architecture","title":"Model Architecture","text":"<ul> <li>SegmentationModel: This is a wrapper for the PyanNet segmentation model used for speaker diarization tasks. Inherits from Pretrained model to be compatible with the HF Trainer. Can be used to train segmentation models to be used for the \"SpeakerDiarisation Task\" in pyannote.</li> </ul> <p>Forward</p> <p><code>forward</code>: Forward pass function of the Pretrained Model.</p> <p>Parameters:</p> <p><code>waveforms(torch.tensor)</code> : A tensor containing audio data to be processed by the model and ensures the waveforms parameter is a PyTorch tensor.</p> <p><code>labels</code>: Ground truth labels for Training. Defaults to None.</p> <p><code>nb_speakers</code>: Number of speakers. Defaults to <code>None</code></p> <p>Returns: A dictionary with loss(if predicted) and predictions.</p> <p>Setup loss function</p> <p><code>setup_loss_func</code>: Sets up the loss function especially when using the powerset classes. ie <code>self.specifications.powerset=True</code></p> <p>Segmentation Loss Function</p> <p><code>segmentation_loss</code>: Defines the permutation-invariant segmentation loss. Computes the loss using either <code>nll_loss</code>(negative log likelihood) for <code>powerset</code> or <code>binary_cross_entropy</code></p> <p>Parameters:</p> <p><code>permutated_prediction</code>: Prediction after permutation. Type: <code>torch.Tensor</code></p> <p><code>target</code>: Ground truth labels. Type: <code>torch.Tensor</code></p> <p><code>weight</code>: Type: <code>Optional[torch.Tensor]</code></p> <p>Returns: Permutation-invariant segmentation loss. <code>torch.Tensor</code></p> <p>To pyannote</p> <p><code>to_pyannote_model</code>: Converts the current model to a pyannote segmentation model for use in pyannote pipelines</p> <pre><code>class SegmentationModel(PreTrainedModel):\n    config_class = SegmentationModelConfig\n\n    def __init__(\n        self,\n        config=SegmentationModelConfig(),\n    ):\n        super().__init__(config)\n\n        self.model = PyanNet_nn(sincnet={\"stride\": 10})\n\n        self.weigh_by_cardinality = config.weigh_by_cardinality\n        self.max_speakers_per_frame = config.max_speakers_per_frame\n        self.chunk_duration = config.chunk_duration\n        self.min_duration = config.min_duration\n        self.warm_up = config.warm_up\n        self.max_speakers_per_chunk = config.max_speakers_per_chunk\n\n        self.specifications = Specifications(\n            problem=Problem.MULTI_LABEL_CLASSIFICATION\n            if self.max_speakers_per_frame is None\n            else Problem.MONO_LABEL_CLASSIFICATION,\n            resolution=Resolution.FRAME,\n            duration=self.chunk_duration,\n            min_duration=self.min_duration,\n            warm_up=self.warm_up,\n            classes=[f\"speaker#{i+1}\" for i in range(self.max_speakers_per_chunk)],\n            powerset_max_classes=self.max_speakers_per_frame,\n            permutation_invariant=True,\n        )\n        self.model.specifications = self.specifications\n        self.model.build()\n        self.setup_loss_func()\n\n    def forward(self, waveforms, labels=None, nb_speakers=None):\n\n        prediction = self.model(waveforms.unsqueeze(1))\n        batch_size, num_frames, _ = prediction.shape\n\n        if labels is not None:\n            weight = torch.ones(batch_size, num_frames, 1, device=waveforms.device)\n            warm_up_left = round(self.specifications.warm_up[0] / self.specifications.duration * num_frames)\n            weight[:, :warm_up_left] = 0.0\n            warm_up_right = round(self.specifications.warm_up[1] / self.specifications.duration * num_frames)\n            weight[:, num_frames - warm_up_right :] = 0.0\n\n            if self.specifications.powerset:\n                multilabel = self.model.powerset.to_multilabel(prediction)\n                permutated_target, _ = permutate(multilabel, labels)\n\n                permutated_target_powerset = self.model.powerset.to_powerset(permutated_target.float())\n                loss = self.segmentation_loss(prediction, permutated_target_powerset, weight=weight)\n\n            else:\n                permutated_prediction, _ = permutate(labels, prediction)\n                loss = self.segmentation_loss(permutated_prediction, labels, weight=weight)\n\n            return {\"loss\": loss, \"logits\": prediction}\n\n        return {\"logits\": prediction}\n\n    def setup_loss_func(self):\n        if self.specifications.powerset:\n            self.model.powerset = Powerset(\n                len(self.specifications.classes),\n                self.specifications.powerset_max_classes,\n            )\n\n    def segmentation_loss(\n        self,\n        permutated_prediction: torch.Tensor,\n        target: torch.Tensor,\n        weight: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n\n\n        if self.specifications.powerset:\n            # `clamp_min` is needed to set non-speech weight to 1.\n            class_weight = torch.clamp_min(self.model.powerset.cardinality, 1.0) if self.weigh_by_cardinality else None\n            seg_loss = nll_loss(\n                permutated_prediction,\n                torch.argmax(target, dim=-1),\n                class_weight=class_weight,\n                weight=weight,\n            )\n        else:\n            seg_loss = binary_cross_entropy(permutated_prediction, target.float(), weight=weight)\n\n        return seg_loss\n\n    @classmethod\n    def from_pyannote_model(cls, pretrained):\n\n        # Initialize model:\n        specifications = copy.deepcopy(pretrained.specifications)\n\n        # Copy pretrained model hyperparameters:\n        chunk_duration = specifications.duration\n        max_speakers_per_frame = specifications.powerset_max_classes\n        weigh_by_cardinality = False\n        min_duration = specifications.min_duration\n        warm_up = specifications.warm_up\n        max_speakers_per_chunk = len(specifications.classes)\n\n        config = SegmentationModelConfig(\n            chunk_duration=chunk_duration,\n            max_speakers_per_frame=max_speakers_per_frame,\n            weigh_by_cardinality=weigh_by_cardinality,\n            min_duration=min_duration,\n            warm_up=warm_up,\n            max_speakers_per_chunk=max_speakers_per_chunk,\n        )\n\n        model = cls(config)\n\n        # Copy pretrained model weights:\n        model.model.hparams = copy.deepcopy(pretrained.hparams)\n        model.model.sincnet = copy.deepcopy(pretrained.sincnet)\n        model.model.sincnet.load_state_dict(pretrained.sincnet.state_dict())\n        model.model.lstm = copy.deepcopy(pretrained.lstm)\n        model.model.lstm.load_state_dict(pretrained.lstm.state_dict())\n        model.model.linear = copy.deepcopy(pretrained.linear)\n        model.model.linear.load_state_dict(pretrained.linear.state_dict())\n        model.model.classifier = copy.deepcopy(pretrained.classifier)\n        model.model.classifier.load_state_dict(pretrained.classifier.state_dict())\n        model.model.activation = copy.deepcopy(pretrained.activation)\n        model.model.activation.load_state_dict(pretrained.activation.state_dict())\n\n        return model\n\n    def to_pyannote_model(self):\n\n        seg_model = PyanNet(sincnet={\"stride\": 10})\n        seg_model.hparams.update(self.model.hparams)\n\n        seg_model.sincnet = copy.deepcopy(self.model.sincnet)\n        seg_model.sincnet.load_state_dict(self.model.sincnet.state_dict())\n\n        seg_model.lstm = copy.deepcopy(self.model.lstm)\n        seg_model.lstm.load_state_dict(self.model.lstm.state_dict())\n\n        seg_model.linear = copy.deepcopy(self.model.linear)\n        seg_model.linear.load_state_dict(self.model.linear.state_dict())\n\n        seg_model.classifier = copy.deepcopy(self.model.classifier)\n        seg_model.classifier.load_state_dict(self.model.classifier.state_dict())\n\n        seg_model.activation = copy.deepcopy(self.model.activation)\n        seg_model.activation.load_state_dict(self.model.activation.state_dict())\n\n        seg_model.specifications = self.specifications\n\n        return seg_model\n</code></pre> <p>Segmentation Model Configuration</p> <ul> <li><code>SegmentationModelConfig</code>Configuration class for the segmentation model, specifying various parameters like chunk duration, maximum speakers per frame, etc.</li> <li>Configuration parameters like chunk duration, number of speakers per chunk/frame, minimum duration, warm-up period, etc.</li> </ul> <pre><code>class SegmentationModelConfig(PretrainedConfig):\n\n    model_type = \"pyannet\"\n\n    def __init__(\n        self,\n        chunk_duration=10,\n        max_speakers_per_frame=2,\n        max_speakers_per_chunk=3,\n        min_duration=None,\n        warm_up=(0.0, 0.0),\n        weigh_by_cardinality=False,\n        **kwargs,\n    ):\n\n        super().__init__(**kwargs)\n        self.chunk_duration = chunk_duration\n        self.max_speakers_per_frame = max_speakers_per_frame\n        self.max_speakers_per_chunk = max_speakers_per_chunk\n        self.min_duration = min_duration\n        self.warm_up = warm_up\n        self.weigh_by_cardinality = weigh_by_cardinality\n        # For now, the model handles only 16000 Hz sampling rate\n        self.sample_rate = 16000\n</code></pre>"},{"location":"tutorials/14-diarization-training/#loss-functions","title":"Loss Functions","text":""},{"location":"tutorials/14-diarization-training/#binary-cross-entropy","title":"Binary Cross-Entropy","text":"<ul> <li>Used when the model does not use the powerset approach.</li> <li>Computes the binary cross-entropy loss between the predicted and actual speaker activity.</li> </ul>"},{"location":"tutorials/14-diarization-training/#negative-log-likelihood-nll-loss","title":"Negative Log-Likelihood (NLL) Loss","text":"<ul> <li>Used when the model uses the powerset approach.</li> <li>Computes the NLL loss considering class weights if specified.</li> </ul>"},{"location":"tutorials/14-diarization-training/#optimization-techniques","title":"Optimization Techniques","text":""},{"location":"tutorials/14-diarization-training/#batch-size","title":"Batch Size","text":"<ul> <li>This refers to the number of samples that you feed into your model at each iteration of the training process. This can be adjusted accordingly to optimise the performance of your model</li> </ul>"},{"location":"tutorials/14-diarization-training/#learning-rate","title":"Learning Rate","text":"<ul> <li>This is an optimization tunning parameter that determines the step-size at each iteration while moving towards a minimum loss function</li> </ul>"},{"location":"tutorials/14-diarization-training/#training-epochs","title":"Training Epochs","text":"<ul> <li>An epoch refers to a complete pass through the entire training dataset. A model is exposed to all the training examples and updates its parametrs basd on the patterns it learns. In our case, we try and iterate and test with 5, 10 and 20 epochs and find that the Diarisation Error Rate remains constant at \"'der': 0.23994926057695026\"</li> </ul>"},{"location":"tutorials/14-diarization-training/#warm-up","title":"Warm-up","text":"<ul> <li>The warm-up period allows the model to adjust at the beginning of each chunk, ensuring the central part of the chunk is more accurate.</li> <li>The warm-up is applied to both the left and right parts of each chunk.</li> </ul>"},{"location":"tutorials/14-diarization-training/#permutation-invariant-training","title":"Permutation-Invariant Training","text":"<ul> <li>This technique permutes predictions and targets to find the optimal alignment, ensuring the loss computation is invariant to the order of speakers.</li> </ul>"},{"location":"tutorials/14-diarization-training/#data-augmentation-methods","title":"Data Augmentation Methods","text":"<ul> <li>For our case this is done using the the DataCollator class. This class is responsible for collecting data and ensuring that the target labels are dynamically padded.</li> <li>Pads the target labels to ensure they have the same shape.</li> <li>Pads with zeros if the number of speakers in a chunk is less than the maximum number of speakers per chunk</li> </ul>"},{"location":"tutorials/14-diarization-training/#preprocessing-steps","title":"Preprocessing Steps","text":"<ul> <li>Preprocessing steps like random overlap and fixed overlap during chunking can be considered a form of augmentation as they provide varied inputs to the model.</li> <li><code>Preprocess</code> class used to handle these preprocessing steps is not detailed here, but it's responsible for preparing the input data.</li> </ul> <pre><code>class Preprocess:\n    def __init__(\n        self,\n        config,\n    ):\n\n        self.chunk_duration = config.chunk_duration\n        self.max_speakers_per_frame = config.max_speakers_per_frame\n        self.max_speakers_per_chunk = config.max_speakers_per_chunk\n        self.min_duration = config.min_duration\n        self.warm_up = config.warm_up\n\n        self.sample_rate = config.sample_rate\n        self.model = SegmentationModel(config).to_pyannote_model()\n\n        # Get the number of frames associated to a chunk:\n        _, self.num_frames_per_chunk, _ = self.model(\n            torch.rand((1, int(self.chunk_duration * self.sample_rate)))\n        ).shape\n\n    def get_labels_in_file(self, file):\n\n\n        file_labels = []\n        for i in range(len(file[\"speakers\"][0])):\n            if file[\"speakers\"][0][i] not in file_labels:\n                file_labels.append(file[\"speakers\"][0][i])\n\n        return file_labels\n\n    def get_segments_in_file(self, file, labels):\n\n\n        file_annotations = []\n\n        for i in range(len(file[\"timestamps_start\"][0])):\n            start_segment = file[\"timestamps_start\"][0][i]\n            end_segment = file[\"timestamps_end\"][0][i]\n            label = labels.index(file[\"speakers\"][0][i])\n            file_annotations.append((start_segment, end_segment, label))\n\n        dtype = [(\"start\", \"&lt;f4\"), (\"end\", \"&lt;f4\"), (\"labels\", \"i1\")]\n\n        annotations = np.array(file_annotations, dtype)\n\n        return annotations\n\n    def get_chunk(self, file, start_time):\n\n\n        sample_rate = file[\"audio\"][0][\"sampling_rate\"]\n\n        assert sample_rate == self.sample_rate\n\n        end_time = start_time + self.chunk_duration\n        start_frame = math.floor(start_time * sample_rate)\n        num_frames_waveform = math.floor(self.chunk_duration * sample_rate)\n        end_frame = start_frame + num_frames_waveform\n\n        waveform = file[\"audio\"][0][\"array\"][start_frame:end_frame]\n\n        labels = self.get_labels_in_file(file)\n\n        file_segments = self.get_segments_in_file(file, labels)\n\n        chunk_segments = file_segments[(file_segments[\"start\"] &lt; end_time) &amp; (file_segments[\"end\"] &gt; start_time)]\n\n        # compute frame resolution:\n        # resolution = self.chunk_duration / self.num_frames_per_chunk\n\n        # discretize chunk annotations at model output resolution\n        step = self.model.receptive_field.step\n        half = 0.5 * self.model.receptive_field.duration\n\n        # discretize chunk annotations at model output resolution\n        start = np.maximum(chunk_segments[\"start\"], start_time) - start_time - half\n        start_idx = np.maximum(0, np.round(start / step)).astype(int)\n\n        # start_idx = np.floor(start / resolution).astype(int)\n        end = np.minimum(chunk_segments[\"end\"], end_time) - start_time - half\n        end_idx = np.round(end / step).astype(int)\n\n        # end_idx = np.ceil(end / resolution).astype(int)\n\n        # get list and number of labels for current scope\n        labels = list(np.unique(chunk_segments[\"labels\"]))\n        num_labels = len(labels)\n        # initial frame-level targets\n        y = np.zeros((self.num_frames_per_chunk, num_labels), dtype=np.uint8)\n\n        # map labels to indices\n        mapping = {label: idx for idx, label in enumerate(labels)}\n\n        for start, end, label in zip(start_idx, end_idx, chunk_segments[\"labels\"]):\n            mapped_label = mapping[label]\n            y[start : end + 1, mapped_label] = 1\n\n        return waveform, y, labels\n\n    def get_start_positions(self, file, overlap, random=False):\n\n        sample_rate = file[\"audio\"][0][\"sampling_rate\"]\n\n        assert sample_rate == self.sample_rate\n\n        file_duration = len(file[\"audio\"][0][\"array\"]) / sample_rate\n        start_positions = np.arange(0, file_duration - self.chunk_duration, self.chunk_duration * (1 - overlap))\n\n        if random:\n            nb_samples = int(file_duration / self.chunk_duration)\n            start_positions = np.random.uniform(0, file_duration, nb_samples)\n\n        return start_positions\n\n    def __call__(self, file, random=False, overlap=0.0):\n\n        new_batch = {\"waveforms\": [], \"labels\": [], \"nb_speakers\": []}\n\n        if random:\n            start_positions = self.get_start_positions(file, overlap, random=True)\n        else:\n            start_positions = self.get_start_positions(file, overlap)\n\n        for start_time in start_positions:\n            waveform, target, label = self.get_chunk(file, start_time)\n\n            new_batch[\"waveforms\"].append(waveform)\n            new_batch[\"labels\"].append(target)\n            new_batch[\"nb_speakers\"].append(label)\n\n        return new_batch\n</code></pre>"},{"location":"tutorials/14-diarization-training/#metrics-and-trainer","title":"Metrics and Trainer","text":"<ul> <li>Initializes the Metrics class for evaluation.</li> <li>Configures the Trainer with the model, training arguments, datasets, data collator, and metrics.</li> <li>For the metrics we have the Diarisation Error Rate(DER), FalseAlarm Rate, MissedDetectionRate and the SpeakerConfusionRate with the implementation in the metrics class below.</li> </ul> <pre><code>import numpy as np\nimport torch\nfrom pyannote.audio.torchmetrics import (DiarizationErrorRate, FalseAlarmRate,\n                                         MissedDetectionRate,\n                                         SpeakerConfusionRate)\nfrom pyannote.audio.utils.powerset import Powerset\n\n\nclass Metrics:\n    \"\"\"Metric class used by the HF trainer to compute speaker diarization metrics.\"\"\"\n\n    def __init__(self, specifications) -&gt; None:\n        \"\"\"init method\n\n        Args:\n            specifications (_type_): specifications attribute from a SegmentationModel.\n        \"\"\"\n        self.powerset = specifications.powerset\n        self.classes = specifications.classes\n        self.powerset_max_classes = specifications.powerset_max_classes\n\n        self.model_powerset = Powerset(\n            len(self.classes),\n            self.powerset_max_classes,\n        )\n\n        self.metrics = {\n            \"der\": DiarizationErrorRate(0.5),\n            \"confusion\": SpeakerConfusionRate(0.5),\n            \"missed_detection\": MissedDetectionRate(0.5),\n            \"false_alarm\": FalseAlarmRate(0.5),\n        }\n\n    def __call__(self, eval_pred):\n\n        logits, labels = eval_pred\n\n        if self.powerset:\n            predictions = self.model_powerset.to_multilabel(torch.tensor(logits))\n        else:\n            predictions = torch.tensor(logits)\n\n        labels = torch.tensor(labels)\n\n        predictions = torch.transpose(predictions, 1, 2)\n        labels = torch.transpose(labels, 1, 2)\n\n        metrics = {\"der\": 0, \"false_alarm\": 0, \"missed_detection\": 0, \"confusion\": 0}\n\n        metrics[\"der\"] += self.metrics[\"der\"](predictions, labels).cpu().numpy()\n        metrics[\"false_alarm\"] += self.metrics[\"false_alarm\"](predictions, labels).cpu().numpy()\n        metrics[\"missed_detection\"] += self.metrics[\"missed_detection\"](predictions, labels).cpu().numpy()\n        metrics[\"confusion\"] += self.metrics[\"confusion\"](predictions, labels).cpu().numpy()\n\n        return metrics\n\n\nclass DataCollator:\n    \"\"\"Data collator that will dynamically pad the target labels to have max_speakers_per_chunk\"\"\"\n\n    def __init__(self, max_speakers_per_chunk) -&gt; None:\n        self.max_speakers_per_chunk = max_speakers_per_chunk\n\n    def __call__(self, features):\n        \"\"\"_summary_\n\n        Args:\n            features (_type_): _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n\n        batch = {}\n\n        speakers = [f[\"nb_speakers\"] for f in features]\n        labels = [f[\"labels\"] for f in features]\n\n        batch[\"labels\"] = self.pad_targets(labels, speakers)\n\n        batch[\"waveforms\"] = torch.stack([f[\"waveforms\"] for f in features])\n\n        return batch\n\n    def pad_targets(self, labels, speakers):\n        \"\"\"\n        labels:\n        speakers:\n\n        Returns:\n            _type_:\n                Collated target tensor of shape (num_frames, self.max_speakers_per_chunk)\n                If one chunk has more than max_speakers_per_chunk speakers, we keep\n                the max_speakers_per_chunk most talkative ones. If it has less, we pad with\n                zeros (artificial inactive speakers).\n        \"\"\"\n\n        targets = []\n\n        for i in range(len(labels)):\n            label = speakers[i]\n            target = labels[i].numpy()\n            num_speakers = len(label)\n\n            if num_speakers &gt; self.max_speakers_per_chunk:\n                indices = np.argsort(-np.sum(target, axis=0), axis=0)\n                target = target[:, indices[: self.max_speakers_per_chunk]]\n\n            elif num_speakers &lt; self.max_speakers_per_chunk:\n                target = np.pad(\n                    target,\n                    ((0, 0), (0, self.max_speakers_per_chunk - num_speakers)),\n                    mode=\"constant\",\n                )\n\n            targets.append(target)\n\n        return torch.from_numpy(np.stack(targets))\n</code></pre>"},{"location":"tutorials/14-diarization-training/#training-script","title":"Training Script","text":"<ul> <li>The script train_segmentation.py   can be used to pre-process a diarization dataset and subsequently fine-tune the pyannote segmentation model. In the following example, we fine-tuned the segmentation model on the English subset of the CallHome dataset, a conversational dataset between native speakers:</li> </ul> <pre><code>!python3 train_segmentation.py \\\n    --dataset_name=diarizers-community/callhome \\\n    --dataset_config_name=eng \\\n    --split_on_subset=data \\\n    --model_name_or_path=pyannote/segmentation-3.0 \\\n    --output_dir=./speaker-segmentation-fine-tuned-callhome-eng \\\n    --do_train \\\n    --do_eval \\\n    --learning_rate=1e-3 \\\n    --num_train_epochs=20 \\\n    --lr_scheduler_type=cosine \\\n    --per_device_train_batch_size=32 \\\n    --per_device_eval_batch_size=32 \\\n    --evaluation_strategy=epoch \\\n    --save_strategy=epoch \\\n    --preprocessing_num_workers=2 \\\n    --dataloader_num_workers=2 \\\n    --logging_steps=100 \\\n    --load_best_model_at_end \\\n    --push_to_hub\n</code></pre>"},{"location":"tutorials/14-diarization-training/#evaluation-script","title":"Evaluation Script","text":"<p>The script test_segmentation.pycan be used to evaluate a fine-tuned model on a diarization dataset. In the following example, we evaluate the fine-tuned model from the previous step on the test split of the CallHome English dataset:</p> <pre><code>!python3 test_segmentation.py \\\n    --dataset_name=diarizers-community/callhome \\\n    --dataset_config_name=eng \\\n    --split_on_subset=data \\\n    --test_split_name=test \\\n    --model_name_or_path=diarizers-community/speaker-segmentation-fine-tuned-callhome-eng \\\n    --preprocessing_num_workers=2 \\\n    --evaluate_with_pipeline\n</code></pre> <p>Sample Output</p> <p></p>"},{"location":"tutorials/14-diarization-training/#inference-with-pyannote","title":"Inference with Pyannote","text":"<ul> <li>The fine-tuned segmentation model can easily be loaded into the pyannote speaker diarization pipeline for inference. To do so, we load the pre-trained speaker diarization pipeline, and subsequently override the segmentation model with our fine-tuned checkpoint:</li> </ul> <pre><code>from diarizers import SegmentationModel\nfrom pyannote.audio import Pipeline\nfrom datasets import load_dataset\nimport torch\n\ndevice = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n# load the pre-trained pyannote pipeline\npipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\")\npipeline.to(device)\n\n# replace the segmentation model with your fine-tuned one\nmodel = SegmentationModel().from_pretrained(\"diarizers-community/speaker-segmentation-fine-tuned-callhome-jpn\")\nmodel = model.to_pyannote_model()\npipeline._segmentation.model = model.to(device)\n\n# load dataset example\ndataset = load_dataset(\"diarizers-community/callhome\", \"jpn\", split=\"data\")\nsample = dataset[0][\"audio\"]\n\n# pre-process inputs\nsample[\"waveform\"] = torch.from_numpy(sample.pop(\"array\")[None, :]).to(device, dtype=model.dtype)\nsample[\"sample_rate\"] = sample.pop(\"sampling_rate\")\n\n# perform inference\ndiarization = pipeline(sample)\n\n# dump the diarization output to disk using RTTM format\nwith open(\"audio.rttm\", \"w\") as rttm:\n    diarization.write_rttm(rttm)\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/","title":"Evaluating the Fine-Tuned Model","text":"<p>The script <code>test_segmentation.py</code> can be used to evaluate a fine-tuned model on a diarization dataset.</p> <p>In the following example, we evaluate the fine-tuned model from the test split of the CallHome English Dataset.</p> <pre><code>python3 test_segmentation.py \\\n    --dataset_name=diarizers-community/callhome \\\n    --dataset_config_name=eng \\\n    --split_on_subset=data \\\n    --test_split_name=test \\\n    --model_name_or_path=diarizers-community/speaker-segmentation-fine-tuned-callhome-eng \\\n    --preprocessing_num_workers=2 \\\n    --evaluate_with_pipeline \\\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/#sample-output","title":"Sample Output","text":"<p>The output above is the default output that can be obtained using the default Evaluation Script.</p> <p>This documentation further explores the evaluation process adding more to the metrics that can be measured during this process and highlighting the editing.</p> <p>Considering there are many metrics that can be obtained throughout the diarization process as documented in the <code>pyannote.audio.metrics</code> documentation.</p> <p></p> <p>In this documentation, we'll focus on the Segmentation Precision, Segmentation Recall and Identification F1 Score.</p>"},{"location":"tutorials/15-diarization-evaluation/#segmentation-precision-and-recall","title":"Segmentation Precision and Recall","text":"<p>Imports</p> <p><code>Segment</code>: Speaker segmentation is the process of dividing an audio recording into segments based on the changing speakers\u2019 identities. The goal of speaker segmentation is to determine the time boundaries where the speaker changes occur, effectively identifying the points at which one speaker\u2019s speech ends, and another\u2019s begins. That said, a <code>Segment</code> is a data structure with <code>start</code> and <code>end</code> time that will then be placed in a <code>Timeline</code></p> <p><code>Timeline</code>: A data structure containing various segments. Reference timelines are provided in the ground truth and are compared against the predicted timelines to calculate <code>segmentation precision</code> and <code>segmentation recall</code></p> <pre><code>from pyannote.core import SlidingWindow, SlidingWindowFeature, Timeline, Segment\nfrom pyannote.metrics import segmentation, identification\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/#testing-the-segmentation-model","title":"Testing the Segmentation Model","text":"<p>Initialization</p> <p><code>class Test</code> : The Segmentation Model test implementation is carried out within the Test Class found in the <code>Test.py</code> file in <code>src/diarizers</code></p> <p>Parameters</p> <p><code>test_dataset</code>: The test dataset to be used. In this example, it will be the test split on the Callhome English dataset.</p> <p><code>model (SegmentationModel)</code>: The model is the finetuned model trained by the <code>train_segmentation.py</code> script.</p> <p><code>step (float, optional)</code>: Steps between successive generated audio chunks. Defaults to 2.5.</p> <p><code>metrics</code>: For this example, the metrics <code>segmentation_precision</code>,<code>segmentation_recall</code>,<code>recall_value</code>,<code>precision_value</code> and <code>count</code> have been added for the purpose of calculating the segmentation recall and precision of the Segmentation Model.</p> <pre><code>class Test:\n\n    def __init__(self, test_dataset, model, step=2.5):\n\n        self.test_dataset = test_dataset\n        self.model = model\n        (self.device,) = get_devices(needs=1)\n        self.inference = Inference(self.model, step=step, device=self.device)\n\n        self.sample_rate = test_dataset[0][\"audio\"][\"sampling_rate\"]\n\n        # Get the number of frames associated to a chunk:\n        _, self.num_frames, _ = self.inference.model(\n            torch.rand((1, int(self.inference.duration * self.sample_rate))).to(self.device)\n        ).shape\n        # compute frame resolution:\n        self.resolution = self.inference.duration / self.num_frames\n\n        self.metrics = {\n            \"der\": DiarizationErrorRate(0.5).to(self.device),\n            \"confusion\": SpeakerConfusionRate(0.5).to(self.device),\n            \"missed_detection\": MissedDetectionRate(0.5).to(self.device),\n            \"false_alarm\": FalseAlarmRate(0.5).to(self.device),\n            \"segmentation_precision\": segmentation.SegmentationPrecision(),\n            \"segmentation_recall\": segmentation.SegmentationRecall(),\n            \"recall_value\":0,\n            \"precision_value\": 0,\n            \"count\": 0,\n        }\n</code></pre> <p>Predict function</p> <p>This function makes a prediction on a dataset row using pyannote inference object.</p> <pre><code>    def predict(self, file):\n        audio = torch.tensor(file[\"audio\"][\"array\"]).unsqueeze(0).to(torch.float32).to(self.device)\n        sample_rate = file[\"audio\"][\"sampling_rate\"]\n\n        input = {\"waveform\": audio, \"sample_rate\": sample_rate}\n\n        prediction = self.inference(input)\n\n        return prediction\n</code></pre> <p>Compute Ground Truth Function</p> <p>This function converts a dataset row into the suitable format for evaluation as the ground truth.</p> <p><code>Returns</code>: numpy array with shape (num_frames, num_speakers).</p> <pre><code>def compute_gt(self, file):\n\n    audio = torch.tensor(file[\"audio\"][\"array\"]).unsqueeze(0).to(torch.float32)\n    sample_rate = file[\"audio\"][\"sampling_rate\"]\n\n    audio_duration = len(audio[0]) / sample_rate\n    num_frames = int(round(audio_duration / self.resolution))\n\n    labels = list(set(file[\"speakers\"]))\n\n    gt = np.zeros((num_frames, len(labels)), dtype=np.uint8)\n\n    for i in range(len(file[\"timestamps_start\"])):\n        start = file[\"timestamps_start\"][i]\n        end = file[\"timestamps_end\"][i]\n        speaker = file[\"speakers\"][i]\n        start_frame = int(round(start / self.resolution))\n        end_frame = int(round(end / self.resolution))\n        speaker_index = labels.index(speaker)\n\n        gt[start_frame:end_frame, speaker_index] += 1\n\n    return gt\n</code></pre> <p>Convert to Timeline</p> <p>This function creates a <code>Timeline</code> using data and labels passed as parameters and converted into <code>Segments</code>. Required in order to calculate Segmentation Precision and Recall.</p> <pre><code>    def convert_to_timeline(self, data, labels):\n        timeline = Timeline()\n        for speaker_index, label in enumerate(labels):\n            segments = np.where(data[:, speaker_index] == 1)[0]\n            if len(segments) &gt; 0:\n                start = segments[0] * self.resolution\n                end = segments[0] * self.resolution\n                for frame in segments[1:]:\n                    if frame == end / self.resolution + 1:\n                        end += self.resolution\n                    else:\n                        timeline.add(Segment(start, end + self.resolution))\n                        start = frame * self.resolution\n                        end = frame * self.resolution\n                timeline.add(Segment(start, end + self.resolution))\n        return timeline\n</code></pre> <p>Compute Metrics on File</p> <p>Function that computes metrics for a dataset row passed into it. This function is run iteratively until the entire dataset has been processed.</p> <pre><code>    def compute_metrics_on_file(self, file):\n        gt = self.compute_gt(file)\n        prediction = self.predict(file)\n\n        sliding_window = SlidingWindow(start=0, step=self.resolution, duration=self.resolution)\n        labels = list(set(file[\"speakers\"]))\n\n        reference = SlidingWindowFeature(data=gt, labels=labels, sliding_window=sliding_window)\n\n        # Convert to Timeline for SegmentationPrecision\n        reference_timeline = self.convert_to_timeline(gt, labels)\n        prediction_timeline = self.convert_to_timeline(prediction.data, labels)\n\n\n        for window, pred in prediction:\n            reference_window = reference.crop(window, mode=\"center\")\n            common_num_frames = min(self.num_frames, reference_window.shape[0])\n\n            _, ref_num_speakers = reference_window.shape\n            _, pred_num_speakers = pred.shape\n\n            if pred_num_speakers &gt; ref_num_speakers:\n                reference_window = np.pad(reference_window, ((0, 0), (0, pred_num_speakers - ref_num_speakers)))\n            elif ref_num_speakers &gt; pred_num_speakers:\n                pred = np.pad(pred, ((0, 0), (0, ref_num_speakers - pred_num_speakers)))\n\n            pred = torch.tensor(pred[:common_num_frames]).unsqueeze(0).permute(0, 2, 1).to(self.device)\n            target = (torch.tensor(reference_window[:common_num_frames]).unsqueeze(0).permute(0, 2, 1)).to(self.device)\n\n            self.metrics[\"der\"](pred, target)\n            self.metrics[\"false_alarm\"](pred, target)\n            self.metrics[\"missed_detection\"](pred, target)\n            self.metrics[\"confusion\"](pred, target)\n\n\n        # Compute precision\n        self.metrics[\"precision_value\"] += self.metrics[\"segmentation_precision\"](reference_timeline, prediction_timeline)\n        self.metrics[\"recall_value\"] += self.metrics[\"segmentation_recall\"](reference_timeline, prediction_timeline)\n        self.metrics[\"count\"] += 1\n</code></pre> <p>Compute Metrics</p> <p>Using all the functions above, the metrics for the Segmentation Model can then be computed and returned at once as shown below.</p> <p>Further information on metrics that extracted from the Segmentation model can be found here</p> <pre><code>    def compute_metrics(self):\n        \"\"\"Main method, used to compute speaker diarization metrics on test_dataset.\n        Returns:\n            dict: metric values.\n        \"\"\"\n\n        for file in tqdm(self.test_dataset):\n            self.compute_metrics_on_file(file)\n        if self.metrics[\"count\"] != 0:\n            self.metrics[\"precision_value\"] /= self.metrics[\"count\"]\n            self.metrics[\"recall_value\"] /= self.metrics[\"count\"]\n\n        return {\n            \"der\": self.metrics[\"der\"].compute(),\n            \"false_alarm\": self.metrics[\"false_alarm\"].compute(),\n            \"missed_detection\": self.metrics[\"missed_detection\"].compute(),\n            \"confusion\": self.metrics[\"confusion\"].compute(),\n            \"segmentation_precision\": self.metrics[\"precision_value\"],\n            \"segmentation_recall\": self.metrics[\"recall_value\"],\n        }\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/#testing-the-speaker-diarization-with-the-fine-tuned-segmentation-model","title":"Testing the Speaker Diarization (With the Fine-tuned Segmentation Model)","text":"<p>The Fine-tuned segmentation model can be run in the Speaker Diarization Pipeline by calling <code>from_pretrained</code> and overwriting the segmentation model with the fine-tuned model. Code can be found in the <code>Test.py</code> script.</p> <pre><code>pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\")\n        pipeline._segmentation.model = model\n</code></pre> <p>Initialization</p> <p>The class <code>TestPipeline</code> will be implementing and testing the Speaker Diarization Pipeline with the finetuned segmentation model.</p> <p>Parameters</p> <p><code>pipeline</code>: Speaker Diarization pipeline</p> <p><code>test_dataset</code>: Data to be tested. In this example, it is data from the Callhome English dataset.</p> <p><code>metrics</code>: Since <code>pyannote.metrics</code> does not offer Identification F1-score, we'll use the Precision and Recall to calculate the <code>identificationF1Score</code></p> <pre><code>class TestPipeline:\n    def __init__(self, test_dataset, pipeline) -&gt; None:\n\n        self.test_dataset = test_dataset\n\n        (self.device,) = get_devices(needs=1)\n        self.pipeline = pipeline.to(self.device)\n        self.sample_rate = test_dataset[0][\"audio\"][\"sampling_rate\"]\n\n        # Get the number of frames associated to a chunk:\n        _, self.num_frames, _ = self.pipeline._segmentation.model(\n            torch.rand((1, int(self.pipeline._segmentation.duration * self.sample_rate))).to(self.device)\n        ).shape\n        # compute frame resolution:\n        self.resolution = self.pipeline._segmentation.duration / self.num_frames\n\n        self.metrics = {\n            \"der\": diarization.DiarizationErrorRate(),\n            \"identification_precision\": identification.IdentificationPrecision(),\n            \"identification_recall\": identification.IdentificationRecall(),\n            \"identification_f1\": 0,\n\n        }\n</code></pre> <p>Compute Ground Truth</p> <p>Function that reformats the Dataset Row to return the ground truth to be used for evaluation.</p> <p>Parameters</p> <p><code>file</code>: A single Dataset Row</p> <pre><code>def compute_gt(self, file):\n\n    \"\"\"\n    Args:\n        file (_type_): dataset row.\n\n    Returns:\n        gt: numpy array with shape (num_frames, num_speakers).\n    \"\"\"\n\n    audio = torch.tensor(file[\"audio\"][\"array\"]).unsqueeze(0).to(torch.float32)\n    sample_rate = file[\"audio\"][\"sampling_rate\"]\n\n    audio_duration = len(audio[0]) / sample_rate\n    num_frames = int(round(audio_duration / self.resolution))\n\n    labels = list(set(file[\"speakers\"]))\n\n    gt = np.zeros((num_frames, len(labels)), dtype=np.uint8)\n\n    for i in range(len(file[\"timestamps_start\"])):\n        start = file[\"timestamps_start\"][i]\n        end = file[\"timestamps_end\"][i]\n        speaker = file[\"speakers\"][i]\n        start_frame = int(round(start / self.resolution))\n        end_frame = int(round(end / self.resolution))\n        speaker_index = labels.index(speaker)\n\n        gt[start_frame:end_frame, speaker_index] += 1\n\n    return gt\n</code></pre> <p>Predict Function</p> <pre><code>def predict(self, file):\n\n    sample = {}\n    sample[\"waveform\"] = (\n        torch.from_numpy(file[\"audio\"][\"array\"])\n        .to(self.device, dtype=self.pipeline._segmentation.model.dtype)\n        .unsqueeze(0)\n    )\n    sample[\"sample_rate\"] = file[\"audio\"][\"sampling_rate\"]\n\n    prediction = self.pipeline(sample)\n    # print(\"Prediction data: \", prediction.data )\n\n    return prediction\n</code></pre> <p>Compute on File</p> <p>Function that calculates the f1 score of a <code>file</code>(Dataset Row) using the <code>precision</code> and <code>recall</code>. It also calculates the <code>der</code>(Diarization Error rate) and can be edited to extract more evaluation metrics such as <code>Segmentation Purity</code> and <code>Segmentation Coverage</code>.</p> <p>For the purpose of this demonstration, the latter two were not obtained. Details about Segmentation Coverage and Segmentation Purity can be obtained here.</p> <pre><code>def compute_metrics_on_file(self, file):\n\n    pred = self.predict(file)\n    gt = self.compute_gt(file)\n\n    sliding_window = SlidingWindow(start=0, step=self.resolution, duration=self.resolution)\n    gt = SlidingWindowFeature(data=gt, sliding_window=sliding_window)\n\n    gt = self.pipeline.to_annotation(\n        gt,\n        min_duration_on=0.0,\n        min_duration_off=self.pipeline.segmentation.min_duration_off,\n    )\n\n    mapping = {label: expected_label for label, expected_label in zip(gt.labels(), self.pipeline.classes())}\n\n    gt = gt.rename_labels(mapping=mapping)\n\n\n    der = self.metrics[\"der\"](pred, gt)\n    identificationPrecision = self.metrics[\"identification_precision\"](pred, gt)\n    identificationRecall = self.metrics[\"identification_recall\"](pred, gt)\n    identificationF1 = (2 * identificationPrecision * identificationRecall) / (identificationRecall + identificationPrecision)\n\n    return {\"der\": der, \"identificationF1\": identificationF1}\n</code></pre> <p>Compute Metrics</p> <p>This function iteratively calls the <code>compute_metrics_on_file</code> function to perform computation on all the files in the dataset.</p> <p><code>Returns</code>: The average values of the <code>der</code>(diarization error rate) and <code>f1</code>(F1 Score).</p> <pre><code>def compute_metrics(self):\n\n    der = 0\n    f1 = 0\n    for file in tqdm(self.test_dataset):\n        met = self.compute_metrics_on_file(file)\n        der += met[\"der\"]\n        f1 += met[\"identificationF1\"]\n\n    der /= len(self.test_dataset)\n    f1 /= len(self.test_dataset)\n\n    return {\"der\": der, \"identificationF1Score\": f1}\n</code></pre>"},{"location":"tutorials/15-diarization-evaluation/#sample-output_1","title":"Sample Output","text":"<p>An example of the output as expected from the edited script.</p> <p></p>"}]}