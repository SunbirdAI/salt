{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Spark TTS Inference Guide with vLLM\n",
    "\n",
    "This notebook demonstrates how to perform text-to-speech (TTS) inference using the **jq/spark-tts-salt** model with vLLM.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Spark TTS is a powerful text-to-speech model that can generate high-quality speech in multiple languages and voices. This implementation uses:\n",
    "- **vLLM** for efficient model inference\n",
    "- **BiCodec tokenizer** for audio token processing\n",
    "- **Retry logic** to handle generation errors gracefully\n",
    "- **Text chunking** to process long texts efficiently\n",
    "\n",
    "## Key Features\n",
    "- Multi-language support (English, Luganda, Swahili, etc.)\n",
    "- Multiple speaker IDs for different voices\n",
    "- Robust error handling with automatic retries\n",
    "- Flexible text chunking strategies\n",
    "- Audio playback in Jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, install all required dependencies. This includes:\n",
    "- **vLLM**: For efficient LLM inference\n",
    "- **transformers & unsloth**: Model loading utilities\n",
    "- **soundfile & librosa**: Audio processing\n",
    "- **torch & torchaudio**: Deep learning framework\n",
    "- **xformers, omegaconf, einx, einops**: Supporting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "!pip install xformers transformers unsloth omegaconf einx einops soundfile librosa torch torchaudio vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all necessary libraries for TTS inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "import os\n",
    "from getpass import getpass\n",
    "import re\n",
    "import soundfile as sf\n",
    "from huggingface_hub import snapshot_download\n",
    "import torch\n",
    "import sys\n",
    "from typing import Tuple, List, Optional\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sparktts-setup",
   "metadata": {},
   "source": [
    "## 3. Clone Spark-TTS Repository\n",
    "\n",
    "Clone the Spark-TTS repository to access the audio tokenizer and utilities.\n",
    "\n",
    "**Note**: Uncomment the git clone line if you haven't cloned the repository yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Spark-TTS repository (uncomment if needed)\n",
    "# !git clone https://github.com/SparkAudio/Spark-TTS\n",
    "\n",
    "# Add Spark-TTS to Python path\n",
    "sys.path.append('Spark-TTS')\n",
    "print(\"Spark-TTS repository path added to sys.path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-token-header",
   "metadata": {},
   "source": [
    "## 4. Set Hugging Face Token\n",
    "\n",
    "Set your Hugging Face token for model access. Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hugging Face token securely\n",
    "os.environ[\"HF_TOKEN\"] = getpass(\"Enter your HF_TOKEN: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-load-header",
   "metadata": {},
   "source": [
    "## 5. Load the TTS Model\n",
    "\n",
    "Load the Spark TTS model using vLLM with `enforce_eager=True` for compatibility.\n",
    "\n",
    "**Model**: `jq/spark-tts-salt`\n",
    "\n",
    "This may take a few minutes depending on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TTS model with vLLM\n",
    "print(\"Loading Spark TTS model...\")\n",
    "model = LLM(\"jq/spark-tts-salt\", enforce_eager=True)\n",
    "print(\"‚úÖ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-header",
   "metadata": {},
   "source": [
    "## 6. Download and Setup Audio Tokenizer\n",
    "\n",
    "Download the BiCodec tokenizer model files from Hugging Face and initialize the audio tokenizer.\n",
    "\n",
    "The tokenizer converts between audio and token representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tokenizer model files\n",
    "model_base_repo = \"unsloth/Spark-TTS-0.5B\"\n",
    "cache_dir = \"Spark-TTS-0.5B\"\n",
    "\n",
    "print(f\"Downloading tokenizer files from {model_base_repo}...\")\n",
    "snapshot_download(\n",
    "    repo_id=model_base_repo,\n",
    "    local_dir=cache_dir,\n",
    "    ignore_patterns=[\"*LLM*\"],  # Skip LLM files, we only need tokenizer\n",
    ")\n",
    "print(f\"‚úÖ Tokenizer files downloaded to {cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init-tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the audio tokenizer\n",
    "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
    "\n",
    "print(\"Initializing audio tokenizer...\")\n",
    "audio_tokenizer = BiCodecTokenizer(cache_dir, device)\n",
    "print(\"‚úÖ Audio tokenizer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utilities-header",
   "metadata": {},
   "source": [
    "## 7. Text Chunking Utilities\n",
    "\n",
    "These functions split long text into manageable chunks for TTS processing.\n",
    "\n",
    "### Three Chunking Strategies:\n",
    "\n",
    "1. **chunk_text**: Splits by sentence boundaries with a maximum character limit\n",
    "2. **chunk_text_simple**: Splits into individual sentences (recommended for TTS)\n",
    "3. **chunk_text_with_count**: Groups a fixed number of sentences per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks based on sentence boundaries.\n",
    "    \n",
    "    This approach preserves natural sentence flow and intonation for TTS.\n",
    "    \n",
    "    Args:\n",
    "        text: The input string to chunk\n",
    "        max_chunk_size: Maximum character length per chunk (soft limit)\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks, each containing one or more complete sentences\n",
    "    \"\"\"\n",
    "    # Split on sentence-ending punctuation (. ! ?) followed by whitespace\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    \n",
    "    chunks: List[str] = []\n",
    "    current_chunk: List[str] = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "        # Start new chunk if adding this sentence would exceed limit\n",
    "        if current_chunk and (current_length + sentence_length + 1) > max_chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_length + 1\n",
    "    \n",
    "    # Add the final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_text_simple(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into individual sentences.\n",
    "    \n",
    "    Recommended for TTS - provides maximum control with one sentence per chunk.\n",
    "    \n",
    "    Args:\n",
    "        text: The input string to chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of individual sentences\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "\n",
    "def chunk_text_with_count(text: str, sentences_per_chunk: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks containing a specific number of sentences.\n",
    "    \n",
    "    Args:\n",
    "        text: The input string to chunk\n",
    "        sentences_per_chunk: Number of sentences to include in each chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    chunks: List[str] = []\n",
    "    \n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = ' '.join(sentences[i:i + sentences_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "print(\"‚úÖ Text chunking utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core-functions-header",
   "metadata": {},
   "source": [
    "## 8. Core TTS Functions\n",
    "\n",
    "These are the main functions that handle the text-to-speech conversion process.\n",
    "\n",
    "### Function Pipeline:\n",
    "1. **get_tts_tokens**: Generates audio tokens from text using the LLM\n",
    "2. **generate_speech_from_text**: Extracts semantic and global tokens\n",
    "3. **generate_speech_segment_with_retry**: Converts tokens to audio with retry logic\n",
    "4. **get_speech_segments**: Processes multiple text chunks\n",
    "5. **text_to_speech**: Main function that orchestrates the entire pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "core-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tts_tokens(text: str, speaker_id: int, temperature: float, model) -> str:\n",
    "    \"\"\"\n",
    "    Generate TTS tokens from input text using the model.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to synthesize\n",
    "        speaker_id: Speaker voice ID (e.g., 248 for Luganda, 246 for Swahili)\n",
    "        temperature: Sampling temperature for generation (0.1-1.0)\n",
    "        model: The loaded vLLM model\n",
    "    \n",
    "    Returns:\n",
    "        String containing audio tokens in special format\n",
    "    \"\"\"\n",
    "    sampling_params = SamplingParams(temperature=temperature, max_tokens=2048)\n",
    "    \n",
    "    # Format prompt with task identifier and speaker ID\n",
    "    prompt = f\"<|task_tts|><|start_content|>{speaker_id}: {text}<|end_content|><|start_global_token|>\"\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        prompts=prompt,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    audio_tokens = outputs[0].outputs[0].text\n",
    "    return audio_tokens\n",
    "\n",
    "\n",
    "def generate_speech_from_text(\n",
    "    text: str,\n",
    "    speaker_id: int = 248,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_audio_tokens: int = 2000,\n",
    "    model=None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generate semantic and global tokens from text.\n",
    "    \n",
    "    The model generates two types of tokens:\n",
    "    - Semantic tokens: Capture linguistic content\n",
    "    - Global tokens: Capture prosody and speaker characteristics\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to synthesize\n",
    "        speaker_id: Speaker voice ID\n",
    "        temperature: Sampling temperature\n",
    "        max_new_audio_tokens: Maximum tokens to generate\n",
    "        model: The loaded vLLM model\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (semantic_token_ids, global_token_ids)\n",
    "    \"\"\"\n",
    "    predicted_tokens = get_tts_tokens(\n",
    "        text=text,\n",
    "        speaker_id=speaker_id,\n",
    "        temperature=temperature,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    # Extract semantic token IDs using regex\n",
    "    semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicted_tokens)\n",
    "    if not semantic_matches:\n",
    "        raise ValueError(\"No semantic tokens found in the generated output.\")\n",
    "\n",
    "    pred_semantic_ids = (\n",
    "        torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0)\n",
    "    )\n",
    "\n",
    "    # Extract global token IDs using regex\n",
    "    global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicted_tokens)\n",
    "    if not global_matches:\n",
    "        print(\"Warning: No global tokens found. Using zeros as fallback.\")\n",
    "        pred_global_ids = torch.zeros((1, 1), dtype=torch.long)\n",
    "    else:\n",
    "        pred_global_ids = (\n",
    "            torch.tensor([int(token) for token in global_matches])\n",
    "            .long()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "\n",
    "    return pred_semantic_ids, pred_global_ids\n",
    "\n",
    "\n",
    "def generate_speech_segment_with_retry(\n",
    "    text: str,\n",
    "    audio_tokenizer,\n",
    "    model,\n",
    "    speaker_id: int = 248,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_audio_tokens: int = 2000,\n",
    "    max_retries: int = 3,\n",
    ") -> Optional[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate a single speech segment with automatic retry logic.\n",
    "    \n",
    "    This function handles common generation errors by retrying with slightly\n",
    "    different parameters if dimension mismatches occur.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to synthesize\n",
    "        audio_tokenizer: The audio tokenizer for detokenization\n",
    "        model: The TTS model\n",
    "        speaker_id: Speaker ID\n",
    "        temperature: Sampling temperature\n",
    "        max_new_audio_tokens: Maximum tokens to generate\n",
    "        max_retries: Maximum number of retry attempts\n",
    "        \n",
    "    Returns:\n",
    "        Audio waveform as numpy array, or None if all retries failed\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Generate tokens\n",
    "            pred_semantic_ids, pred_global_ids = generate_speech_from_text(\n",
    "                text=text,\n",
    "                speaker_id=speaker_id,\n",
    "                temperature=temperature,\n",
    "                max_new_audio_tokens=max_new_audio_tokens,\n",
    "                model=model,\n",
    "            )\n",
    "            \n",
    "            # Log token shapes for debugging\n",
    "            print(f\"   Attempt {attempt + 1}: semantic shape={pred_semantic_ids.shape}, \"\n",
    "                  f\"global shape={pred_global_ids.shape}\")\n",
    "            \n",
    "            # Detokenize to waveform\n",
    "            wav_np = audio_tokenizer.detokenize(\n",
    "                pred_global_ids.to(device), pred_semantic_ids.to(device)\n",
    "            )\n",
    "            \n",
    "            return wav_np\n",
    "            \n",
    "        except RuntimeError as e:\n",
    "            error_msg = str(e)\n",
    "            print(f\"‚ö†Ô∏è  Attempt {attempt + 1}/{max_retries} failed\")\n",
    "            print(f\"   Error: {error_msg}\")\n",
    "            \n",
    "            # Check if it's a dimension mismatch error\n",
    "            if \"cannot be multiplied\" in error_msg or \"shape\" in error_msg.lower():\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"   Retrying with adjusted temperature...\")\n",
    "                    # Slightly vary temperature to get different generation\n",
    "                    temperature = temperature + np.random.uniform(-0.05, 0.05)\n",
    "                    temperature = float(np.clip(temperature, 0.1, 1.0))\n",
    "                    time.sleep(0.5)\n",
    "                else:\n",
    "                    print(f\"   ‚ùå All {max_retries} attempts failed\")\n",
    "                    return None\n",
    "            else:\n",
    "                # Different error, re-raise\n",
    "                raise\n",
    "                \n",
    "        except ValueError as e:\n",
    "            print(f\"‚ö†Ô∏è  ValueError on attempt {attempt + 1}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"   Retrying...\")\n",
    "                time.sleep(0.5)\n",
    "            else:\n",
    "                print(f\"   ‚ùå All {max_retries} attempts failed\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_speech_segments(\n",
    "    text_chunks: List[str],\n",
    "    audio_tokenizer,\n",
    "    model,\n",
    "    speaker_id: int = 248,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_audio_tokens: int = 2000,\n",
    "    max_retries: int = 3,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Generate speech segments for multiple text chunks.\n",
    "    \n",
    "    Processes each chunk independently and adds silence for failed chunks.\n",
    "    \n",
    "    Args:\n",
    "        text_chunks: List of text strings to synthesize\n",
    "        audio_tokenizer: The audio tokenizer\n",
    "        model: The TTS model\n",
    "        speaker_id: Speaker ID\n",
    "        temperature: Sampling temperature\n",
    "        max_new_audio_tokens: Maximum tokens per chunk\n",
    "        max_retries: Maximum retry attempts per chunk\n",
    "        \n",
    "    Returns:\n",
    "        List of audio segments as numpy arrays\n",
    "    \"\"\"\n",
    "    segments = []\n",
    "    total_chunks = len(text_chunks)\n",
    "    \n",
    "    for i, text in enumerate(text_chunks, 1):\n",
    "        print(f\"\\nüìù Processing chunk {i}/{total_chunks}\")\n",
    "        print(f\"   Text: '{text[:60]}...'\" if len(text) > 60 else f\"   Text: '{text}'\")\n",
    "        \n",
    "        wav_np = generate_speech_segment_with_retry(\n",
    "            text=text,\n",
    "            audio_tokenizer=audio_tokenizer,\n",
    "            model=model,\n",
    "            speaker_id=speaker_id,\n",
    "            temperature=temperature,\n",
    "            max_new_audio_tokens=max_new_audio_tokens,\n",
    "            max_retries=max_retries,\n",
    "        )\n",
    "        \n",
    "        if wav_np is not None:\n",
    "            segments.append(wav_np)\n",
    "            print(f\"‚úÖ Chunk {i} completed successfully\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Chunk {i} failed. Adding silence placeholder...\")\n",
    "            # Add 500ms of silence\n",
    "            silence = np.zeros(int(16000 * 0.5), dtype=np.float32)\n",
    "            segments.append(silence)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "\n",
    "def text_to_speech(\n",
    "    text: str,\n",
    "    audio_tokenizer,\n",
    "    model,\n",
    "    chunk_text_simple,\n",
    "    speaker_id: int = 248,\n",
    "    temperature: float = 0.7,\n",
    "    max_new_audio_tokens: int = 2048,\n",
    "    sample_rate: int = 16000,\n",
    "    max_retries: int = 3,\n",
    ") -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Convert text to speech waveform - Main TTS function.\n",
    "    \n",
    "    This is the primary function you should use for TTS conversion.\n",
    "    It handles text chunking, generation, and concatenation automatically.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text to synthesize\n",
    "        audio_tokenizer: The audio tokenizer\n",
    "        model: The TTS model\n",
    "        chunk_text_simple: Function to chunk text into sentences\n",
    "        speaker_id: Speaker ID (default: 248)\n",
    "        temperature: Sampling temperature (default: 0.7)\n",
    "        max_new_audio_tokens: Maximum tokens per chunk (default: 2048)\n",
    "        sample_rate: Output sample rate (default: 16000 Hz)\n",
    "        max_retries: Maximum retry attempts per chunk (default: 3)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (waveform_array, sample_rate)\n",
    "    \"\"\"\n",
    "    # Chunk the text into sentences\n",
    "    texts = chunk_text_simple(text)\n",
    "    texts = [t.strip() for t in texts if len(t.strip()) > 0]\n",
    "    \n",
    "    print(f\"\\nüéôÔ∏è  Starting TTS conversion\")\n",
    "    print(f\"   Total chunks: {len(texts)}\")\n",
    "    print(f\"   Speaker ID: {speaker_id}\")\n",
    "    print(f\"   Temperature: {temperature}\")\n",
    "    \n",
    "    # Generate speech segments\n",
    "    speech_segments = get_speech_segments(\n",
    "        text_chunks=texts,\n",
    "        audio_tokenizer=audio_tokenizer,\n",
    "        model=model,\n",
    "        speaker_id=speaker_id,\n",
    "        temperature=temperature,\n",
    "        max_new_audio_tokens=max_new_audio_tokens,\n",
    "        max_retries=max_retries,\n",
    "    )\n",
    "    \n",
    "    # Concatenate all segments\n",
    "    if speech_segments:\n",
    "        result_wav = np.concatenate(speech_segments)\n",
    "        duration = len(result_wav) / sample_rate\n",
    "        print(f\"\\n‚úÖ TTS conversion completed!\")\n",
    "        print(f\"   Total duration: {duration:.2f} seconds\")\n",
    "        print(f\"   Waveform shape: {result_wav.shape}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  No speech segments generated. Returning silence.\")\n",
    "        result_wav = np.zeros(sample_rate, dtype=np.float32)\n",
    "    \n",
    "    return result_wav, sample_rate\n",
    "\n",
    "\n",
    "def save_wav(\n",
    "    text: str,\n",
    "    outfile: str,\n",
    "    audio_tokenizer,\n",
    "    model,\n",
    "    chunk_text_simple,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate speech and save to a WAV file.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        outfile: Output file path (e.g., 'output.wav')\n",
    "        audio_tokenizer: The audio tokenizer\n",
    "        model: The TTS model\n",
    "        chunk_text_simple: Text chunking function\n",
    "        **kwargs: Additional arguments for text_to_speech function\n",
    "    \"\"\"\n",
    "    wav, sr = text_to_speech(\n",
    "        text=text,\n",
    "        audio_tokenizer=audio_tokenizer,\n",
    "        model=model,\n",
    "        chunk_text_simple=chunk_text_simple,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    sf.write(outfile, wav, sr)\n",
    "    print(f\"\\nüíæ Audio saved to: {outfile}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Core TTS functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage-header",
   "metadata": {},
   "source": [
    "## 9. Usage Examples\n",
    "\n",
    "Now let's see how to use the TTS system with different examples.\n",
    "\n",
    "### Speaker IDs Reference:\n",
    "- **248**: Luganda speaker\n",
    "- **246**: Swahili speaker\n",
    "- *More speaker IDs available in the model documentation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-header",
   "metadata": {},
   "source": [
    "### Example 1: English Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample English text\n",
    "english_text = (\n",
    "    \"Hello, I'm Prosi Nafula. I am a nurse who takes care of many people who have cancer \"\n",
    "    \"and who have questions about their illness and what to expect. There are many types of cancer. \"\n",
    "    \"The type of cancer you have is named after the place where it started. For example, if cancer \"\n",
    "    \"starts in the breast then it is called breast cancer. Cancer doesn't spread from one person to \"\n",
    "    \"another but it can spread through your own body. All cancers need to be treated.\"\n",
    ")\n",
    "\n",
    "print(f\"Text length: {len(english_text)} characters\")\n",
    "print(f\"Text preview: {english_text[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1-generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate speech (adjust speaker_id as needed)\n",
    "speaker_id = 248  # Use appropriate speaker ID\n",
    "temperature = 0.7\n",
    "\n",
    "result_wav, sr = text_to_speech(\n",
    "    text=english_text,\n",
    "    audio_tokenizer=audio_tokenizer,\n",
    "    model=model,\n",
    "    chunk_text_simple=chunk_text_simple,\n",
    "    speaker_id=speaker_id,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1-play",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the generated audio\n",
    "display(Audio(result_wav, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to file\n",
    "# sf.write('output_english.wav', result_wav, sr)\n",
    "# print(\"Audio saved to output_english.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2-header",
   "metadata": {},
   "source": [
    "### Example 2: Luganda Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Luganda text\n",
    "luganda_text = (\n",
    "    \"Nze Prosi Nafula. Ndi musawo akola ku bantu abalina kookolo era abalina ebibuuzo ku bulwadde bwabwe n'ekyo kye basuubira. \"\n",
    "    \"Waliwo ebika bya kookolo bingi. Ekika kya kookolo ky'olina kiyitibwa erinnya ly'ekifo we kyatandikira. \"\n",
    "    \"Okugeza, kookolo bw'atandikira mu mabeere, ayitibwa kookolo w'amabeere. Kookolo tasaasaana okuva ku muntu omu okudda ku mulala \"\n",
    "    \"naye asobola okusaasaana mu mubiri gwo. Kkookolo yenna yeetaaga okujjanjabibwa.\"\n",
    ")\n",
    "\n",
    "print(f\"Text length: {len(luganda_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2-generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate Luganda speech\n",
    "speaker_id = 248  # Luganda speaker\n",
    "temperature = 0.7\n",
    "\n",
    "result_wav_luganda, sr = text_to_speech(\n",
    "    text=luganda_text,\n",
    "    audio_tokenizer=audio_tokenizer,\n",
    "    model=model,\n",
    "    chunk_text_simple=chunk_text_simple,\n",
    "    speaker_id=speaker_id,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2-play",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the generated Luganda audio\n",
    "display(Audio(result_wav_luganda, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3-header",
   "metadata": {},
   "source": [
    "### Example 3: Swahili Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Swahili text\n",
    "swahili_text = (\n",
    "    \"Habari, naitwa Prosi Nafula. Mimi ni muuguzi ambaye hushughulikia watu wengi walio na saratani \"\n",
    "    \"na ambao wana maswali kuhusu ugonjwa wao na kile wanachoweza kutarajia. Kuna aina nyingi za saratani. \"\n",
    "    \"Aina ya saratani unayokuwa nayo inaitwa kwa jina la mahali ilipoanza. Kwa mfano, saratani ikiwa imeanza \"\n",
    "    \"katika matiti basi inaitwa saratani ya matiti. Saratani haisambaii kutoka mtu mmoja hadi mwingine lakini \"\n",
    "    \"inaweza kusambaa katika mwili wako. Kansa zote zinahitaji kutibiwa.\"\n",
    ")\n",
    "\n",
    "print(f\"Text length: {len(swahili_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3-generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate Swahili speech\n",
    "speaker_id = 246  # Swahili speaker\n",
    "temperature = 0.7\n",
    "\n",
    "result_wav_swahili, sr = text_to_speech(\n",
    "    text=swahili_text,\n",
    "    audio_tokenizer=audio_tokenizer,\n",
    "    model=model,\n",
    "    chunk_text_simple=chunk_text_simple,\n",
    "    speaker_id=speaker_id,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3-play",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the generated Swahili audio\n",
    "display(Audio(result_wav_swahili, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-usage-header",
   "metadata": {},
   "source": [
    "## 10. Custom Usage\n",
    "\n",
    "Use this cell to generate speech from your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your custom text here\n",
    "my_text = \"Your text goes here.\"\n",
    "\n",
    "# Configure parameters\n",
    "my_speaker_id = 248  # Choose appropriate speaker ID\n",
    "my_temperature = 0.7  # 0.1 (conservative) to 1.0 (creative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate speech\n",
    "my_wav, my_sr = text_to_speech(\n",
    "    text=my_text,\n",
    "    audio_tokenizer=audio_tokenizer,\n",
    "    model=model,\n",
    "    chunk_text_simple=chunk_text_simple,\n",
    "    speaker_id=my_speaker_id,\n",
    "    temperature=my_temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-play",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play your audio\n",
    "display(Audio(my_wav, rate=my_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file (optional)\n",
    "output_filename = 'my_tts_output.wav'\n",
    "sf.write(output_filename, my_wav, my_sr)\n",
    "print(f\"‚úÖ Audio saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tips-header",
   "metadata": {},
   "source": [
    "## 11. Tips and Best Practices\n",
    "\n",
    "### Temperature Settings:\n",
    "- **0.1-0.3**: More consistent but potentially monotone\n",
    "- **0.5-0.7**: Balanced (recommended)\n",
    "- **0.8-1.0**: More varied but potentially less stable\n",
    "\n",
    "### Text Chunking Strategies:\n",
    "- **chunk_text_simple**: Best for most use cases (one sentence per chunk)\n",
    "- **chunk_text**: Good for controlling chunk size\n",
    "- **chunk_text_with_count**: Good for grouping related sentences\n",
    "\n",
    "### Handling Errors:\n",
    "- The system automatically retries failed chunks up to 3 times\n",
    "- Failed chunks are replaced with silence to maintain timing\n",
    "- Adjust `max_retries` parameter if needed\n",
    "\n",
    "### Performance Tips:\n",
    "- Longer texts take more time to process\n",
    "- GPU acceleration significantly speeds up generation\n",
    "- Consider breaking very long texts into multiple batches\n",
    "\n",
    "### Common Issues:\n",
    "1. **Dimension mismatch errors**: Usually resolved by retry logic\n",
    "2. **No audio output**: Check speaker_id and ensure text is not empty\n",
    "3. **Poor quality**: Try adjusting temperature or using different speaker_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "You now have a complete TTS inference pipeline using Spark TTS with vLLM!\n",
    "\n",
    "### Key Features Covered:\n",
    "‚úÖ Model loading and initialization\n",
    "‚úÖ Audio tokenizer setup\n",
    "‚úÖ Text chunking strategies\n",
    "‚úÖ Robust generation with retry logic\n",
    "‚úÖ Multi-language support\n",
    "‚úÖ Audio playback and saving\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different speaker IDs\n",
    "- Try various temperature settings\n",
    "- Test with your own texts and languages\n",
    "- Integrate into your applications\n",
    "\n",
    "### Resources:\n",
    "- [Spark TTS GitHub](https://github.com/SparkAudio/Spark-TTS)\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [Model on Hugging Face](https://huggingface.co/jq/spark-tts-salt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
