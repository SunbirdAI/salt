{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Spark TTS Inference Guide with vLLM\n",
    "\n",
    "This notebook demonstrates how to perform text-to-speech (TTS) inference using the **Sunbird/spark-tts-salt** model with vLLM. Tested on RTX 4090 (24GB).\n",
    "\n",
    "## Overview\n",
    "\n",
    "Spark TTS is a powerful text-to-speech model that can generate high-quality speech in multiple languages and voices. This implementation uses:\n",
    "- **vLLM** for efficient model inference\n",
    "- **BiCodec tokenizer** for audio token processing\n",
    "- **Retry logic** to handle generation errors gracefully\n",
    "- **Text chunking** to process long texts efficiently\n",
    "\n",
    "## Key Features\n",
    "- Multi-language support (English, Luganda, Swahili, etc.)\n",
    "- Multiple speaker IDs for different voices\n",
    "- Robust error handling with automatic retries\n",
    "- Flexible text chunking strategies\n",
    "- Audio playback in Jupyter notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, install all required dependencies. This includes:\n",
    "- **vLLM**: For efficient LLM inference\n",
    "- **soundfile & librosa**: Audio processing\n",
    "- **xformers, omegaconf, einx, einops**: Supporting libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4350ef6a-3e39-4584-b2a7-ab517330e656",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q einx einops soundfile librosa vllm omegaconf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-header",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Import all necessary libraries for TTS inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "import os\n",
    "from getpass import getpass\n",
    "import re\n",
    "import soundfile as sf\n",
    "from huggingface_hub import snapshot_download\n",
    "import torch\n",
    "import sys\n",
    "from typing import Tuple, List, Optional\n",
    "import numpy as np\n",
    "from IPython.display import Audio, display\n",
    "import time\n",
    "import huggingface_hub\n",
    "\n",
    "# Determine device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sparktts-setup",
   "metadata": {},
   "source": [
    "## 3. Clone Spark-TTS Repository\n",
    "\n",
    "Clone the Spark-TTS repository to access the audio tokenizer and utilities.\n",
    "\n",
    "**Note**: Uncomment the git clone line if you haven't cloned the repository yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clone-repo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Spark-TTS repository (uncomment if needed)\n",
    "if not os.path.exists('Spark-TTS'):\n",
    "    !git clone https://github.com/SparkAudio/Spark-TTS\n",
    "\n",
    "# Add Spark-TTS to Python path\n",
    "sys.path.append('Spark-TTS')\n",
    "print(\"Spark-TTS repository path added to sys.path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hf-token-header",
   "metadata": {},
   "source": [
    "## 4. Set Hugging Face Token\n",
    "\n",
    "Set your Hugging Face token for model access. Get your token from: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hf-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-load-header",
   "metadata": {},
   "source": [
    "## 5. Load the TTS Model\n",
    "\n",
    "Load the Spark TTS model using vLLM.\n",
    "\n",
    "**Model**: `Sunbird/spark-tts-salt`\n",
    "\n",
    "This may take a few minutes depending on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TTS model with vLLM\n",
    "print(\"Loading Spark TTS model...\")\n",
    "model = LLM(\n",
    "    \"Sunbird/spark-tts-salt\",\n",
    "    enforce_eager=False,\n",
    "    gpu_memory_utilization=0.5) # Leave some VRAM for the audio tokeniser\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-header",
   "metadata": {},
   "source": [
    "## 6. Download and Setup Audio Tokenizer\n",
    "\n",
    "Download the BiCodec tokenizer model files from Hugging Face and initialize the audio tokenizer.\n",
    "\n",
    "The tokenizer converts between audio and token representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "download-tokenizer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download tokenizer model files\n",
    "model_base_repo = \"unsloth/Spark-TTS-0.5B\"\n",
    "cache_dir = \"Spark-TTS-0.5B\"\n",
    "\n",
    "if not os.path.exists(cache_dir):\n",
    "    print(f\"Downloading tokenizer files from {model_base_repo}...\")\n",
    "    snapshot_download(\n",
    "        repo_id=model_base_repo,\n",
    "        local_dir=cache_dir,\n",
    "        ignore_patterns=[\"*LLM*\"],  # Skip LLM files, we only need tokenizer\n",
    "    )\n",
    "    print(f\"✅ Tokenizer files downloaded to {cache_dir}\")\n",
    "\n",
    "# Initialize the audio tokenizer\n",
    "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
    "\n",
    "print(\"Initializing audio tokenizer...\")\n",
    "audio_tokenizer = BiCodecTokenizer(cache_dir, device)\n",
    "print(\"✅ Audio tokenizer initialized!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utilities-header",
   "metadata": {},
   "source": [
    "## 7. Text Chunking Utilities\n",
    "\n",
    "These functions split long text into manageable chunks for TTS processing.\n",
    "\n",
    "### Three Chunking Strategies:\n",
    "\n",
    "1. **chunk_text**: Splits by sentence boundaries with a maximum character limit\n",
    "2. **chunk_text_simple**: Splits into individual sentences (recommended for TTS)\n",
    "3. **chunk_text_with_count**: Groups a fixed number of sentences per chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "text-chunking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_chunk_size: int = 500) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks based on sentence boundaries.\n",
    "    \n",
    "    This approach preserves natural sentence flow and intonation for TTS.\n",
    "    \n",
    "    Args:\n",
    "        text: The input string to chunk\n",
    "        max_chunk_size: Maximum character length per chunk (soft limit)\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks, each containing one or more complete sentences\n",
    "    \"\"\"\n",
    "    # Split on sentence-ending punctuation (. ! ?) followed by whitespace\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    \n",
    "    chunks: List[str] = []\n",
    "    current_chunk: List[str] = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip()\n",
    "        if not sentence:\n",
    "            continue\n",
    "        \n",
    "        sentence_length = len(sentence)\n",
    "        \n",
    "        # Start new chunk if adding this sentence would exceed limit\n",
    "        if current_chunk and (current_length + sentence_length + 1) > max_chunk_size:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        \n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_length + 1\n",
    "    \n",
    "    # Add the final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def chunk_text_simple(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into individual sentences.\n",
    "    \n",
    "    Recommended for TTS - provides maximum control with one sentence per chunk.\n",
    "    \n",
    "    Args:\n",
    "        text: The input string to chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of individual sentences\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "\n",
    "def chunk_text_with_count(text: str, sentences_per_chunk: int = 3) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks containing a specific number of sentences.\n",
    "    \n",
    "    Args:\n",
    "        text: The input string to chunk\n",
    "        sentences_per_chunk: Number of sentences to include in each chunk\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    chunks: List[str] = []\n",
    "    \n",
    "    for i in range(0, len(sentences), sentences_per_chunk):\n",
    "        chunk = ' '.join(sentences[i:i + sentences_per_chunk])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e20c98-feab-40c7-90f9-37eeb4f29a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precomputed global tokens for each speaker\n",
    "GLOBAL_IDS_BY_SPEAKER = {\n",
    " 241: [1755, 1265, 184, 3545, 2718, 2405, 3237, 1360, 3621, 1850, 37, 3382, 736,\n",
    "       3380, 3131, 2036, 244, 2128, 254, 2550, 3181, 764, 1277, 502, 2941, 1993,\n",
    "       3556, 1428, 3505, 3245, 3506, 1540],\n",
    " 242: [1367, 1522, 308, 4061, 1449, 2468, 2193, 1349, 3458, 2339, 1651, 3174,\n",
    "       501, 3364, 3194, 2041, 442, 1061, 502, 2234, 2397, 358, 3829, 2490, 2031,\n",
    "       1002, 3548, 586, 3445, 1419, 4093, 2908],\n",
    " 243: [2051, 242, 2684, 4062, 2654, 2252, 353, 3657, 2759, 3254, 1649, 3366,\n",
    "       1017, 3600, 3131, 3813, 1535, 1595, 1059, 237, 2158, 1174, 4085, 2174,\n",
    "       3791, 990, 3274, 2693, 3829, 2271, 2650, 1689],\n",
    " 245: [2031, 2545, 116, 4060, 746, 1385, 3301, 1312, 3638, 1846, 85, 3190, 1016,\n",
    "       3384, 3134, 954, 244, 1104, 235, 2549, 3357, 508, 1278, 1974, 2621, 1896,\n",
    "       3812, 2185, 3061, 2941, 1187, 5],\n",
    " 246: [1811, 1138, 2873, 3309, 2639, 723, 3363, 974, 1612, 2531, 1769, 3376,\n",
    "       933, 3848, 3195, 2180, 2359, 1275, 3493, 3260, 2279, 3715, 3508, 2433,\n",
    "       4082, 1087, 3545, 1449, 160, 3531, 2908, 2094],\n",
    " 248: [2559, 1523, 440, 3789, 1438, 373, 2212, 1248, 3369, 1847, 36, 3126, 480,\n",
    "       3380, 3133, 2041, 248, 2384, 730, 2554, 3182, 1785, 1277, 1013, 2425,\n",
    "       1932, 3560, 1177, 2736, 2430, 2722, 261]\n",
    "}\n",
    "\n",
    "def text_to_speech(text, audio_tokenizer, model, speaker_id, temperature):\n",
    "    '''Create a wav array of speech from text.'''\n",
    "    texts = chunk_text_simple(text)\n",
    "    texts = [t.strip() for t in texts if len(t.strip()) > 0]\n",
    "\n",
    "    sampling_params = SamplingParams(temperature=temperature, max_tokens=2048)\n",
    "    \n",
    "    global_tokens = GLOBAL_IDS_BY_SPEAKER[speaker_id]\n",
    "    \n",
    "    prompts = []\n",
    "    for text in texts:\n",
    "        prompt = f\"<|task_tts|><|start_content|>{speaker_id}: {text}<|end_content|><|start_global_token|>\"\n",
    "        prompt += ''.join([f'<|bicodec_global_{t}|>' for t in global_tokens]) + '<|end_global_token|><|start_semantic_token|>'\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        prompts=prompts,\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "    \n",
    "    speech_segments = []\n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "        predicted_tokens = outputs[i].outputs[0].text\n",
    "        semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicted_tokens)\n",
    "        if not semantic_matches:\n",
    "            raise ValueError(\"No semantic tokens found in the generated output.\")\n",
    "        \n",
    "        pred_semantic_ids = (\n",
    "            torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        pred_global_ids = torch.Tensor([global_tokens]).long()\n",
    "        \n",
    "        wav_np = audio_tokenizer.detokenize(\n",
    "            pred_global_ids.to(device), pred_semantic_ids.to(device)\n",
    "        )\n",
    "        speech_segments.append(wav_np)\n",
    "    \n",
    "    result_wav = np.concatenate(speech_segments)\n",
    "    \n",
    "    return result_wav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4015f03-ec8e-4258-bb92-e09703aeb9f5",
   "metadata": {},
   "source": [
    "## Usage examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example1-header",
   "metadata": {},
   "source": [
    "### Example 1: English Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c19e86-cfa8-458e-8399-d89bec093f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "text = (\"Hello, I'm Prosi Nafula. I am a nurse who takes care of many people who have cancer \"\n",
    "    \"and who have questions about their illness and what to expect. There are many types of cancer. \"\n",
    "    \"The type of cancer you have is named after the place where it started. For example, if cancer \"\n",
    "    \"starts in the breast then it is called breast cancer. Cancer doesn't spread from one person to \"\n",
    "    \"another but it can spread through your own body. All cancers need to be treated.\")\n",
    "\n",
    "# Optional long-form test: generate an hour's worth of speech (should take ~20 seconds)\n",
    "if False:\n",
    "    import urllib.request\n",
    "    # THE ANALYSIS OF MIND By Bertrand Russell\n",
    "    url = \"https://www.gutenberg.org/cache/epub/2529/pg2529.txt\"\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        full_text = response.read().decode('utf-8')\n",
    "    book_text = ' '.join(full_text.split()[610:12000])\n",
    "    \n",
    "\n",
    "# 241: Acholi (female)\n",
    "# 242: Ateso (female)\n",
    "# 243: Runyankore (female)\n",
    "# 245: Lugbara (female)\n",
    "# 246: Swahili (male)\n",
    "# 248: Luganda (female)\n",
    "\n",
    "speaker_id = 241  \n",
    "temperature = 0.7\n",
    "\n",
    "result_wav = text_to_speech(\n",
    "    text=text,\n",
    "    audio_tokenizer=audio_tokenizer,\n",
    "    model=model,\n",
    "    speaker_id=speaker_id,\n",
    "    temperature=temperature\n",
    ")\n",
    "\n",
    "duration = len(result_wav) / 16000\n",
    "print(f\"\\n✅ TTS conversion completed!\")\n",
    "print(f\"   Total duration: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2041835c-ec23-435c-a891-5e517a856010",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Audio(result_wav, rate=16000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example1-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save to file\n",
    "if False:\n",
    "    sf.write('output_english.wav', result_wav, sr)\n",
    "    print(\"Audio saved to output_english.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example2-header",
   "metadata": {},
   "source": [
    "### Example 2: Luganda Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Luganda text\n",
    "luganda_text = (\n",
    "    \"Nze Prosi Nafula. Ndi musawo akola ku bantu abalina kookolo era abalina ebibuuzo ku bulwadde bwabwe n'ekyo kye basuubira. \"\n",
    "    \"Waliwo ebika bya kookolo bingi. Ekika kya kookolo ky'olina kiyitibwa erinnya ly'ekifo we kyatandikira. \"\n",
    "    \"Okugeza, kookolo bw'atandikira mu mabeere, ayitibwa kookolo w'amabeere. Kookolo tasaasaana okuva ku muntu omu okudda ku mulala \"\n",
    "    \"naye asobola okusaasaana mu mubiri gwo. Kkookolo yenna yeetaaga okujjanjabibwa.\"\n",
    ")\n",
    "\n",
    "print(f\"Text length: {len(luganda_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2-generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate Luganda speech\n",
    "speaker_id = 248  # Luganda speaker\n",
    "temperature = 0.7\n",
    "\n",
    "result_wav_luganda = text_to_speech(\n",
    "    text=luganda_text,\n",
    "    audio_tokenizer=audio_tokenizer,\n",
    "    model=model,\n",
    "    speaker_id=speaker_id,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example2-play",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the generated Luganda audio\n",
    "display(Audio(result_wav_luganda, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example3-header",
   "metadata": {},
   "source": [
    "### Example 3: Swahili Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Swahili text\n",
    "swahili_text = (\n",
    "    \"Habari, naitwa Prosi Nafula. Mimi ni muuguzi ambaye hushughulikia watu wengi walio na saratani \"\n",
    "    \"na ambao wana maswali kuhusu ugonjwa wao na kile wanachoweza kutarajia. Kuna aina nyingi za saratani. \"\n",
    "    \"Aina ya saratani unayokuwa nayo inaitwa kwa jina la mahali ilipoanza. Kwa mfano, saratani ikiwa imeanza \"\n",
    "    \"katika matiti basi inaitwa saratani ya matiti. Saratani haisambaii kutoka mtu mmoja hadi mwingine lakini \"\n",
    "    \"inaweza kusambaa katika mwili wako. Kansa zote zinahitaji kutibiwa.\"\n",
    ")\n",
    "\n",
    "print(f\"Text length: {len(swahili_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3-generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate Swahili speech\n",
    "speaker_id = 246  # Swahili speaker\n",
    "temperature = 0.7\n",
    "\n",
    "result_wav_swahili = text_to_speech(\n",
    "    text=swahili_text,\n",
    "    audio_tokenizer=audio_tokenizer,\n",
    "    model=model,\n",
    "    speaker_id=speaker_id,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example3-play",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the generated Swahili audio\n",
    "display(Audio(result_wav_swahili, rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "custom-usage-header",
   "metadata": {},
   "source": [
    "## 10. Custom Usage\n",
    "\n",
    "Use this cell to generate speech from your own text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your custom text here\n",
    "my_text = \"Your text goes here.\"\n",
    "\n",
    "# Configure parameters\n",
    "my_speaker_id = 248  # Choose appropriate speaker ID\n",
    "my_temperature = 0.7  # 0.1 (conservative) to 1.0 (creative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-generate",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Generate speech\n",
    "my_wav, my_sr = text_to_speech(\n",
    "    text=my_text,\n",
    "    audio_tokenizer=audio_tokenizer,\n",
    "    model=model,\n",
    "    speaker_id=my_speaker_id,\n",
    "    temperature=my_temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-play",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play your audio\n",
    "display(Audio(my_wav, rate=my_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "custom-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file (optional)\n",
    "output_filename = 'my_tts_output.wav'\n",
    "sf.write(output_filename, my_wav, my_sr)\n",
    "print(f\"✅ Audio saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a56bd9-1498-4f0c-87e0-2273125538d3",
   "metadata": {},
   "source": [
    "## Re-calculate the global tokens for each speaker.\n",
    "\n",
    "These tokens determine e.g. speaker speed and pitch, and stay roughly the same, so we precompute them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55165f49-0757-4e8d-bd37-2d40f9767a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 241: Acholi (female)\n",
    "# 242: Ateso (female)\n",
    "# 243: Runyankore (female)\n",
    "# 245: Lugbara (female)\n",
    "# 246: Swahili (male)\n",
    "# 248: Luganda (female)\n",
    "\n",
    "global_ids_by_speaker = {}\n",
    "\n",
    "for speaker_id in [241, 242, 243, 245, 246, 248]:\n",
    "\n",
    "    text = \"I am a nurse who takes care of many people who have cancer and who have questions about their illness and what to expect.\"\n",
    "    prompt = f\"<|task_tts|><|start_content|>{speaker_id}: {text}<|end_content|><|start_global_token|>\"\n",
    "    outputs = model.generate(\n",
    "        prompts=[prompt],\n",
    "        sampling_params=sampling_params\n",
    "    )\n",
    "    predicted_tokens = outputs[0].outputs[0].text\n",
    "    global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicted_tokens)\n",
    "    if not global_matches:\n",
    "        print(\"Warning: No global tokens found. Using zeros as fallback.\")\n",
    "        pred_global_ids = torch.zeros((1, 1), dtype=torch.long)\n",
    "    else:\n",
    "        pred_global_ids = (\n",
    "            torch.tensor([int(token) for token in global_matches])\n",
    "            .long()\n",
    "            .unsqueeze(0)\n",
    "        )\n",
    "    global_ids_by_speaker[speaker_id] = [int(t) for t in pred_global_ids.numpy()[0]]   \n",
    "\n",
    "import pprint\n",
    "pprint.pp(global_ids_by_speaker, width=80, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tips-header",
   "metadata": {},
   "source": [
    "## 11. Tips and Best Practices\n",
    "\n",
    "### Temperature Settings:\n",
    "- **0.1-0.3**: More consistent but potentially monotone\n",
    "- **0.5-0.7**: Balanced (recommended)\n",
    "- **0.8-1.0**: More varied but potentially less stable\n",
    "\n",
    "### Text Chunking Strategies:\n",
    "- **chunk_text_simple**: Best for most use cases (one sentence per chunk)\n",
    "- **chunk_text**: Good for controlling chunk size\n",
    "- **chunk_text_with_count**: Good for grouping related sentences\n",
    "\n",
    "### Handling Errors:\n",
    "- The system automatically retries failed chunks up to 3 times\n",
    "- Failed chunks are replaced with silence to maintain timing\n",
    "- Adjust `max_retries` parameter if needed\n",
    "\n",
    "### Performance Tips:\n",
    "- Longer texts take more time to process\n",
    "- GPU acceleration significantly speeds up generation\n",
    "- Consider breaking very long texts into multiple batches\n",
    "\n",
    "### Common Issues:\n",
    "1. **Dimension mismatch errors**: Usually resolved by retry logic\n",
    "2. **No audio output**: Check speaker_id and ensure text is not empty\n",
    "3. **Poor quality**: Try adjusting temperature or using different speaker_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-header",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "You now have a complete TTS inference pipeline using Spark TTS with vLLM!\n",
    "\n",
    "### Key Features Covered:\n",
    "✅ Model loading and initialization\n",
    "✅ Audio tokenizer setup\n",
    "✅ Text chunking strategies\n",
    "✅ Robust generation with retry logic\n",
    "✅ Multi-language support\n",
    "✅ Audio playback and saving\n",
    "\n",
    "### Next Steps:\n",
    "- Experiment with different speaker IDs\n",
    "- Try various temperature settings\n",
    "- Test with your own texts and languages\n",
    "- Integrate into your applications\n",
    "\n",
    "### Resources:\n",
    "- [Spark TTS GitHub](https://github.com/SparkAudio/Spark-TTS)\n",
    "- [vLLM Documentation](https://docs.vllm.ai/)\n",
    "- [Model on Hugging Face](https://huggingface.co/jq/spark-tts-salt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
