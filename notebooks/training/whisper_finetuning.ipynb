{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on RTX 6000Ada 48GB (per-device batch size 2) and H100 80GB (per-device batch size 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZLQq9FJMWLX"
   },
   "outputs": [],
   "source": [
    "!pip install -q jiwer\n",
    "!pip install -q evaluate\n",
    "!pip install -qU accelerate\n",
    "!pip install -Uq torch\n",
    "!pip install -Uq torchvision\n",
    "!pip install -q transformers[torch]\n",
    "!pip install -q soundfile\n",
    "!git clone https://github.com/jqug/salt.git\n",
    "!pip install -qr salt/requirements.txt\n",
    "!pip install -q peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wWOTwrVCqnF"
   },
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "use_mlflow = True\n",
    "\n",
    "import importlib.metadata\n",
    "installed = [\n",
    "    dist.metadata['Name']\n",
    "    for dist in importlib.metadata.distributions()\n",
    "]\n",
    "\n",
    "if use_wandb:\n",
    "  !pip install -q wandb\n",
    "  import wandb\n",
    "  %set_env WANDB_LOG_MODEL=True\n",
    "  %set_env WANDB_WATCH=all\n",
    "  %set_env WANDB_NOTEBOOK_NAME=whisper_base_en_sb.ipynb\n",
    "  wandb.login()\n",
    "\n",
    "if use_mlflow:\n",
    "  if 'mlflow' not in installed:\n",
    "      !pip install -q mlflow\n",
    "      ## requirements to log system/GPU metrics in mlflow\n",
    "  !pip install -q psutil\n",
    "  !pip install -q pynvml\n",
    "  import os\n",
    "  from getpass import getpass\n",
    "  import mlflow\n",
    "  import mlflow.pytorch\n",
    "  from mlflow import MlflowClient\n",
    "\n",
    "  # Set MLflow tracking credentials\n",
    "  MLFLOW_TRACKING_USERNAME = getpass('Enter the MLFLOW_TRACKING_USERNAME: ')\n",
    "  os.environ['MLFLOW_TRACKING_USERNAME'] = MLFLOW_TRACKING_USERNAME\n",
    "\n",
    "  MLFLOW_TRACKING_PASSWORD = getpass('Enter the MLFLOW_TRACKING_PASSWORD: ')\n",
    "  os.environ['MLFLOW_TRACKING_PASSWORD'] = MLFLOW_TRACKING_PASSWORD\n",
    "\n",
    "  # Set the MLflow tracking URI\n",
    "  mlflow.set_tracking_uri('https://mlflow-sunbird-ce0ecfc14244.herokuapp.com/')\n",
    "  mlflow.system_metrics.enable_system_metrics_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KKzjwqnb6Dh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Union, List, Dict, Any\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "import datasets\n",
    "import numpy as np\n",
    "import yaml\n",
    "import evaluate\n",
    "import salt.dataset\n",
    "import salt.metrics\n",
    "import salt.constants\n",
    "from salt.utils import DataCollatorCTCWithPadding as dcwp\n",
    "import huggingface_hub\n",
    "import peft\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlbdSLfKNYfF"
   },
   "outputs": [],
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf salt\n",
    "# !git clone https://github.com/jqug/salt.git\n",
    "from importlib import reload\n",
    "reload(salt.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1Vd4A4UIwLk"
   },
   "outputs": [],
   "source": [
    "yaml_config = f'''\n",
    "pretrained_model: openai/whisper-large-v3\n",
    "mlflow_experiment_name : stt-whisper\n",
    "\n",
    "use_peft: False\n",
    "lora_config:\n",
    "    r: 32\n",
    "    lora_alpha: 64\n",
    "    target_modules: [\"q_proj\", \"v_proj\"]\n",
    "    lora_dropout: 0.05\n",
    "    bias: \"none\"\n",
    "\n",
    "training_args:\n",
    "    output_dir: whisper-large-v3-multilingual\n",
    "    per_device_train_batch_size: 2\n",
    "    per_device_eval_batch_size: 2\n",
    "    gradient_accumulation_steps: 32  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate: 1.0e-5\n",
    "    warmup_steps: 500\n",
    "    max_steps: 7500\n",
    "    gradient_checkpointing: True\n",
    "    gradient_checkpointing_kwargs:\n",
    "      use_reentrant: True\n",
    "    fp16: True\n",
    "    eval_strategy: steps\n",
    "    predict_with_generate: True\n",
    "    generation_max_length: 100\n",
    "    save_steps: 250\n",
    "    eval_steps: 250\n",
    "    logging_steps: 250\n",
    "    load_best_model_at_end: True\n",
    "    metric_for_best_model: loss\n",
    "    greater_is_better: False\n",
    "    push_to_hub: True\n",
    "    hub_model_id: jq/whisper-large-v3-salt-plus-xog-myx-kin-swa\n",
    "    save_total_limit: 2\n",
    "    \n",
    "train:\n",
    "    download_datasets_in_parallel: True\n",
    "    huggingface_load:\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: common-voice-sample-packed-lug\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: common-voice-sample-packed-swa\n",
    "          split: train[:-25]\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: common-voice-sample-packed-kin\n",
    "          split: train[:-25]\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: makerere-radio-speech\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: makerere-yogera-ach\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: makerere-yogera-lug\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: makerere-yogera-nyn\n",
    "        # Save some myx and xog data for validation\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: makerere-yogera-myx\n",
    "          split: train[:-100]\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: makerere-yogera-xog\n",
    "          split: train[:-100]\n",
    "        # Real-world data\n",
    "        - path: Sunbird/salt-ucfd\n",
    "          name: eng\n",
    "          split: train\n",
    "        - path: Sunbird/salt-ucfd\n",
    "          name: lug\n",
    "          split: train    \n",
    "        - path: Sunbird/salt-ucfd\n",
    "          name: numbers-eng\n",
    "          split: train\n",
    "        - path: Sunbird/salt-ucfd\n",
    "          name: numbers-lug\n",
    "          split: train  \n",
    "        - path: Sunbird/salt-tracfm\n",
    "          name: lug\n",
    "          split: train\n",
    "        # Main SALT ASR training data\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-lug\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-eng\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-ach\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-lgg\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-teo\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-nyn\n",
    "          split: train\n",
    "        # Google FLEURS\n",
    "        - path: google/fleurs\n",
    "          split: train\n",
    "          name: lg_ug\n",
    "          trust_remote_code: True\n",
    "        - path: google/fleurs\n",
    "          split: train\n",
    "          name: sw_ke\n",
    "          trust_remote_code: True\n",
    "    source:\n",
    "      type: speech\n",
    "      language: [lug,eng,ach,lgg,teo,nyn,myx,xog,swa,kin]\n",
    "      preprocessing:\n",
    "        # Downsample some examples to 8KHz (to simulate phone audio) \n",
    "        - set_sample_rate:\n",
    "            rate: 8_000\n",
    "            p: 0.2\n",
    "        # Then upsample again\n",
    "        - set_sample_rate:\n",
    "            rate: 16_000\n",
    "        - normalize_audio\n",
    "        - augment_audio_speed:\n",
    "            low: 0.95\n",
    "            high: 1.15\n",
    "        - augment_audio_noise:\n",
    "            max_relative_amplitude: 0.5\n",
    "            noise_audio_repo:\n",
    "                path: Sunbird/urban-noise\n",
    "                name: small\n",
    "                split: train       \n",
    "    target:\n",
    "      type: text\n",
    "      preprocessing:\n",
    "        - ensure_text_ends_with_punctuation\n",
    "      language: [lug,eng,ach,lgg,teo,nyn,myx,xog,swa,kin]\n",
    "    shuffle: True\n",
    "validation:\n",
    "    huggingface_load:\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-eng\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-lug\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-ach\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-lgg\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-teo\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-nyn\n",
    "          split: dev\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: makerere-yogera-myx\n",
    "          split: train[-100:]\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: makerere-yogera-xog\n",
    "          split: train[-100:]\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: common-voice-sample-packed-swa\n",
    "          split: train[-25:]\n",
    "        - path: Sunbird/external-speech-data\n",
    "          name: common-voice-sample-packed-kin\n",
    "          split: train[-25:]\n",
    "    source:\n",
    "      type: speech\n",
    "      language: [lug,eng,ach,lgg,teo,nyn,myx,xog,swa,kin]\n",
    "      preprocessing:\n",
    "        - set_sample_rate:\n",
    "            rate: 16_000\n",
    "    target:\n",
    "      type: text\n",
    "      language: [lug,eng,ach,lgg,teo,nyn,myx,xog,swa,kin]\n",
    "'''\n",
    "\n",
    "config = yaml.safe_load(yaml_config)\n",
    "train_ds = salt.dataset.create(config['train'], verbose=True)\n",
    "valid_ds = salt.dataset.create(config['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeE3B_MdJxAu"
   },
   "outputs": [],
   "source": [
    "salt.utils.show_dataset(train_ds, audio_features=['source'], N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BNxLEzRpNey"
   },
   "outputs": [],
   "source": [
    "feature_extractor = transformers.WhisperFeatureExtractor.from_pretrained(\n",
    "    config['pretrained_model'])\n",
    "processor = transformers.WhisperProcessor.from_pretrained(\n",
    "    config['pretrained_model'], language=None, task=\"transcribe\")\n",
    "model = transformers.WhisperForConditionalGeneration.from_pretrained(\n",
    "    config['pretrained_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00Jd-YTThouQ"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]    \n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor, decoder_start_token_id=model.config.decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in prompts: preceding text which is used to guide the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = datasets.load_dataset(\n",
    "    'Sunbird/salt', 'text-all', split='train').to_pandas()\n",
    "prompts = datasets.load_dataset(\n",
    "    'Sunbird/prompts', split='train').to_pandas()\n",
    "joined = pd.merge(sentences, prompts, on='id', how='inner')\n",
    "SALT_PROMPT_LANGUAGES = ['eng', 'ach', 'lgg', 'lug', 'nyn', 'teo']\n",
    "sentence_to_prompt = {}\n",
    "for language in SALT_PROMPT_LANGUAGES:\n",
    "    sentence_to_prompt[language] = dict(\n",
    "        zip(joined[f'{language}_text'], joined[f'{language}_prompt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mzVFDogXgLG"
   },
   "outputs": [],
   "source": [
    "language_id_tokens = salt.constants.SALT_LANGUAGE_TOKENS_WHISPER\n",
    "\n",
    "def prepare_dataset(example, p_prompt = 0.5):    \n",
    "    audio = example[\"source\"]\n",
    "    input_features = feature_extractor(\n",
    "        audio, sampling_rate=16000, device='cuda',\n",
    "        do_normalize=True).input_features[0]\n",
    "\n",
    "    # Encode target text to label ids\n",
    "    labels = processor.tokenizer(str(example[\"target\"])).input_ids\n",
    "\n",
    "    # Insert the language ID token into the second position of the sequence.\n",
    "    labels.insert(1, language_id_tokens[example[\"target.language\"]])\n",
    "\n",
    "    # If a prompt is known for a particular sentence, add it to the\n",
    "    # training example with probability `p_prompt`.\n",
    "    if example[\"target.language\"] in sentence_to_prompt:\n",
    "        prompt = sentence_to_prompt[example[\"target.language\"]].get(example[\"target\"], None)\n",
    "        if prompt:\n",
    "            if np.random.random() < p_prompt:\n",
    "                prompt_ids = list(processor.get_prompt_ids(prompt))\n",
    "                labels = prompt_ids + labels  \n",
    "\n",
    "    # Create a new dictionary with the processed data\n",
    "    processed_example = {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": np.array(labels),\n",
    "        \"source.language\": example[\"source.language\"],\n",
    "        \"target.language\": example[\"target.language\"]\n",
    "    }\n",
    "\n",
    "    return processed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05Zyqa3cYCFW"
   },
   "outputs": [],
   "source": [
    "train_data = train_ds.map(prepare_dataset, remove_columns=[\"source\", \"target\"])\n",
    "val_data = valid_ds.map(prepare_dataset, remove_columns=[\"source\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UB4g9cW4rZ-u"
   },
   "outputs": [],
   "source": [
    "compute_metrics = salt.metrics.multilingual_eval_fn(\n",
    "      valid_ds, [evaluate.load('wer'), evaluate.load('cer')],\n",
    "      processor.tokenizer, log_first_N_predictions=5,\n",
    "      speech_processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCsAGEQtremE"
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "if config['use_peft']:\n",
    "    model = peft.prepare_model_for_kbit_training(model)\n",
    "    lora_config = peft.LoraConfig(**config['lora_config'])\n",
    "    model.enable_input_require_grads()\n",
    "    model = peft.get_peft_model(model, lora_config)\n",
    "    model.config.use_cache = False\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d25tMcDRrh-x"
   },
   "source": [
    "Launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = transformers.Seq2SeqTrainingArguments(\n",
    "  **config[\"training_args\"],\n",
    "  report_to= [\n",
    "      platform for platform, use in [(\"wandb\", use_wandb), (\"mlflow\", use_mlflow)] if use]\n",
    ")\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    processing_class=processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log the config settings for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mlflow:\n",
    "    mlflow.log_params(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the full model (not just the adapter weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoicKYTCrxew"
   },
   "outputs": [],
   "source": [
    "processor.push_to_hub(config['training_args']['hub_model_id'])\n",
    "model.push_to_hub(config['training_args']['hub_model_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the model on the first test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(valid_ds))\n",
    "input_features = processor(example[\"source\"], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n",
    "transcription = processor.decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
