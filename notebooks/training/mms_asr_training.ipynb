{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunbirdAI/leb/blob/main/notebooks/Multilingual_ASR_training_single_language_%2B_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MMS ASR training should take around 2 hours on a single RTX 6000 Ada GPU.\n",
        "- We fine-tune from the existing MMS adapters (which are available for all the SALT languages). This means that we have to reuse the tokenizers for each language. Note that in some cases the tokenizer vocabulary is incomplete, e.g. might be missing 'q' or 'z' characters for some languages.\n",
        "- This notebook mixes in some Ugandan English to the training samples, so that the resulting model has some multi-lingual capability -- at least to identify English phrases in case a speaker does code switching."
      ],
      "metadata": {
        "id": "x0o_p4ITU75i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1paBwQoLcYk"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q jiwer\n",
        "!pip install -q evaluate\n",
        "!pip install -qU accelerate\n",
        "!pip install -q transformers[torch]\n",
        "!git clone https://github.com/sunbirdai/leb.git\n",
        "!pip install -qr leb/requirements.txt\n",
        "!pip install -q mlflow\n",
        "\n",
        "## requirements to log system/GPU metrics in mlflow\n",
        "!pip install psutil\n",
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWe3IWmIY-2l"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoFeatureExtractor,\n",
        "    AutoModelForCTC,\n",
        "    AutoProcessor,\n",
        "    HfArgumentParser,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    Wav2Vec2CTCTokenizer,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2ForCTC,\n",
        "    Wav2Vec2Processor,\n",
        "    is_apex_available,\n",
        "    set_seed,\n",
        ")\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Union, List, Dict\n",
        "import string\n",
        "import os\n",
        "import json\n",
        "import datasets\n",
        "import numpy as np\n",
        "import yaml\n",
        "import evaluate\n",
        "import mlflow\n",
        "from getpass import getpass\n",
        "import leb.dataset\n",
        "import leb.metrics\n",
        "from leb.utils import DataCollatorCTCWithPadding as dcwp\n",
        "import mlflow.pytorch\n",
        "from mlflow import MlflowClient\n",
        "import huggingface_hub\n",
        "from transformers.models.wav2vec2.modeling_wav2vec2 import WAV2VEC2_ADAPTER_SAFE_FILE\n",
        "from safetensors.torch import save_file as safe_save_file\n",
        "\n",
        "# import wandb # If using weights and biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Gd9pXmUW6S6"
      },
      "outputs": [],
      "source": [
        "# Set MLflow tracking credentials\n",
        "MLFLOW_TRACKING_USERNAME = getpass('Enter the MLFLOW_TRACKING_USERNAME: ') # enter your provided username\n",
        "os.environ['MLFLOW_TRACKING_USERNAME'] = MLFLOW_TRACKING_USERNAME\n",
        "\n",
        "MLFLOW_TRACKING_PASSWORD = getpass('Enter the MLFLOW_TRACKING_PASSWORD: ') # enter your provided password\n",
        "os.environ['MLFLOW_TRACKING_PASSWORD'] = MLFLOW_TRACKING_PASSWORD\n",
        "\n",
        "# Set the MLflow tracking URI\n",
        "mlflow.set_tracking_uri('https://mlflow-sunbird-ce0ecfc14244.herokuapp.com/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTjDd9y38F6S"
      },
      "outputs": [],
      "source": [
        "# Train a model with a single local language plus English (Uganda accent) support\n",
        "language = 'lgg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IholA_KCD7Sm"
      },
      "outputs": [],
      "source": [
        "# languages currently available in SALT multispeaker STT dataset\n",
        "languages = {\n",
        "    \"acholi\": \"ach\",\n",
        "    \"lugbara\": \"lgg\",\n",
        "    \"luganda\": \"lug\",\n",
        "    \"ateso\": \"teo\",\n",
        "    \"runyankole\": \"nyn\",\n",
        "    \"english\": \"eng\"\n",
        "}\n",
        "\n",
        "yaml_config = f'''\n",
        "pretrained_model: facebook/mms-1b-all\n",
        "pretrained_adapter: {language}\n",
        "mlflow_experiment_name : stt-multilingual\n",
        "mlflow_run_name: {language}_from_pretrained\n",
        "adapter_save_id: {language}+eng\n",
        "\n",
        "training_args:\n",
        "    output_dir: stt\n",
        "    per_device_train_batch_size: 24\n",
        "    gradient_accumulation_steps: 2\n",
        "    evaluation_strategy: steps\n",
        "    max_steps: 1200\n",
        "    gradient_checkpointing: True\n",
        "    gradient_checkpointing_kwargs:\n",
        "      use_reentrant: True\n",
        "    fp16: True\n",
        "    save_steps: 100\n",
        "    eval_steps: 100\n",
        "    logging_steps: 100\n",
        "    learning_rate: 3.0e-4\n",
        "    warmup_steps: 100\n",
        "    save_total_limit: 2\n",
        "    # push_to_hub: True\n",
        "    load_best_model_at_end: True\n",
        "    metric_for_best_model: loss\n",
        "    greater_is_better: False\n",
        "    weight_decay: 0.01\n",
        "\n",
        "Wav2Vec2ForCTC_args:\n",
        "    attention_dropout: 0.0\n",
        "    hidden_dropout: 0.0\n",
        "    feat_proj_dropout: 0.0\n",
        "    layerdrop: 0.0\n",
        "    ctc_loss_reduction: mean\n",
        "    ignore_mismatched_sizes: True\n",
        "\n",
        "train:\n",
        "    huggingface_load:\n",
        "        - path: Sunbird/salt\n",
        "          name: multispeaker-{language}\n",
        "          split: train\n",
        "    source:\n",
        "      type: speech\n",
        "      language: [{language},eng]\n",
        "      preprocessing:\n",
        "        - set_sample_rate:\n",
        "            rate: 16_000\n",
        "        - augment_audio_noise:\n",
        "            max_relative_amplitude: 0.5\n",
        "    target:\n",
        "      type: text\n",
        "      language: [{language},eng]\n",
        "      preprocessing:\n",
        "        - lower_case\n",
        "        - clean_and_remove_punctuation:\n",
        "            allowed_punctuation: \"'\"\n",
        "    shuffle: True\n",
        "validation:\n",
        "    huggingface_load:\n",
        "        - path: Sunbird/salt\n",
        "          name: multispeaker-{language}\n",
        "          split: dev\n",
        "        - path: Sunbird/salt\n",
        "          name: multispeaker-eng\n",
        "          split: dev\n",
        "    source:\n",
        "      type: speech\n",
        "      language: [{language},eng]\n",
        "      preprocessing:\n",
        "        - set_sample_rate:\n",
        "            rate: 16_000\n",
        "    target:\n",
        "      type: text\n",
        "      language: [{language},eng]\n",
        "      preprocessing:\n",
        "        - lower_case\n",
        "        - clean_and_remove_punctuation:\n",
        "            allowed_punctuation: \"'\"\n",
        "'''\n",
        "\n",
        "config = yaml.safe_load(yaml_config)\n",
        "train_ds = leb.dataset.create(config['train'])\n",
        "valid_ds = leb.dataset.create(config['validation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZKyBhmdieAv"
      },
      "outputs": [],
      "source": [
        "leb.utils.show_dataset(train_ds, audio_features=['source'], N=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99fFmCQQ-mS6"
      },
      "outputs": [],
      "source": [
        "if config.get('pretrained_adapter'):\n",
        "  # If fine-tuning from an existing adapter, we have to use the matching\n",
        "  # vocabulary.\n",
        "  tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(config['pretrained_model'])\n",
        "  tokenizer.set_target_lang(config['pretrained_adapter'])\n",
        "else:\n",
        "  # Otherwise, create a new vocabulary. Assume that the preprocessing leaves\n",
        "  # only lower case characters, digits and specific special characters.\n",
        "  language = '-'.join(config['train']['source']['language'])\n",
        "  vocab = list(string.ascii_lowercase)\n",
        "  vocab += ['[UNK]', '[PAD]', '|', \"'\"]\n",
        "  vocab_dict = {\n",
        "      language: {v: i for i, v in enumerate(vocab)}\n",
        "  }\n",
        "  # vocab_dict[language]['|'] = vocab_dict[language][' ']\n",
        "  with open(\"vocab.json\", \"w\") as vocab_file:\n",
        "      json.dump(vocab_dict, vocab_file)\n",
        "  tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\n",
        "      \"./\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\",\n",
        "      target_lang=language)\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor(\n",
        "    feature_size=1, sampling_rate=16000, padding_value=0.0,\n",
        "    do_normalize=True, return_attention_mask=True)\n",
        "processor = Wav2Vec2Processor(\n",
        "    feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
        "data_collator = dcwp(processor=processor, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxkMnzyIKyGa"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # check that all files have the correct sampling rate\n",
        "    batch[\"input_values\"] = processor(\n",
        "        batch[\"source\"], sampling_rate=16000\n",
        "    ).input_values\n",
        "    # Setup the processor for targets\n",
        "    batch[\"labels\"] = processor(text=batch[\"target\"]).input_ids\n",
        "\n",
        "    return batch\n",
        "\n",
        "final_train_dataset = train_ds.map(\n",
        "    prepare_dataset,\n",
        "    batch_size=4,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "final_val_dataset = valid_ds.map(\n",
        "    prepare_dataset,\n",
        "    batch_size=4,\n",
        "    batched=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfCtBYqEblKh"
      },
      "outputs": [],
      "source": [
        "model = Wav2Vec2ForCTC.from_pretrained(\n",
        "    config['pretrained_model'],\n",
        "    **config[\"Wav2Vec2ForCTC_args\"],\n",
        "    pad_token_id=processor.tokenizer.pad_token_id,\n",
        "    vocab_size=len(processor.tokenizer)\n",
        ")\n",
        "\n",
        "if config.get('pretrained_adapter'):\n",
        "  model.load_adapter(config['pretrained_adapter'])\n",
        "else:\n",
        "  model.init_adapter_layers()\n",
        "\n",
        "model.freeze_base_model()\n",
        "adapter_weights = model._get_adapters()\n",
        "for param in adapter_weights.values():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rBwCRvAb57u"
      },
      "outputs": [],
      "source": [
        "compute_metrics = leb.metrics.multilingual_eval_fn(\n",
        "      valid_ds, [evaluate.load('wer'), evaluate.load('cer')],\n",
        "      tokenizer, log_first_N_predictions=2,\n",
        "      speech_processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDPL_qUOzt2G"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "  **config[\"training_args\"],\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=final_train_dataset,\n",
        "    eval_dataset=final_val_dataset,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[leb.utils.MlflowExtendedLoggingCallback()]\n",
        ")\n",
        "\n",
        "experiment_name = config['mlflow_experiment_name']\n",
        "\n",
        "if not mlflow.get_experiment_by_name(experiment_name):\n",
        "  mlflow.create_experiment(experiment_name)\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "with mlflow.start_run(run_name=config['mlflow_run_name'], log_system_metrics=True) as run:\n",
        "\n",
        "    mlflow.set_tag(\"developer\", os.environ['MLFLOW_TRACKING_USERNAME'])\n",
        "\n",
        "    mlflow.log_params(config)\n",
        "\n",
        "    train_output = trainer.train()\n",
        "\n",
        "    # evaluate the model to get the latest metrics including WER\n",
        "    eval_metrics = trainer.evaluate()\n",
        "\n",
        "    # Save and log the model\n",
        "    trainer.save_model()\n",
        "\n",
        "    adapter_file = WAV2VEC2_ADAPTER_SAFE_FILE.format(config['adapter_save_id'])\n",
        "    adapter_file = os.path.join(training_args.output_dir, adapter_file)\n",
        "    safe_save_file(model._get_adapters(), adapter_file, metadata={\"format\": \"pt\"})\n",
        "\n",
        "    artifact_path = \"model_artifacts\"\n",
        "    # mlflow.log_artifact(f\"{experiment_name}/config.json\", artifact_path)\n",
        "    mlflow.log_artifact(\n",
        "        f\"{config['training_args']['output_dir']}/preprocessor_config.json\",\n",
        "        artifact_path)\n",
        "    mlflow.log_artifact(\n",
        "        f\"{config['training_args']['output_dir']}/training_args.bin\",\n",
        "        artifact_path)\n",
        "    mlflow.log_artifact(\n",
        "        f\"{config['training_args']['output_dir']}/adapter.{config['adapter_save_id']}.safetensors\",\n",
        "        artifact_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30648,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}