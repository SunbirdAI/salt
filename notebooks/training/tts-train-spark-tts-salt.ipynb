{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ddae2-3d00-4791-aa25-e13e3a321997",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unsloth\n",
    "!git clone https://github.com/SparkAudio/Spark-TTS\n",
    "!pip install omegaconf einx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63a982-ba53-454f-8256-4a47c8d03a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85936f0f-ca1c-4908-a1c9-93b43be9e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e387bc8-ade2-42fb-ab85-93697b95e5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm.notebook as tqdm\n",
    "from unsloth import FastModel\n",
    "from transformers import CsmForConditionalGeneration\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_dataset, Audio, Dataset\n",
    "from IPython.display import Audio, display\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56deac8a-4afd-44de-a203-fbe58158b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model and code\n",
    "snapshot_download(\"unsloth/Spark-TTS-0.5B\", local_dir = \"Spark-TTS-0.5B\")\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = f\"Spark-TTS-0.5B/LLM\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = torch.float32, # Spark seems to only work on float32 for now\n",
    "    full_finetuning = True, # We support full finetuning now!\n",
    "    load_in_4bit = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711a06a-79c9-40b2-aa88-3b1ddae30d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from getpass import getpass\n",
    "import os\n",
    "MLFLOW_TRACKING_USERNAME = getpass('Enter the MLFLOW_TRACKING_USERNAME: ')\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = MLFLOW_TRACKING_USERNAME\n",
    "MLFLOW_TRACKING_PASSWORD = getpass('Enter the MLFLOW_TRACKING_PASSWORD: ')\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = MLFLOW_TRACKING_PASSWORD\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://mlflow-sunbird-ce0ecfc14244.herokuapp.com\" \n",
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = \"tts-csm-1b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b31c9ed-cbc9-42d8-8e60-0dae67de1faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lug = load_dataset(\n",
    "    \"Sunbird/salt\", \"studio-lug\", split=\"train\").map(lambda example: {\"speaker_id\": 1})\n",
    "\n",
    "ds_eng = load_dataset(\n",
    "    \"Sunbird/salt\", \"studio-eng\", split=\"train\").map(lambda example: {\"speaker_id\": 1})\n",
    "\n",
    "ds_ach = load_dataset(\n",
    "    \"Sunbird/salt\", \"studio-ach\", split=\"train\").map(lambda example: {\"speaker_id\": 2})\n",
    "\n",
    "ds_swa = load_dataset(\n",
    "    \"Sunbird/salt\", \"studio-swa\", split=\"train\").map(lambda example: {\"speaker_id\": 3})\n",
    "\n",
    "ds_lgg = load_dataset(\n",
    "    \"Sunbird/salt\", \"studio-lgg\", split=\"train\").map(lambda example: {\"speaker_id\": 4})\n",
    "\n",
    "ds_nyn = load_dataset(\n",
    "    \"Sunbird/salt\", \"studio-nyn\", split=\"train\").map(lambda example: {\"speaker_id\": 5})\n",
    "\n",
    "ds_teo = load_dataset(\n",
    "    \"Sunbird/salt\", \"studio-teo\", split=\"train\").map(lambda example: {\"speaker_id\": 6})\n",
    "\n",
    "dataset = datasets.concatenate_datasets(\n",
    "    [ds_ach,ds_lug, ds_eng, ds_swa, ds_lgg, ds_nyn, ds_teo]).shuffle(seed=42)\n",
    "\n",
    "sampling_rate = 24000\n",
    "dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=sampling_rate))\n",
    "\n",
    "dataset = dataset.filter(\n",
    "    lambda example: (0.5 * sampling_rate) < len(example[\"audio\"][\"array\"]) < (8 * sampling_rate),\n",
    "    num_proc=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bdbb2-5b40-45fe-a387-65d589c52ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import torchaudio.transforms as T\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "from sparktts.models.audio_tokenizer import BiCodecTokenizer\n",
    "from sparktts.utils.audio import audio_volume_normalize\n",
    "\n",
    "audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B\", \"cuda\")\n",
    "def extract_wav2vec2_features( wavs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"extract wav2vec2 features\"\"\"\n",
    "\n",
    "        if wavs.shape[0] != 1:\n",
    "\n",
    "             raise ValueError(f\"Expected batch size 1, but got shape {wavs.shape}\")\n",
    "        wav_np = wavs.squeeze(0).cpu().numpy()\n",
    "\n",
    "        processed = audio_tokenizer.processor(\n",
    "            wav_np,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "        )\n",
    "        input_values = processed.input_values\n",
    "\n",
    "        input_values = input_values.to(audio_tokenizer.feature_extractor.device)\n",
    "\n",
    "        model_output = audio_tokenizer.feature_extractor(\n",
    "            input_values,\n",
    "        )\n",
    "\n",
    "        if model_output.hidden_states is None:\n",
    "             raise ValueError(\"Wav2Vec2Model did not return hidden states. Ensure config `output_hidden_states=True`.\")\n",
    "\n",
    "        num_layers = len(model_output.hidden_states)\n",
    "        required_layers = [11, 14, 16]\n",
    "        if any(l >= num_layers for l in required_layers):\n",
    "             raise IndexError(f\"Requested hidden state indices {required_layers} out of range for model with {num_layers} layers.\")\n",
    "\n",
    "        feats_mix = (\n",
    "            model_output.hidden_states[11] + model_output.hidden_states[14] + model_output.hidden_states[16]\n",
    "        ) / 3\n",
    "\n",
    "        return feats_mix\n",
    "    \n",
    "def formatting_audio_func(example):\n",
    "    text = f\"{example['speaker_id']}: {example['text']}\" if \"speaker_id\" in example else example[\"text\"]\n",
    "    audio_array = example[\"audio\"][\"array\"]\n",
    "    sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "    target_sr = audio_tokenizer.config['sample_rate']\n",
    "\n",
    "    if sampling_rate != target_sr:\n",
    "        resampler = T.Resample(orig_freq=sampling_rate, new_freq=target_sr)\n",
    "        audio_tensor_temp = torch.from_numpy(audio_array).float()\n",
    "        audio_array = resampler(audio_tensor_temp).numpy()\n",
    "\n",
    "    if audio_tokenizer.config[\"volume_normalize\"]:\n",
    "        audio_array = audio_volume_normalize(audio_array)\n",
    "\n",
    "    ref_wav_np = audio_tokenizer.get_ref_clip(audio_array)\n",
    "\n",
    "    audio_tensor = torch.from_numpy(audio_array).unsqueeze(0).float().to(audio_tokenizer.device)\n",
    "    ref_wav_tensor = torch.from_numpy(ref_wav_np).unsqueeze(0).float().to(audio_tokenizer.device)\n",
    "\n",
    "    feat = extract_wav2vec2_features(audio_tensor)\n",
    "\n",
    "    batch = {\n",
    "        \"wav\": audio_tensor,\n",
    "        \"ref_wav\": ref_wav_tensor,\n",
    "        \"feat\": feat.to(audio_tokenizer.device),\n",
    "    }\n",
    "\n",
    "    semantic_token_ids, global_token_ids = audio_tokenizer.model.tokenize(batch)\n",
    "\n",
    "    global_tokens = \"\".join(\n",
    "        [f\"<|bicodec_global_{i}|>\" for i in global_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
    "    )\n",
    "    semantic_tokens = \"\".join(\n",
    "        [f\"<|bicodec_semantic_{i}|>\" for i in semantic_token_ids.squeeze().cpu().numpy()] # Squeeze batch dim\n",
    "    )\n",
    "\n",
    "    inputs = [\n",
    "        \"<|task_tts|>\",\n",
    "        \"<|start_content|>\",\n",
    "        text,\n",
    "        \"<|end_content|>\",\n",
    "        \"<|start_global_token|>\",\n",
    "        global_tokens,\n",
    "        \"<|end_global_token|>\",\n",
    "        \"<|start_semantic_token|>\",\n",
    "        semantic_tokens,\n",
    "        \"<|end_semantic_token|>\",\n",
    "        \"<|im_end|>\"\n",
    "    ]\n",
    "    inputs = \"\".join(inputs)\n",
    "    return {\"text\": inputs}\n",
    "\n",
    "\n",
    "dataset = dataset.take(12_000).map(formatting_audio_func, remove_columns=[\"audio\"])\n",
    "print(\"Moving Bicodec model and Wav2Vec2Model to cpu.\")\n",
    "audio_tokenizer.model.cpu()\n",
    "audio_tokenizer.feature_extractor.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727f25c-1374-4d9d-9cc8-5becb1d01b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = False, # We're doing full float32 s disable mixed precision\n",
    "        bf16 = False, # We're doing full float32 s disable mixed precision\n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0249efc-4781-4246-ae6d-9cb6b368f545",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390a4c27-0229-4987-b750-35536ae27e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_audio_tokenizer = BiCodecTokenizer(\"Spark-TTS-0.5B\", \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8313c2-a35c-4800-b061-213cb7a80272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "FastModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_speech_from_text(\n",
    "    text: str,\n",
    "    temperature: float = 0.8,   # Generation temperature\n",
    "    top_k: int = 50,            # Generation top_k\n",
    "    top_p: float = 1,        # Generation top_p\n",
    "    max_new_audio_tokens: int = 2048, # Max tokens for audio part\n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates speech audio from text using default voice control parameters.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text input to be converted to speech.\n",
    "        temperature (float): Sampling temperature for generation.\n",
    "        top_k (int): Top-k sampling parameter.\n",
    "        top_p (float): Top-p (nucleus) sampling parameter.\n",
    "        max_new_audio_tokens (int): Max number of new tokens to generate (limits audio length).\n",
    "        device (torch.device): Device to run inference on.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Generated waveform as a NumPy array.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = \"\".join([\n",
    "        \"<|task_tts|>\",\n",
    "        \"<|start_content|>\",\n",
    "        text,\n",
    "        \"<|end_content|>\",\n",
    "        \"<|start_global_token|>\"\n",
    "    ])\n",
    "    \n",
    "    # prompt = \"\".join([\n",
    "    #     \"<|task_controllable_tts|>\",\n",
    "    #     \"<|start_content|>\",\n",
    "    #     text,\n",
    "    #     \"<|end_content|>\",\n",
    "    #     \"<|start_style_label|>\",\n",
    "    #     #\"<|pitch_label_0|>\"\n",
    "    #     #\"<|gender_0|>\",\n",
    "    #     \"<|speed_label_0|>\",\n",
    "    #     \"<|end_style_label|>\",\n",
    "    # ])\n",
    "\n",
    "    model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "\n",
    "    print(\"Generating token sequence...\")\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=max_new_audio_tokens, # Limit generation length\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        eos_token_id=tokenizer.eos_token_id, # Stop token\n",
    "        pad_token_id=tokenizer.pad_token_id # Use models pad token id\n",
    "    )\n",
    "    print(\"Token sequence generated.\")\n",
    "\n",
    "    generated_ids_trimmed = generated_ids[:, model_inputs.input_ids.shape[1]:]\n",
    "\n",
    "    predicts_text = tokenizer.batch_decode(generated_ids_trimmed, skip_special_tokens=False)[0]\n",
    "    # print(f\"\\nGenerated Text (for parsing):\\n{predicts_text}\\n\") # Debugging\n",
    "\n",
    "    # Extract semantic token IDs using regex\n",
    "    semantic_matches = re.findall(r\"<\\|bicodec_semantic_(\\d+)\\|>\", predicts_text)\n",
    "    if not semantic_matches:\n",
    "        print(\"Warning: No semantic tokens found in the generated output.\")\n",
    "        # Handle appropriately - perhaps return silence or raise error\n",
    "        return np.array([], dtype=np.float32)\n",
    "\n",
    "    pred_semantic_ids = torch.tensor([int(token) for token in semantic_matches]).long().unsqueeze(0) # Add batch dim\n",
    "\n",
    "    # Extract global token IDs using regex (assuming controllable mode also generates these)\n",
    "    global_matches = re.findall(r\"<\\|bicodec_global_(\\d+)\\|>\", predicts_text)\n",
    "    if not global_matches:\n",
    "         print(\"Warning: No global tokens found in the generated output (controllable mode). Might use defaults or fail.\")\n",
    "         pred_global_ids = torch.zeros((1, 1), dtype=torch.long)\n",
    "    else:\n",
    "         pred_global_ids = torch.tensor([int(token) for token in global_matches]).long().unsqueeze(0) # Add batch dim\n",
    "\n",
    "    pred_global_ids = pred_global_ids.unsqueeze(0) # Shape becomes (1, 1, N_global)\n",
    "\n",
    "    print(f\"Found {pred_semantic_ids.shape[1]} semantic tokens.\")\n",
    "    print(f\"Found {pred_global_ids.shape[2]} global tokens.\")\n",
    "\n",
    "\n",
    "    # 5. Detokenize using BiCodecTokenizer\n",
    "    print(\"Detokenizing audio tokens...\")\n",
    "    # Ensure audio_tokenizer and its internal model are on the correct device\n",
    "    audio_tokenizer.device = device\n",
    "    audio_tokenizer.model.to(device)\n",
    "\n",
    "    return pred_global_ids, pred_semantic_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d553a44-6454-468b-af69-851b79538f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'There are 123 of them on 11th June 2025.'\n",
    "#input_text = \"Nsobola okwogera Oluganda n'ennimi endala.\"\n",
    "#input_text = \"Once there was a boy who lived on the moon. All his friends were down on Earth.\"\n",
    "#input_text = \"2: Ekitiibwa ky'omuntu eky'obutonde; okwenkanankana, wamu n'obuyinza obutayinza kugyibwawo ebyabantu bonna, gwe musingi gw'eddembe; obwenkanya n'emirembe mu nsi.\"\n",
    "#input_text = \"Ndi musomesa mu Makerere University in the department of computer science.\"\n",
    "\n",
    "pred_global_ids, pred_semantic_ids = generate_speech_from_text(input_text)\n",
    "device = 'cuda'\n",
    "wav_np = gpu_audio_tokenizer.detokenize(\n",
    "    pred_global_ids.to(device).squeeze(0),\n",
    "    pred_semantic_ids.to(device) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b78832-7902-402f-b75f-ad2584eb6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = audio_tokenizer.config.get(\"sample_rate\", 16000)\n",
    "from IPython.display import Audio, display\n",
    "display(Audio(wav_np, rate=sample_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb78582-ab11-40f2-b1ae-a07f245634a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub('jq/spark-tts-salt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1f2e4-fa61-4f51-bb5c-dd12911ea251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
