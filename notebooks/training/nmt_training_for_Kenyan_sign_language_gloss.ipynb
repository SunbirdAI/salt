{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunbirdAI/salt/blob/main/notebooks/NMT_training_for_Kenyan_Sign_Language_gloss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine translation example: Kenyan Sign Language Gloss\n",
        "\n",
        "This example shows how to train a machine translation model for KSL, English and Swahili, using Maseno university datasets and HuggingFace extended with Sunbird AI [SALT](https://github.com/SunbirdAI/salt/).\n",
        "\n",
        "In this example, we combine English-KSL and English-Swahili training data, to obtain a model which can translate in any direction, including Swahili to KSL.\n",
        "\n",
        "The model used here is [NLLB 600M](https://huggingface.co/facebook/nllb-200-distilled-600M), which can be trained on a free Colab instance. If more GPU memory is available, then a larger model (e.g. NLLB 1.3B) is likely to give better performance."
      ],
      "metadata": {
        "id": "xKSQLySFvb0c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fM22URX1IrV_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Install packages\n",
        "!pip install -qU transformers\n",
        "!pip install -qU datasets\n",
        "!pip install -q accelerate\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q sacremoses\n",
        "!pip install -q wandb\n",
        "\n",
        "# Sunbird African Language Technology (SALT) utilities\n",
        "!git clone https://github.com/sunbirdai/salt.git\n",
        "!pip install -qr salt/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNCKY1wral2q",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "import huggingface_hub\n",
        "import datasets\n",
        "import evaluate\n",
        "import tqdm\n",
        "import salt.dataset\n",
        "import salt.utils\n",
        "import salt.metrics\n",
        "import yaml\n",
        "import wandb\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qjZVzVsAqRU"
      },
      "outputs": [],
      "source": [
        "huggingface_hub.notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-btsrsNhZwt"
      },
      "outputs": [],
      "source": [
        "# Where should output files be stored locally\n",
        "drive_folder = \"./artifacts\"\n",
        "\n",
        "if not os.path.exists(drive_folder):\n",
        "  %mkdir $drive_folder\n",
        "\n",
        "# Large batch sizes generally give good results for translation\n",
        "effective_train_batch_size = 480\n",
        "train_batch_size = 6\n",
        "eval_batch_size = train_batch_size\n",
        "\n",
        "gradient_accumulation_steps = int(effective_train_batch_size / train_batch_size)\n",
        "\n",
        "# Everything in one yaml string, so that it can all be logged.\n",
        "yaml_config = '''\n",
        "training_args:\n",
        "  output_dir: \"{drive_folder}\"\n",
        "  eval_strategy: steps\n",
        "  eval_steps: 100\n",
        "  save_steps: 100\n",
        "  gradient_accumulation_steps: {gradient_accumulation_steps}\n",
        "  learning_rate: 3.0e-4  # Include decimal point to parse as float\n",
        "  # optim: adafactor\n",
        "  per_device_train_batch_size: {train_batch_size}\n",
        "  per_device_eval_batch_size: {eval_batch_size}\n",
        "  weight_decay: 0.01\n",
        "  save_total_limit: 3\n",
        "  max_steps: 500\n",
        "  predict_with_generate: True\n",
        "  fp16: True\n",
        "  logging_dir: \"{drive_folder}\"\n",
        "  load_best_model_at_end: True\n",
        "  metric_for_best_model: loss\n",
        "  seed: 123\n",
        "  push_to_hub: False\n",
        "\n",
        "max_input_length: 128\n",
        "eval_pretrained_model: False\n",
        "early_stopping_patience: 4\n",
        "data_dir: .\n",
        "\n",
        "# Use a 600M parameter model here, which is easier to train on a free Colab\n",
        "# instance. Bigger models work better, however: results will be improved\n",
        "# if able to train on nllb-200-1.3B instead.\n",
        "model_checkpoint: facebook/nllb-200-distilled-600M\n",
        "\n",
        "datasets:\n",
        "  train:\n",
        "    huggingface_load:\n",
        "      # We will load two datasets here: English/KSL Gloss, and also SALT\n",
        "      # Swahili/English, so that we can try out multi-way translation.\n",
        "\n",
        "      - path: EzekielMW/Eng_KSLGloss\n",
        "        split: train[:-500]\n",
        "      - path: sunbird/salt\n",
        "        name: text-all\n",
        "        split: train\n",
        "    source:\n",
        "      # This is a text translation only, no audio.\n",
        "      type: text\n",
        "      # The source text can be any of English, KSL or Swahili.\n",
        "      language: [eng,ksl,swa]\n",
        "      preprocessing:\n",
        "        # The models are case sensitive, so if the training text is all\n",
        "        # capitals, then it will only learn to translate capital letters and\n",
        "        # won't understand lower case. Make everything lower case for now.\n",
        "        - lower_case\n",
        "        # We can also augment the spelling of the input text, which makes the\n",
        "        # model more robust to spelling errors.\n",
        "        - augment_characters\n",
        "    target:\n",
        "      type: text\n",
        "      # The target text with any of English, KSL or Swahili.\n",
        "      language: [eng,ksl,swa]\n",
        "      # The models are case sensitive: make everything lower case for now.\n",
        "      preprocessing:\n",
        "        - lower_case\n",
        "\n",
        "    shuffle: True\n",
        "    allow_same_src_and_tgt_language: False\n",
        "\n",
        "  validation:\n",
        "    huggingface_load:\n",
        "      # Use the last 500 of the KSL examples for validation.\n",
        "      - path: EzekielMW/Eng_KSLGloss\n",
        "        split: train[-500:]\n",
        "      # Add some Swahili validation text.\n",
        "      - path: sunbird/salt\n",
        "        name: text-all\n",
        "        split: dev\n",
        "    source:\n",
        "      type: text\n",
        "      language: [swa,ksl,eng]\n",
        "      preprocessing:\n",
        "        - lower_case\n",
        "    target:\n",
        "      type: text\n",
        "      language: [swa,ksl,eng]\n",
        "      preprocessing:\n",
        "        - lower_case\n",
        "    allow_same_src_and_tgt_language: False\n",
        "'''\n",
        "\n",
        "yaml_config = yaml_config.format(\n",
        "    drive_folder=drive_folder,\n",
        "    train_batch_size=train_batch_size,\n",
        "    eval_batch_size=eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        ")\n",
        "\n",
        "config = yaml.safe_load(yaml_config)\n",
        "\n",
        "training_settings = transformers.Seq2SeqTrainingArguments(\n",
        "    **config[\"training_args\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Aer4abSStvM"
      },
      "outputs": [],
      "source": [
        "# The default HuggingFace NLLB models can only be trained with one target\n",
        "# language. Use a SALT wrapper which makes it trainable for multilingual\n",
        "# translation.\n",
        "model = salt.utils.TrainableM2MForConditionalGeneration.from_pretrained(\n",
        "    config['model_checkpoint'])\n",
        "tokenizer = transformers.NllbTokenizer.from_pretrained(\n",
        "    config['model_checkpoint'],\n",
        "    src_lang='eng_Latn',\n",
        "    tgt_lang='eng_Latn')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6UeS40BnaPU"
      },
      "outputs": [],
      "source": [
        "# The pre-trained model that we use has support for some African languages, but\n",
        "# we need to adapt the tokenizer to languages that it wasn't trained with,\n",
        "# such as KSL. Here we reuse the token from a different language.\n",
        "LANGUAGE_CODES = [\"eng\", \"swa\", \"ksl\"]\n",
        "\n",
        "code_mapping = {\n",
        "    # Exact/close mapping\n",
        "    'eng': 'eng_Latn',\n",
        "    'swa': 'swh_Latn',\n",
        "    # Random mapping\n",
        "    'ksl': 'ace_Latn',\n",
        "}\n",
        "\n",
        "offset = tokenizer.sp_model_size + tokenizer.fairseq_offset\n",
        "\n",
        "for code in LANGUAGE_CODES:\n",
        "    i = tokenizer.convert_tokens_to_ids(code_mapping[code])\n",
        "    tokenizer._added_tokens_encoder[code] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA1NhqNDneXq"
      },
      "outputs": [],
      "source": [
        "label_pad_token_id = -100\n",
        "data_collator = transformers.DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model = model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysiiSjZMQV1J"
      },
      "outputs": [],
      "source": [
        "def preprocess(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        examples['source'],\n",
        "        text_target=examples['target'],\n",
        "        max_length=config['max_input_length'],\n",
        "        truncation=True)\n",
        "\n",
        "    # For NLLB models, set the language code for the sources and targets\n",
        "    model_inputs['forced_bos_token_id'] = []\n",
        "    for i in range(len(examples['source'])):\n",
        "      source_language = examples['source.language'][i]\n",
        "      target_language = examples['target.language'][i]\n",
        "      model_inputs['input_ids'][i][0] = tokenizer.convert_tokens_to_ids(\n",
        "          source_language)\n",
        "      target_language_token = tokenizer.convert_tokens_to_ids(target_language)\n",
        "      model_inputs['labels'][i][0] = target_language_token\n",
        "      model_inputs['forced_bos_token_id'].append(target_language_token)\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "\n",
        "train_dataset = salt.dataset.create(config['datasets']['train'])\n",
        "eval_dataset = salt.dataset.create(config['datasets']['validation'])\n",
        "\n",
        "# Take a look at some of the data rows after shuffling and preprocessing\n",
        "salt.utils.show_dataset(train_dataset, N=10)\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    preprocess,\n",
        "    batched=True,\n",
        "    remove_columns=['source', 'source.language', 'target', 'target.language'])\n",
        "eval_dataset = eval_dataset.map(\n",
        "    preprocess,\n",
        "    batched=True)\n",
        "\n",
        "# Use a SALT function which computed the evaluation score separately for\n",
        "# different languages.\n",
        "compute_metrics = salt.metrics.multilingual_eval_fn(\n",
        "      eval_dataset, [evaluate.load('sacrebleu')],\n",
        "      tokenizer, log_first_N_predictions=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYa8uuTzxpRc"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.init(project='translate-ksl-eng-swa', config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjocS5ApPr-i"
      },
      "outputs": [],
      "source": [
        "transformers.generation.utils.ForcedBOSTokenLogitsProcessor = salt.utils.ForcedVariableBOSTokenLogitsProcessor\n",
        "\n",
        "trainer = transformers.Seq2SeqTrainer(\n",
        "  model,\n",
        "  training_settings,\n",
        "  train_dataset = train_dataset,\n",
        "  eval_dataset = eval_dataset,\n",
        "  data_collator = data_collator,\n",
        "  tokenizer = tokenizer,\n",
        "  compute_metrics = compute_metrics,\n",
        "  callbacks = [\n",
        "      transformers.EarlyStoppingCallback(\n",
        "          early_stopping_patience = (config\n",
        "           ['early_stopping_patience']))],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.push_to_hub('your_repo_id')\n",
        "# tokenizer.push_to_hub('your_repo_id')"
      ],
      "metadata": {
        "id": "J56dgIpqr7pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.generation.utils.ForcedBOSTokenLogitsProcessor = transformers.ForcedBOSTokenLogitsProcessor\n",
        "\n",
        "def translate(text, source_language, target_language):\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  # We trained the model above with only lower-case text, so make sure the\n",
        "  # inputs here are lower-case too.\n",
        "  inputs = tokenizer(text.lower(), return_tensors=\"pt\").to(device)\n",
        "  inputs['input_ids'][0][0] = tokenizer.convert_tokens_to_ids(source_language)\n",
        "  translated_tokens = model.to(device).generate(\n",
        "      **inputs,\n",
        "      forced_bos_token_id=tokenizer.convert_tokens_to_ids(target_language),\n",
        "      max_length=100,\n",
        "      num_beams=5,\n",
        "  )\n",
        "\n",
        "  result = tokenizer.batch_decode(\n",
        "      translated_tokens, skip_special_tokens=True)[0]\n",
        "\n",
        "  # Change KSL glosses to upper case\n",
        "  if target_language == 'ksl':\n",
        "    result = result.upper()\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "2lQiwO0Krl8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('where is the nearest hospital', 'eng', 'ksl')"
      ],
      "metadata": {
        "id": "sXpRWc71gRmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IE9jfoJeNxEx"
      },
      "outputs": [],
      "source": [
        "translate('ME SCHOOL GO', 'ksl', 'eng')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate('nataka kununua tikiti', 'swa', 'ksl')"
      ],
      "metadata": {
        "id": "I9MkFwvisyWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('ME SCHOOL GO', 'ksl', 'swa')"
      ],
      "metadata": {
        "id": "kdRuw95wtbJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}