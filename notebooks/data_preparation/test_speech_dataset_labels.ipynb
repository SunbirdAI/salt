{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunbirdAI/salt/blob/main/notebooks/Test_ASR_data_labels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL8s5NxAlHeU",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/sunbirdai/salt.git\n",
        "!pip install -qr salt/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import io\n",
        "import soundfile as sf\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "import editdistance\n",
        "from IPython import display\n",
        "from huggingface_hub  import login\n",
        "from tqdm.notebook import tqdm\n",
        "import math\n",
        "import salt.utils"
      ],
      "metadata": {
        "id": "vPEBXxINln3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login()"
      ],
      "metadata": {
        "id": "P2fJ0FGh3JXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The repository to check\n",
        "repository_path = 'Sunbird/salt-corrected'\n",
        "splits = ['train', 'dev', 'test']\n",
        "languages = ['lug', 'eng', 'nyn', 'lgg', 'teo', 'ach']\n",
        "config_names = [f'corrected-{language}' for language in languages]\n",
        "\n",
        "# Where to store the ASR transcriptions of all audio\n",
        "transcriptions_path = 'evie-8/salt-corrected-asr-data-transcriptions'\n",
        "# Set to true to upload the transcriptions to HuggingFace\n",
        "upload_transcriptions = False\n",
        "\n",
        "# The model used to transcribe the audio\n",
        "asr_model = 'Sunbird/sunbird-mms'"
      ],
      "metadata": {
        "id": "CDPaq5KDYm5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply an ASR model to the audio recordings\n",
        "\n",
        "We try to detect any speech in the corresponding language, and then check for discrepancies with the label. This only needs to be run once."
      ],
      "metadata": {
        "id": "8A6FRahjbWDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "MODEL_SAMPLE_RATE = 16_000\n",
        "\n",
        "def bytes_to_audio(byte_data):\n",
        "\n",
        "    audio_file = io.BytesIO(byte_data)\n",
        "\n",
        "    audio, sample_rate = sf.read(audio_file)\n",
        "    return audio, sample_rate\n",
        "\n",
        "\n",
        "def transcribe_batch(audio_batch, sample_rate_batch):\n",
        "    audio_resampled_batch = [\n",
        "        librosa.resample(\n",
        "            audio, orig_sr=orig_sr, target_sr=MODEL_SAMPLE_RATE)\n",
        "            if orig_sr != MODEL_SAMPLE_RATE else audio\n",
        "            for audio, orig_sr in zip(audio_batch, sample_rate_batch)\n",
        "    ]\n",
        "    inputs_batch = [\n",
        "        processor(\n",
        "            audio, sampling_rate=16_000, return_tensors=\"pt\").to(device)\n",
        "        for audio in audio_resampled_batch\n",
        "    ]\n",
        "    with torch.no_grad():\n",
        "        outputs_batch = [\n",
        "            model(**input).logits for input in inputs_batch\n",
        "        ]\n",
        "    return [\n",
        "        processor.decode(torch.argmax(output, dim=-1)[0])\n",
        "        for output in outputs_batch\n",
        "    ]\n",
        "\n",
        "# Loop over languages and splits\n",
        "processor = transformers.AutoProcessor.from_pretrained(asr_model)\n",
        "model = transformers.Wav2Vec2ForCTC.from_pretrained(asr_model)\n",
        "device = 'cuda:0'\n",
        "model.to(device)\n",
        "\n",
        "for language, config_name in zip(languages, config_names):\n",
        "\n",
        "  if language == 'eng':\n",
        "    model.load_adapter('eng')\n",
        "  else:\n",
        "    model.load_adapter(f'{language}+eng')\n",
        "\n",
        "  processor.tokenizer.set_target_lang(language)\n",
        "\n",
        "  for split in splits:\n",
        "\n",
        "    print(f'dataset: {config_name}, split: {split}')\n",
        "\n",
        "    ds = datasets.load_dataset(\n",
        "        repository_path, config_name, split=split)\n",
        "    df = ds.to_pandas()\n",
        "\n",
        "    # Batching\n",
        "    n = len(df)\n",
        "    n_batches = math.ceil(n / BATCH_SIZE)\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(total=n_batches, desc=\"Transcribing\")\n",
        "\n",
        "    # Placeholder for results\n",
        "    transcriptions = []\n",
        "\n",
        "    for i in range(n_batches):\n",
        "        start = i * BATCH_SIZE\n",
        "        end = start + BATCH_SIZE\n",
        "        sample = df.iloc[start:end]\n",
        "\n",
        "        audio_batch = []\n",
        "        sample_rate_batch = []\n",
        "\n",
        "        for audio_bytes in sample['audio']:\n",
        "            audio, sample_rate = bytes_to_audio(audio_bytes['bytes'])\n",
        "            audio_batch.append(audio)\n",
        "            sample_rate_batch.append(sample_rate)\n",
        "\n",
        "        transcriptions_batch = transcribe_batch(audio_batch, sample_rate_batch)\n",
        "        transcriptions.extend(transcriptions_batch)\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.update()\n",
        "\n",
        "    # Assign transcriptions back to dataframe\n",
        "    df[\"transcription\"] = transcriptions\n",
        "\n",
        "    # Close progress bar\n",
        "    pbar.close()\n",
        "\n",
        "    edit_distances = [editdistance.eval(t1.lower(), t2)\n",
        "                      for t1, t2 in zip(df['text'], transcriptions)]\n",
        "\n",
        "    df['edit_distance'] = edit_distances\n",
        "    del(df['audio'])\n",
        "\n",
        "    suspicious = np.where(np.array(edit_distances) > 30)[0]\n",
        "    print(f'Found {len(suspicious)} suspicious entries out of '\n",
        "          f'{len(df)} in {config_name}/{split}')\n",
        "\n",
        "    ds = datasets.Dataset.from_pandas(df)\n",
        "    if upload_transcriptions:\n",
        "      ds.push_to_hub(\n",
        "          transcriptions_path,\n",
        "          config_name=config_name, private=False, split=split)"
      ],
      "metadata": {
        "id": "qgr9X4U41rk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Investigate misaligments\n",
        "\n",
        "Examine the 'suspicious' cases where there seems to be a high edit distance between the ASR-derived transcription and the label."
      ],
      "metadata": {
        "id": "czaOu3MDbj1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold at which we decide a label is 'suspicious', as it deviates too\n",
        "# much from the ASR transcription. Make this lower to be more strict.\n",
        "edit_distance_threshold = 30"
      ],
      "metadata": {
        "id": "tymUC_QCdrhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for language, config_name in zip(languages, config_names):\n",
        "  print(f\"Language: {language}\")\n",
        "  for split in splits:\n",
        "\n",
        "    ds_transcriptions = datasets.load_dataset(\n",
        "        transcriptions_path,\n",
        "        name=config_name,\n",
        "        split='validation' if split == 'dev' else split\n",
        "    )\n",
        "    # TODO: change the split name in the transcription repo\n",
        "    df_transcriptions = ds_transcriptions.to_pandas()\n",
        "\n",
        "    ds_source = datasets.load_dataset(\n",
        "        repository_path, name=config_name, split=split)\n",
        "    df_source = ds_source.to_pandas()\n",
        "\n",
        "    suspicious_ids = []\n",
        "    for id, row in df_transcriptions.iterrows():\n",
        "      if row['edit_distance'] > edit_distance_threshold:\n",
        "        suspicious_ids.append(row['id'])\n",
        "\n",
        "    print(\n",
        "        f\"   {language}/{split}: {len(suspicious_ids)} / {len(ds_transcriptions)} \"\n",
        "        \"audio recordings don't match the text.\")\n",
        "    print(f\"   suspicious: {suspicious_ids}\\n\\n\" )\n",
        "\n",
        "    #sampling mismatched audios\n",
        "    if len(suspicious_ids):\n",
        "\n",
        "      '''\n",
        "      suspicious_file = df_org.loc[df_org['id'] == ids[0]]\n",
        "      file_transcribed = df.loc[df['id'] == ids[0]]\n",
        "      audio_data = suspicious_file['audio'].values[0]\n",
        "\n",
        "      audio, sample_rate = bytes_to_audio(audio_data['bytes'])\n",
        "      display.display(display.Audio(audio, rate=sample_rate))\n",
        "      print(f\"Text : {file_transcribed['text'].values[0]}\")\n",
        "      print(f\"Transcription: {file_transcribed['transcription'].values[0]}\")\n",
        "      '''\n",
        "      df_suspicious = df_transcriptions[df_transcriptions['id'].isin(suspicious_ids)]\n",
        "\n",
        "      # TODO: remove these debugging break statements, which stop the execution\n",
        "      # after processing the first split.\n",
        "      break\n",
        "    break\n",
        "  break\n"
      ],
      "metadata": {
        "id": "t1Q0tfdCbQHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_transcriptions['audio'] = df_source['audio']\n",
        "df_suspicious = df_transcriptions[\n",
        "    df_transcriptions['edit_distance'] > edit_distance_threshold]\n",
        "df_suspicious = df_suspicious[\n",
        "    ['id', 'text', 'transcription', 'audio']]\n",
        "df_suspicious['audio'] = [bytes_to_audio(audio['bytes'])[0].astype(np.float32)\n",
        "                          for audio in list(df_suspicious['audio'])]\n",
        "df_suspicious['id'] = np.array(\n",
        "    [int(id) for id in list(df_suspicious['id'])],\n",
        "    dtype=np.int32)"
      ],
      "metadata": {
        "id": "e612qVF3h_Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salt.utils.show_dataset(df_suspicious, audio_features=['audio'], N=5)"
      ],
      "metadata": {
        "id": "mybVGIfphII1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps: find a way of nicely presenting the suspicious entries, so that we can step through them one by one and decide on the action (remove from dataset for invalid entries, or change the ID for misaligned entries).\n",
        "\n",
        "Bonus: a nice table showing the number of suspicious entries for each language/split."
      ],
      "metadata": {
        "id": "pAgXqCtdrPKR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T6vyiRYHre6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}