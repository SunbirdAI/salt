{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4zDnR45o1gf"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/jqug/salt.git\n",
    "!pip install -q sacremoses\n",
    "!pip install -q pandas\n",
    "!pip install -q datasets\n",
    "\n",
    "import gzip\n",
    "import io\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import sacremoses\n",
    "import salt.constants\n",
    "import datasets\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQF4tEMh5_5u"
   },
   "source": [
    "# Prepare MT560 data\n",
    "\n",
    "This is a big dataset, around 35GB compressed. Only a small part of it is relevant to the languages we are interested in, though. So first we find which lines have language codes  `lug` (Luganda), `ach` (Acholi), or `nyn` (Runyankore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import concurrent.futures\n",
    "from tqdm.notebook import tqdm\n",
    "import salt.constants\n",
    "import time\n",
    "from IPython import display\n",
    "import os\n",
    "import gzip \n",
    "import sacremoses\n",
    "import collections\n",
    "\n",
    "!wget -nc https://object.pouta.csc.fi/OPUS-MT560/train.v1.lang.gz\n",
    "\n",
    "print(\"Reading languages list...\")\n",
    "languages = pd.read_csv('train.v1.lang.gz', engine='c', names=['code'])\n",
    "language_codes = set(salt.constants.SALT_LANGUAGE_NAMES.keys())\n",
    "print(f\"Dataset loaded with {len(languages)} lines and {len(language_codes)} language codes\")\n",
    "\n",
    "print(\"Counting examples by language...\")\n",
    "lang_counts = languages['code'].value_counts().to_dict()\n",
    "\n",
    "# Sort by count for better readability\n",
    "print(\"Language distribution:\")\n",
    "for code, count in sorted(language_counts.items(), key=lambda x: -x[1]):\n",
    "    if count > 0:  # Only show languages that have at least one line\n",
    "        print(f'{count} lines of language {code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt560_subsets['alz'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FM-mTak-yhN"
   },
   "source": [
    "Now retrieve the actual sentences. This should take ~30 minutes to download and ~50 minutes to iterate over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQ-EIiMgs5as"
   },
   "outputs": [],
   "source": [
    "!wget https://object.pouta.csc.fi/OPUS-MT560/train.v1.eng.tok.gz\n",
    "!wget https://object.pouta.csc.fi/OPUS-MT560/train.v1.src.tok.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "language_codes = set(salt.constants.SALT_LANGUAGE_NAMES.keys())\n",
    "detokenizer = sacremoses.MosesDetokenizer(lang='en')\n",
    "\n",
    "mt560_subsets = collections.defaultdict(list)\n",
    "\n",
    "with gzip.open('train.v1.src.tok.gz', 'r') as src_file, gzip.open('train.v1.eng.tok.gz', 'r') as eng_file:\n",
    "    for line_src, line_eng, language in tqdm(zip(src_file, eng_file, languages['code']), total=len(languages)):\n",
    "        if language in language_codes:\n",
    "            src_processed = detokenizer.detokenize([line_src.decode('utf8')])\n",
    "            eng_processed = detokenizer.detokenize([line_eng.decode('utf8')])\n",
    "            mt560_subsets[language].extend([{f'{language}_target_text': src_processed, 'eng_text': eng_processed}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for selected_language in ['swa', 'kin', 'ibo', 'lug', 'ach', 'nyn', 'koo', 'ttj']:\n",
    "    print('###', selected_language, '###')\n",
    "    df = pd.DataFrame.from_dict(mt560_subsets[selected_language])\n",
    "    \n",
    "    # Filter out rows where the text in one language is more than 2.5 times as long as the other\n",
    "    len_eng = df[\"eng_text\"].str.len()\n",
    "    len_src = df[f\"{selected_language}_target_text\"].str.len()\n",
    "    df = df[\n",
    "        np.maximum(len_eng, len_src) < 2.5 * np.minimum(len_eng, len_src)\n",
    "    ]\n",
    "    \n",
    "    ds = datasets.Dataset.from_pandas(df)\n",
    "    ds.push_to_hub(\n",
    "        \"Sunbird/external-translation-data\",\n",
    "        config_name=f'mt560_{selected_language}_unidirectional',\n",
    "        private=True, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
