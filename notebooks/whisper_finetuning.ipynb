{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZLQq9FJMWLX"
   },
   "outputs": [],
   "source": [
    "!pip install -q jiwer\n",
    "!pip install -q evaluate\n",
    "!pip install -qU accelerate\n",
    "!pip install -q transformers[torch]\n",
    "!git clone https://github.com/sunbirdai/salt.git\n",
    "!pip install -qr salt/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wWOTwrVCqnF"
   },
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "use_mlflow = True\n",
    "\n",
    "if use_wandb:\n",
    "  !pip install -q wandb\n",
    "  import wandb\n",
    "  %set_env WANDB_LOG_MODEL=True\n",
    "  %set_env WANDB_WATCH=all\n",
    "  %set_env WANDB_NOTEBOOK_NAME=whisper_base_en_sb.ipynb\n",
    "  wandb.login()\n",
    "\n",
    "if use_mlflow:\n",
    "  !pip install -q mlflow\n",
    "  ## requirements to log system/GPU metrics in mlflow\n",
    "  !pip install -q psutil\n",
    "  !pip install -q pynvml\n",
    "  import os\n",
    "  from getpass import getpass\n",
    "  import mlflow\n",
    "  import mlflow.pytorch\n",
    "  from mlflow import MlflowClient\n",
    "\n",
    "  # Set MLflow tracking credentials\n",
    "  MLFLOW_TRACKING_USERNAME = getpass('Enter the MLFLOW_TRACKING_USERNAME: ')\n",
    "  os.environ['MLFLOW_TRACKING_USERNAME'] = MLFLOW_TRACKING_USERNAME\n",
    "\n",
    "  MLFLOW_TRACKING_PASSWORD = getpass('Enter the MLFLOW_TRACKING_PASSWORD: ')\n",
    "  os.environ['MLFLOW_TRACKING_PASSWORD'] = MLFLOW_TRACKING_PASSWORD\n",
    "\n",
    "  # Set the MLflow tracking URI\n",
    "  mlflow.set_tracking_uri('https://mlflow-sunbird-ce0ecfc14244.herokuapp.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KKzjwqnb6Dh"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import transformers\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Union, List, Dict, Any\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "import datasets\n",
    "import numpy as np\n",
    "import yaml\n",
    "import evaluate\n",
    "import mlflow\n",
    "import salt.dataset\n",
    "import salt.metrics\n",
    "from salt.utils import DataCollatorCTCWithPadding as dcwp\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlbdSLfKNYfF"
   },
   "outputs": [],
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1erbxiSJBPX"
   },
   "outputs": [],
   "source": [
    "# The following code prepares datasets with one local language plus English\n",
    "language = 'lug'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1Vd4A4UIwLk"
   },
   "outputs": [],
   "source": [
    "yaml_config = f'''\n",
    "pretrained_model: openai/whisper-medium\n",
    "mlflow_experiment_name : stt-whisper-callcentre\n",
    "mlflow_run_name: {language}_eng_from_pretrained\n",
    "\n",
    "training_args:\n",
    "    output_dir: ./whisper-medium-sb-{language}-eng\n",
    "    per_device_train_batch_size: 12\n",
    "    per_device_eval_batch_size: 4\n",
    "    gradient_accumulation_steps: 1  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate: 1.0e-4\n",
    "    warmup_steps: 100\n",
    "    max_steps: 20000\n",
    "    gradient_checkpointing: True\n",
    "    gradient_checkpointing_kwargs:\n",
    "      use_reentrant: True\n",
    "    fp16: True\n",
    "    eval_strategy: steps\n",
    "    predict_with_generate: True\n",
    "    generation_max_length: 100\n",
    "    save_steps: 200\n",
    "    eval_steps: 100\n",
    "    logging_steps: 25\n",
    "    load_best_model_at_end: True\n",
    "    metric_for_best_model: loss\n",
    "    greater_is_better: False\n",
    "    push_to_hub: True\n",
    "    hub_model_id: jq/whisper-medium-sb-lug-eng\n",
    "    save_total_limit: 2\n",
    "\n",
    "Wav2Vec2ForCTC_args:\n",
    "    attention_dropout: 0.0\n",
    "    hidden_dropout: 0.0\n",
    "    feat_proj_dropout: 0.0\n",
    "    layerdrop: 0.0\n",
    "    ctc_loss_reduction: mean\n",
    "    ignore_mismatched_sizes: True\n",
    "\n",
    "train:\n",
    "    huggingface_load:\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-{language}\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-eng\n",
    "          split: train\n",
    "        # Common Voice training data can optionally be added\n",
    "        - path: mozilla-foundation/common_voice_13_0\n",
    "          split: train\n",
    "          name: lg\n",
    "          trust_remote_code: True\n",
    "        # And Google FLEURS\n",
    "        - path: google/fleurs\n",
    "          split: train\n",
    "          name: lg_ug\n",
    "          trust_remote_code: True\n",
    "    source:\n",
    "      type: speech\n",
    "      language: [{language},eng]\n",
    "      preprocessing:\n",
    "        # Downsample some examples to 8KHz (to simulate phone audio) \n",
    "        - set_sample_rate:\n",
    "            rate: 8_000\n",
    "            p: 0.5\n",
    "        # Then upsample again\n",
    "        - set_sample_rate:\n",
    "            rate: 16_000\n",
    "        - augment_audio_noise:\n",
    "            max_relative_amplitude: 0.5\n",
    "    target:\n",
    "      type: text\n",
    "      language: [{language},eng]\n",
    "    shuffle: True\n",
    "validation:\n",
    "    huggingface_load:\n",
    "        # Evaluate on call center data\n",
    "        - path: Sunbird/salt-practical-eval\n",
    "          name: ucfd_eng\n",
    "          split: test\n",
    "        - path: Sunbird/salt-practical-eval\n",
    "          name: ucfd_lug\n",
    "          split: test\n",
    "    source:\n",
    "      type: speech\n",
    "      language: [{language},eng]\n",
    "      preprocessing:\n",
    "        - set_sample_rate:\n",
    "            rate: 16_000\n",
    "    target:\n",
    "      type: text\n",
    "      language: [{language},eng]\n",
    "'''\n",
    "\n",
    "config = yaml.safe_load(yaml_config)\n",
    "train_ds = salt.dataset.create(config['train'])\n",
    "valid_ds = salt.dataset.create(config['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeE3B_MdJxAu"
   },
   "outputs": [],
   "source": [
    "salt.utils.show_dataset(train_ds, audio_features=['source'], N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BNxLEzRpNey"
   },
   "outputs": [],
   "source": [
    "feature_extractor = transformers.WhisperFeatureExtractor.from_pretrained(\n",
    "    config['pretrained_model'])\n",
    "processor = transformers.WhisperProcessor.from_pretrained(\n",
    "    config['pretrained_model'], language=None, task=\"transcribe\")\n",
    "model = transformers.WhisperForConditionalGeneration.from_pretrained(\n",
    "    config['pretrained_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00Jd-YTThouQ"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor, decoder_start_token_id=model.config.decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PiRbHRblWBMo"
   },
   "outputs": [],
   "source": [
    "view_example = False\n",
    "\n",
    "if view_example:\n",
    "  train_iterator = iter(valid_ds)\n",
    "  example = next(train_iterator)\n",
    "\n",
    "  example.keys()\n",
    "  input_str = example[\"target\"]\n",
    "  labels = processor.tokenizer(input_str).input_ids\n",
    "  decoded_with_special = processor.tokenizer.decode(labels, skip_special_tokens=False)\n",
    "  decoded_str = processor.tokenizer.decode(labels, skip_special_tokens=True)\n",
    "\n",
    "  print(f\"Input:                 {input_str}\")\n",
    "  print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
    "  print(f\"Decoded w/out special: {decoded_str}\")\n",
    "  print(f\"Are equal:             {input_str == decoded_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mzVFDogXgLG"
   },
   "outputs": [],
   "source": [
    "# Mapping from SALT languages to Whisper language tokens\n",
    "language_id_tokens = {\n",
    "    'eng': 50259,\n",
    "    'ach': 50357,\n",
    "    'lgg': 50356,\n",
    "    'lug': 50355,\n",
    "    'nyn': 50354,\n",
    "    'teo': 50353,\n",
    "}\n",
    "\n",
    "def prepare_dataset(example):\n",
    "    # Extract the audio data from the 'source' key\n",
    "    audio = example[\"source\"]\n",
    "\n",
    "    # Compute log-Mel input features from the audio array\n",
    "    input_features = feature_extractor(audio, sampling_rate=16000).input_features[0]\n",
    "\n",
    "    # Encode target text to label ids\n",
    "    labels = processor.tokenizer(example[\"target\"]).input_ids\n",
    "\n",
    "    # Insert the language ID token into the second position of the sequence.\n",
    "    labels.insert(1, language_id_tokens[example[\"target.language\"]])\n",
    "\n",
    "    # Create a new dictionary with the processed data\n",
    "    processed_example = {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": labels,\n",
    "        \"source.language\": example[\"source.language\"],\n",
    "        \"target.language\": example[\"target.language\"]\n",
    "    }\n",
    "\n",
    "    return processed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05Zyqa3cYCFW"
   },
   "outputs": [],
   "source": [
    "train_data = train_ds.map(prepare_dataset, remove_columns=[\"source\", \"target\"])\n",
    "val_data = valid_ds.map(prepare_dataset, remove_columns=[\"source\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UB4g9cW4rZ-u"
   },
   "outputs": [],
   "source": [
    "compute_metrics = salt.metrics.multilingual_eval_fn(\n",
    "      valid_ds, [evaluate.load('wer'), evaluate.load('cer')],\n",
    "      processor.tokenizer, log_first_N_predictions=5,\n",
    "      speech_processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCsAGEQtremE"
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d25tMcDRrh-x"
   },
   "outputs": [],
   "source": [
    "report_to = []\n",
    "if use_wandb:\n",
    "    report_to.append('wandb')\n",
    "if use_mlflow:\n",
    "    report_to.append('mlflow')\n",
    "    experiment_name = config['mlflow_experiment_name']\n",
    "    if not mlflow.get_experiment_by_name(experiment_name):\n",
    "      mlflow.create_experiment(experiment_name)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "training_args = transformers.Seq2SeqTrainingArguments(\n",
    "  **config[\"training_args\"],\n",
    "  report_to=report_to\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kfg8N-PkrmoI"
   },
   "outputs": [],
   "source": [
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbzhK-01ruvm"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoicKYTCrxew"
   },
   "outputs": [],
   "source": [
    "model.push_to_hub(\"whisper-medium-lug-eng\")\n",
    "tokenizer.push_to_hub(\"whisper-medium-lug-eng\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the model on the first test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(valid_ds))\n",
    "input_features = processor(example[\"source\"], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n",
    "transcription = processor.decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
