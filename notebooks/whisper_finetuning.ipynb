{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ZLQq9FJMWLX"
   },
   "outputs": [],
   "source": [
    "!pip install -q jiwer\n",
    "!pip install -q evaluate\n",
    "!pip install -qU accelerate\n",
    "!pip install -q transformers[torch]\n",
    "!git clone https://github.com/sunbirdai/salt.git\n",
    "!pip install -qr salt/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wWOTwrVCqnF"
   },
   "outputs": [],
   "source": [
    "use_wandb = False\n",
    "use_mlflow = True\n",
    "\n",
    "if use_wandb:\n",
    "  !pip install -q wandb\n",
    "  import wandb\n",
    "  %set_env WANDB_LOG_MODEL=True\n",
    "  %set_env WANDB_WATCH=all\n",
    "  %set_env WANDB_NOTEBOOK_NAME=whisper_base_en_sb.ipynb\n",
    "  wandb.login()\n",
    "\n",
    "if use_mlflow:\n",
    "  !pip install -q mlflow\n",
    "  ## requirements to log system/GPU metrics in mlflow\n",
    "  !pip install -q psutil\n",
    "  !pip install -q pynvml\n",
    "  import os\n",
    "  from getpass import getpass\n",
    "  import mlflow\n",
    "  import mlflow.pytorch\n",
    "  from mlflow import MlflowClient\n",
    "\n",
    "  # Set MLflow tracking credentials\n",
    "  MLFLOW_TRACKING_USERNAME = getpass('Enter the MLFLOW_TRACKING_USERNAME: ')\n",
    "  os.environ['MLFLOW_TRACKING_USERNAME'] = MLFLOW_TRACKING_USERNAME\n",
    "\n",
    "  MLFLOW_TRACKING_PASSWORD = getpass('Enter the MLFLOW_TRACKING_PASSWORD: ')\n",
    "  os.environ['MLFLOW_TRACKING_PASSWORD'] = MLFLOW_TRACKING_PASSWORD\n",
    "\n",
    "  # Set the MLflow tracking URI\n",
    "  mlflow.set_tracking_uri('https://mlflow-sunbird-ce0ecfc14244.herokuapp.com/')\n",
    "  mlflow.system_metrics.enable_system_metrics_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KKzjwqnb6Dh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Union, List, Dict, Any\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "import datasets\n",
    "import numpy as np\n",
    "import yaml\n",
    "import evaluate\n",
    "import salt.dataset\n",
    "import salt.metrics\n",
    "from salt.utils import DataCollatorCTCWithPadding as dcwp\n",
    "import huggingface_hub\n",
    "import peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlbdSLfKNYfF"
   },
   "outputs": [],
   "source": [
    "huggingface_hub.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "i1Vd4A4UIwLk"
   },
   "outputs": [],
   "source": [
    "\n",
    "yaml_config = f'''\n",
    "pretrained_model: openai/whisper-large-v2\n",
    "mlflow_experiment_name : stt-whisper-lug-eng\n",
    "mlflow_run_name: whisper-large-lug-eng\n",
    "\n",
    "use_peft: True\n",
    "lora_config:\n",
    "    r: 32\n",
    "    lora_alpha: 64\n",
    "    target_modules: [\"q_proj\", \"v_proj\"]\n",
    "    lora_dropout: 0.05\n",
    "    bias: \"none\"\n",
    "\n",
    "training_args:\n",
    "    output_dir: ./artifacts\n",
    "    per_device_train_batch_size: 16\n",
    "    per_device_eval_batch_size: 8\n",
    "    gradient_accumulation_steps: 16  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate: 1.0e-3\n",
    "    warmup_steps: 50\n",
    "    max_steps: 2000\n",
    "    gradient_checkpointing: True\n",
    "    gradient_checkpointing_kwargs:\n",
    "      use_reentrant: True\n",
    "    fp16: True\n",
    "    eval_strategy: steps\n",
    "    predict_with_generate: True\n",
    "    generation_max_length: 100\n",
    "    save_steps: 50\n",
    "    eval_steps: 50\n",
    "    logging_steps: 50\n",
    "    load_best_model_at_end: True\n",
    "    metric_for_best_model: loss\n",
    "    greater_is_better: False\n",
    "    push_to_hub: True\n",
    "    hub_model_id: whisper-large-multilingual-adapter\n",
    "    save_total_limit: 3\n",
    "\n",
    "train:\n",
    "    huggingface_load:\n",
    "        # Call centre data\n",
    "        - path: Sunbird/salt-ucfd\n",
    "          name: ucfd_eng\n",
    "          split: train\n",
    "        - path: Sunbird/salt-ucfd\n",
    "          name: ucfd_lug\n",
    "          split: train    \n",
    "        # Main SALT ASR training data\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-lug\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-eng\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-ach\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-lgg\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-teo\n",
    "          split: train\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-nyn\n",
    "          split: train\n",
    "        # Common Voice\n",
    "        - path: mozilla-foundation/common_voice_13_0\n",
    "          split: train\n",
    "          name: lg\n",
    "          trust_remote_code: True\n",
    "        # Google FLEURS\n",
    "        - path: google/fleurs\n",
    "          split: train\n",
    "          name: lg_ug\n",
    "          trust_remote_code: True\n",
    "    source:\n",
    "      type: speech\n",
    "      language: [lug,eng,ach,lgg,teo,nyn]\n",
    "      preprocessing:\n",
    "        # Downsample some examples to 8KHz (to simulate phone audio) \n",
    "        - set_sample_rate:\n",
    "            rate: 8_000\n",
    "            p: 0.2\n",
    "        # Then upsample again\n",
    "        - set_sample_rate:\n",
    "            rate: 16_000\n",
    "        - augment_audio_noise:\n",
    "            max_relative_amplitude: 0.5\n",
    "    target:\n",
    "      type: text\n",
    "      language: [lug,eng,ach,lgg,teo,nyn]\n",
    "    shuffle: True\n",
    "validation:\n",
    "    huggingface_load:\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-eng\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-lug\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-ach\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-lgg\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-teo\n",
    "          split: dev\n",
    "        - path: Sunbird/salt\n",
    "          name: multispeaker-nyn\n",
    "          split: dev\n",
    "    source:\n",
    "      type: speech\n",
    "      language: [lug,eng,ach,lgg,teo,nyn]\n",
    "      preprocessing:\n",
    "        - set_sample_rate:\n",
    "            rate: 16_000\n",
    "    target:\n",
    "      type: text\n",
    "      language: [lug,eng,ach,lgg,teo,nyn]\n",
    "'''\n",
    "\n",
    "config = yaml.safe_load(yaml_config)\n",
    "train_ds = salt.dataset.create(config['train'])\n",
    "valid_ds = salt.dataset.create(config['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yeE3B_MdJxAu"
   },
   "outputs": [],
   "source": [
    "salt.utils.show_dataset(train_ds, audio_features=['source'], N=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BNxLEzRpNey"
   },
   "outputs": [],
   "source": [
    "feature_extractor = transformers.WhisperFeatureExtractor.from_pretrained(\n",
    "    config['pretrained_model'])\n",
    "processor = transformers.WhisperProcessor.from_pretrained(\n",
    "    config['pretrained_model'], language=None, task=\"transcribe\")\n",
    "model = transformers.WhisperForConditionalGeneration.from_pretrained(\n",
    "    config['pretrained_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00Jd-YTThouQ"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]    \n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "    processor=processor, decoder_start_token_id=model.config.decoder_start_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mzVFDogXgLG"
   },
   "outputs": [],
   "source": [
    "# Mapping from SALT languages to Whisper language tokens\n",
    "language_id_tokens = {\n",
    "    'eng': 50259,\n",
    "    'ach': 50357,\n",
    "    'lgg': 50356,\n",
    "    'lug': 50355,\n",
    "    'nyn': 50354,\n",
    "    'teo': 50353,\n",
    "}\n",
    "\n",
    "def prepare_dataset(example):\n",
    "    # Extract the audio data from the 'source' key\n",
    "    audio = example[\"source\"]\n",
    "\n",
    "    # Compute log-Mel input features from the audio array\n",
    "    input_features = feature_extractor(\n",
    "        audio, sampling_rate=16000, device='cuda').input_features[0]\n",
    "\n",
    "    # Encode target text to label ids\n",
    "    labels = processor.tokenizer(example[\"target\"]).input_ids\n",
    "\n",
    "    # Insert the language ID token into the second position of the sequence.\n",
    "    labels.insert(1, language_id_tokens[example[\"target.language\"]])\n",
    "\n",
    "    # Create a new dictionary with the processed data\n",
    "    processed_example = {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": labels,\n",
    "        \"source.language\": example[\"source.language\"],\n",
    "        \"target.language\": example[\"target.language\"]\n",
    "    }\n",
    "\n",
    "    return processed_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05Zyqa3cYCFW"
   },
   "outputs": [],
   "source": [
    "train_data = train_ds.map(prepare_dataset, remove_columns=[\"source\", \"target\"])\n",
    "val_data = valid_ds.map(prepare_dataset, remove_columns=[\"source\", \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UB4g9cW4rZ-u"
   },
   "outputs": [],
   "source": [
    "compute_metrics = salt.metrics.multilingual_eval_fn(\n",
    "      valid_ds, [evaluate.load('wer'), evaluate.load('cer')],\n",
    "      processor.tokenizer, log_first_N_predictions=5,\n",
    "      speech_processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCsAGEQtremE"
   },
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "\n",
    "if config['use_peft']:\n",
    "    model = peft.prepare_model_for_kbit_training(model)\n",
    "    lora_config = peft.LoraConfig(**config['lora_config'])\n",
    "    model.enable_input_require_grads()\n",
    "    model = peft.get_peft_model(model, lora_config)\n",
    "    model.config.use_cache = False\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d25tMcDRrh-x"
   },
   "source": [
    "Launch the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Kfg8N-PkrmoI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Too many dataloader workers: 4 (max is dataset.n_shards=1). Stopping 3 dataloader workers.\n",
      "WARNING:datasets.iterable_dataset:Too many dataloader workers: 4 (max is dataset.n_shards=1). Stopping 3 dataloader workers.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n    data.append(next(self.dataset_iter))\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 2011, in __iter__\n    yield from self._iter_pytorch()\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1936, in _iter_pytorch\n    for key, example in ex_iterable:\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 954, in __iter__\n    yield from self._iter()\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1045, in _iter\n    transformed_example.update(self.function(*function_args, **self.fn_kwargs))\n  File \"/tmp/ipykernel_10238/1754533562.py\", line 16, in prepare_dataset\n    input_features = feature_extractor(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/whisper/feature_extraction_whisper.py\", line 306, in __call__\n    input_features = extract_fbank_features(input_features[0], device)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/whisper/feature_extraction_whisper.py\", line 136, in _torch_extract_fbank_features\n    waveform = waveform.to(device)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 288, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mSeq2SeqTrainingArguments(\n\u001b[1;32m      2\u001b[0m   \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_args\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m   report_to\u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      4\u001b[0m       platform \u001b[38;5;28;01mfor\u001b[39;00m platform, use \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_wandb), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlflow\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_mlflow)] \u001b[38;5;28;01mif\u001b[39;00m use]\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mSeq2SeqTrainer(\n\u001b[1;32m      8\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mfeature_extractor,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1939\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1937\u001b[0m     \u001b[38;5;66;03m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1938\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1946\u001b[0m     hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2246\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2243\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2245\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2247\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:677\u001b[0m, in \u001b[0;36mDataLoaderDispatcher.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    676\u001b[0m first_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m next_batch, next_batch_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    678\u001b[0m batch_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_iteration:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:631\u001b[0m, in \u001b[0;36mDataLoaderDispatcher._fetch_batches\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    629\u001b[0m batches \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mnum_processes):\n\u001b[0;32m--> 631\u001b[0m     batches\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     batch \u001b[38;5;241m=\u001b[39m concatenate(batches, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\n    data.append(next(self.dataset_iter))\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 2011, in __iter__\n    yield from self._iter_pytorch()\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1936, in _iter_pytorch\n    for key, example in ex_iterable:\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 954, in __iter__\n    yield from self._iter()\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1045, in _iter\n    transformed_example.update(self.function(*function_args, **self.fn_kwargs))\n  File \"/tmp/ipykernel_10238/1754533562.py\", line 16, in prepare_dataset\n    input_features = feature_extractor(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/whisper/feature_extraction_whisper.py\", line 306, in __call__\n    input_features = extract_fbank_features(input_features[0], device)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/whisper/feature_extraction_whisper.py\", line 136, in _torch_extract_fbank_features\n    waveform = waveform.to(device)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 288, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n"
     ]
    }
   ],
   "source": [
    "training_args = transformers.Seq2SeqTrainingArguments(\n",
    "  **config[\"training_args\"],\n",
    "  report_to= [\n",
    "      platform for platform, use in [(\"wandb\", use_wandb), (\"mlflow\", use_mlflow)] if use]\n",
    ")\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log the config settings for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_mlflow:\n",
    "    last_run_id = mlflow.last_active_run().info.run_id\n",
    "    with mlflow.start_run(run_id=last_run_id):\n",
    "        mlflow.log_params(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the full model (not just the adapter weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoicKYTCrxew"
   },
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()\n",
    "model.push_to_hub(config['training_args']['hub_model_id'] + '-merged')\n",
    "processor.push_to_hub(config['training_args']['hub_model_id'] + '-merged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the model on the first test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(valid_ds))\n",
    "input_features = processor(example[\"source\"], sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "with torch.no_grad():\n",
    "    predicted_ids = model.generate(input_features.to(\"cuda\"))[0]\n",
    "transcription = processor.decode(predicted_ids)\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
