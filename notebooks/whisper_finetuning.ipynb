{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZLQq9FJMWLX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
        "!apt update\n",
        "!apt install -y ffmpeg\n",
        "\n",
        "!pip uninstall -y transformers datasets\n",
        "!pip install audiomentations\n",
        "!pip install git+https://github.com/huggingface/datasets\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa soundfile\n",
        "!pip install evaluate>=0.3.0\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install more-itertools\n",
        "!pip install wandb\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate -U\n",
        "##more"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wWOTwrVCqnF"
      },
      "outputs": [],
      "source": [
        "%set_env WANDB_LOG_MODEL=True\n",
        "%set_env WANDB_WATCH=all\n",
        "%set_env WANDB_NOTEBOOK_NAME=whisper_base_en_sb.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjEaZWcnhv8o"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, IterableDatasetDict, load_dataset, interleave_datasets, Audio\n",
        "import evaluate\n",
        "\n",
        "import torch\n",
        "import string\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "from transformers import WhisperForConditionalGeneration\n",
        "from transformers import WhisperProcessor\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers import Seq2SeqTrainer\n",
        "from transformers import WhisperTokenizer\n",
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "\n",
        "import wandb\n",
        "from IPython.display import clear_output\n",
        "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
        "import numpy as np\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainerCallback\n",
        "from transformers.integrations import WandbCallback\n",
        "from transformers.trainer_pt_utils import IterableDatasetShard\n",
        "from torch.utils.data import IterableDataset\n",
        "from datasets import load_dataset, Audio\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import holoviews as hv\n",
        "import panel as pn\n",
        "import tempfile\n",
        "from bokeh.resources import INLINE\n",
        "hv.extension(\"bokeh\", logo=False)\n",
        "\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import jiwer\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "clear_output()\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8fCjwvgh3OW"
      },
      "outputs": [],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlbdSLfKNYfF"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sunbirdai/leb.git\n",
        "!pip install -r leb/requirements.txt\n",
        "!pip install datasets==2.16.1\n",
        "!pip install mlflow\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "mvYfnhpSTM5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "import leb.dataset\n",
        "from leb.utils import DataCollatorCTCWithPadding as dcwp\n",
        "from datasets import Audio\n",
        "from datasets import load_dataset, DatasetDict"
      ],
      "metadata": {
        "id": "WZKt9pC8UFSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# languages currently available in SALT multispeaker STT dataset\n",
        "languages = {\n",
        "    \"english\": \"eng\"\n",
        "}\n",
        "\n",
        "yaml_config = '''\n",
        "common_source: &common_source\n",
        "  type: speech\n",
        "  language: [eng]\n",
        "  preprocessing:\n",
        "    - set_sample_rate:\n",
        "        rate: 16_000\n",
        "\n",
        "common_target: &common_target\n",
        "  type: text\n",
        "  language: [eng]\n",
        "  preprocessing:\n",
        "    - lower_case\n",
        "    - clean_and_remove_punctuation\n",
        "\n",
        "Wav2Vec2ForCTC_args:\n",
        "    attention_dropout: 0.0\n",
        "    hidden_dropout: 0.0\n",
        "    feat_proj_dropout: 0.0\n",
        "    layerdrop: 0.0\n",
        "    ctc_loss_reduction: mean\n",
        "    ignore_mismatched_sizes: True\n",
        "\n",
        "train:\n",
        "    huggingface_load:\n",
        "        # - path: mozilla-foundation/common_voice_13_0\n",
        "        #   split: train\n",
        "        #   name: lg\n",
        "        #   trust_remote_code: True\n",
        "        - path: Sunbird/salt\n",
        "          name: multispeaker-eng\n",
        "          split: train\n",
        "    source: *common_source\n",
        "\n",
        "    target: *common_target\n",
        "    shuffle: True\n",
        "validation:\n",
        "    huggingface_load:\n",
        "        - path: Sunbird/salt\n",
        "          name: multispeaker-eng\n",
        "          split: dev\n",
        "    source: *common_source\n",
        "    target: *common_target\n",
        "\n",
        "'''\n",
        "\n",
        "config = yaml.safe_load(yaml_config)\n",
        "train_ds = leb.dataset.create(config['train'])\n",
        "valid_ds = leb.dataset.create(config['validation'])"
      ],
      "metadata": {
        "id": "NGonq6dLTXcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "language = 'eng'"
      ],
      "metadata": {
        "id": "E1erbxiSJBPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yaml_config = f'''\n",
        "pretrained_model: openai/whisper-base\n",
        "pretrained_adapter: {language}\n",
        "mlflow_experiment_name : stt-whisper-{language}\n",
        "mlflow_run_name: {language}_from_pretrained\n",
        "\n",
        "training_args:\n",
        "    output_dir: stt\n",
        "    per_device_train_batch_size: 24\n",
        "    gradient_accumulation_steps: 2\n",
        "    evaluation_strategy: steps\n",
        "    max_steps: 1200\n",
        "    gradient_checkpointing: True\n",
        "    gradient_checkpointing_kwargs:\n",
        "      use_reentrant: True\n",
        "    fp16: True\n",
        "    save_steps: 100\n",
        "    eval_steps: 100\n",
        "    logging_steps: 100\n",
        "    learning_rate: 3.0e-4\n",
        "    warmup_steps: 100\n",
        "    save_total_limit: 2\n",
        "    # push_to_hub: True\n",
        "    load_best_model_at_end: True\n",
        "    metric_for_best_model: loss\n",
        "    greater_is_better: False\n",
        "    weight_decay: 0.01\n",
        "\n",
        "Wav2Vec2ForCTC_args:\n",
        "    attention_dropout: 0.0\n",
        "    hidden_dropout: 0.0\n",
        "    feat_proj_dropout: 0.0\n",
        "    layerdrop: 0.0\n",
        "    ctc_loss_reduction: mean\n",
        "    ignore_mismatched_sizes: True\n",
        "\n",
        "train:\n",
        "    huggingface_load:\n",
        "        - path: Sunbird/salt\n",
        "          name: multispeaker-{language}\n",
        "          split: train\n",
        "    source:\n",
        "      type: speech\n",
        "      language: [{language},eng]\n",
        "      preprocessing:\n",
        "        - set_sample_rate:\n",
        "            rate: 16_000\n",
        "        - augment_audio_noise:\n",
        "            max_relative_amplitude: 0.5\n",
        "    target:\n",
        "      type: text\n",
        "      language: [{language},eng]\n",
        "      preprocessing:\n",
        "        - lower_case\n",
        "        - clean_and_remove_punctuation:\n",
        "            allowed_punctuation: \"'\"\n",
        "    shuffle: True\n",
        "validation:\n",
        "    huggingface_load:\n",
        "        - path: Sunbird/salt\n",
        "          name: multispeaker-{language}\n",
        "          split: dev\n",
        "        - path: Sunbird/salt\n",
        "          name: multispeaker-eng\n",
        "          split: dev\n",
        "    source:\n",
        "      type: speech\n",
        "      language: [{language},eng]\n",
        "      preprocessing:\n",
        "        - set_sample_rate:\n",
        "            rate: 16_000\n",
        "    target:\n",
        "      type: text\n",
        "      language: [{language},eng]\n",
        "      preprocessing:\n",
        "        - lower_case\n",
        "        - clean_and_remove_punctuation:\n",
        "            allowed_punctuation: \"'\"\n",
        "'''\n",
        "\n",
        "config = yaml.safe_load(yaml_config)\n",
        "train_ds = leb.dataset.create(config['train'])\n",
        "valid_ds = leb.dataset.create(config['validation'])"
      ],
      "metadata": {
        "id": "i1Vd4A4UIwLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config['pretrained_model']"
      ],
      "metadata": {
        "id": "yeE3B_MdJxAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BNxLEzRpNey"
      },
      "outputs": [],
      "source": [
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(config['pretrained_model'])\n",
        "tokenizer = WhisperTokenizer.from_pretrained(config['pretrained_model'], language=\"english\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterator = iter(train_ds)\n",
        "example = next(train_iterator)"
      ],
      "metadata": {
        "id": "FiXMtMIBUr3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example.keys()"
      ],
      "metadata": {
        "id": "gLwe5xTqWRTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = example[\"target\"]"
      ],
      "metadata": {
        "id": "PiRbHRblWBMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JMU40AMpidl"
      },
      "outputs": [],
      "source": [
        "labels = tokenizer(input_str).input_ids\n",
        "decoded_with_special = tokenizer.decode(labels, skip_special_tokens=False)\n",
        "decoded_str = tokenizer.decode(labels, skip_special_tokens=True)\n",
        "\n",
        "print(f\"Input:                 {input_str}\")\n",
        "print(f\"Decoded w/ special:    {decoded_with_special}\")\n",
        "print(f\"Decoded w/out special: {decoded_str}\")\n",
        "print(f\"Are equal:             {input_str == decoded_str}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPAhoH_lpqUX"
      },
      "outputs": [],
      "source": [
        "processor = WhisperProcessor.from_pretrained(config['pretrained_model'], language=None, task=\"transcribe\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(example):\n",
        "    # Extract the audio data from the 'source' key\n",
        "    audio = example[\"source\"]\n",
        "\n",
        "    # Compute log-Mel input features from the audio array\n",
        "    input_features = feature_extractor(audio, sampling_rate=16000).input_features[0]\n",
        "\n",
        "    # Encode target text to label ids\n",
        "    labels = tokenizer(example[\"target\"]).input_ids\n",
        "\n",
        "    # Create a new dictionary with the processed data\n",
        "    processed_example = {\n",
        "        \"input_features\": input_features,\n",
        "        \"labels\": labels,\n",
        "        \"source.language\": example[\"source.language\"],\n",
        "        \"target.language\": example[\"target.language\"]\n",
        "    }\n",
        "\n",
        "    return processed_example"
      ],
      "metadata": {
        "id": "4mzVFDogXgLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_ds.map(prepare_dataset, remove_columns=[\"source\", \"target\"])\n",
        "val_data = valid_ds.map(prepare_dataset, remove_columns=[\"source\", \"target\"])"
      ],
      "metadata": {
        "id": "05Zyqa3cYCFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterator = iter(train_data)\n",
        "example = next(train_iterator)\n"
      ],
      "metadata": {
        "id": "kUbbMNlsYkh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv8uTaizqhrZ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVDQXPfnrVk3"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWWP3rFUrX2m"
      },
      "outputs": [],
      "source": [
        "\n",
        "metric = evaluate.load(\"wer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB4g9cW4rZ-u"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZeBOMyurcjH"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(config['pretrained_model'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCsAGEQtremE"
      },
      "outputs": [],
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []\n",
        "# forced_decoder_ids = processor.tokenizer.get_decoder_prompt_ids(language=\"Swahili\", task=\"transcribe\")\n",
        "\n",
        "\n",
        "def custom_generate(self, *args, **kwargs):\n",
        "    kwargs[\"language\"] = \"en\" # 'en', 'nl'\n",
        "\n",
        "    return WhisperForConditionalGeneration.generate(self, *args, **kwargs)\n",
        "\n",
        "model.generate = custom_generate.__get__(model, WhisperForConditionalGeneration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d25tMcDRrh-x"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-base-sb-english\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=1200,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=200,\n",
        "    eval_steps=200,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"wandb\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfg8N-PkrmoI"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=val_data,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbzhK-01ruvm"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoicKYTCrxew"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(\"akera/whisper-base-sb-english\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xg9npjoiVLAt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}