{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOKTAv/2UJ1tdhsyCQYPWJ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunbirdAI/salt/blob/main/notebooks/DSA_2024_Tutorial_on_speech_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q jiwer\n",
        "!pip install -q evaluate\n",
        "!pip install -qU accelerate\n",
        "!pip install -q transformers[torch]\n",
        "!git clone https://github.com/sunbirdai/salt.git\n",
        "!pip install -qr salt/requirements.txt"
      ],
      "metadata": {
        "id": "RjEQcUkRFmhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import salt.dataset\n",
        "import salt.utils\n",
        "import salt.metrics\n",
        "import yaml\n",
        "import transformers\n",
        "from IPython import display\n",
        "import torch\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "1J8tRFBcOpdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start with some of Meta's models: [MMS](https://huggingface.co/docs/transformers/en/model_doc/mms) for speech recognition and generation, and [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb) for translation. These support several African languages, so make a useful starting point. However for some languages, training data was limited and so the models need some refinement. We'll look at both how to add on a new language which the model doesn't know about at all, and also how to improve performance for a language which is supported in the model but not very well.\n",
        "\n",
        "First of all, let's run through an example: how to fine tune an English speech recognition model to work better with a specific accent, in this case Ugandan. If you want to try this for another language, select one below (`lug`=Luganda, `ach`=Acholi, `teo`=Ateso, `nyn`=Runyankole, `lgg`=Lugbara).\n",
        "\n",
        "We'll start by loading some evaluation data."
      ],
      "metadata": {
        "id": "PZcsqG52GJAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language = 'eng' #@param [\"eng\", \"lug\", \"ach\", \"nyn\", \"teo\", \"lgg\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QGI8186K9C8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset_config = f'''\n",
        "huggingface_load:\n",
        "  path: Sunbird/salt\n",
        "  split: dev\n",
        "  name: multispeaker-{language}\n",
        "source:\n",
        "  type: speech\n",
        "  language: {language}\n",
        "  preprocessing:\n",
        "    - set_sample_rate:\n",
        "        rate: 16_000\n",
        "target:\n",
        "  type: text\n",
        "  language: {language}\n",
        "  preprocessing:\n",
        "    - lower_case\n",
        "    - clean_and_remove_punctuation:\n",
        "        allowed_punctuation: \"'\"\n",
        "shuffle: True\n",
        "'''\n",
        "\n",
        "config = yaml.safe_load(validation_dataset_config)\n",
        "ds_validation = salt.dataset.create(config)\n",
        "salt.utils.show_dataset(ds_validation, N=5, audio_features=['source'])"
      ],
      "metadata": {
        "id": "PjIRYysP9u7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load a base model (Meta MMS) and see what output it gives for one of these audio samples."
      ],
      "metadata": {
        "id": "AhgwC52hQvRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = 'facebook/mms-1b-all'\n",
        "\n",
        "processor = transformers.Wav2Vec2Processor.from_pretrained(pretrained_model)\n",
        "processor.tokenizer.set_target_lang(language)\n",
        "data_collator = salt.utils.DataCollatorCTCWithPadding(\n",
        "    processor=processor, padding=True)\n",
        "# Is there a GPU?\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = transformers.Wav2Vec2ForCTC.from_pretrained(pretrained_model).to(device)\n",
        "\n",
        "model.load_adapter(language)"
      ],
      "metadata": {
        "id": "f5ti1tQDRMWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a single example from the test set"
      ],
      "metadata": {
        "id": "5jL7HNtCZaLY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = next(iter(ds_validation))"
      ],
      "metadata": {
        "id": "MErM4IdbXUe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hear the audio and see what the correct text should be"
      ],
      "metadata": {
        "id": "cwiBhoFeZdAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display.display(display.Audio(data=example['source'], rate=16000))\n",
        "print('Correct text: ' + example['target'])"
      ],
      "metadata": {
        "id": "zZYT7byUYHmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What does the model say?"
      ],
      "metadata": {
        "id": "szqXOPi9Zh3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(example['source'], sampling_rate=16_000, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs.to(device)).logits\n",
        "\n",
        "ids = torch.argmax(outputs, dim=-1)[0]\n",
        "transcription = processor.decode(ids)\n",
        "\n",
        "print('Model prediction: ' + transcription)"
      ],
      "metadata": {
        "id": "F1Sv6JoaYFzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll evaluate a base model using this data, to see how well it does on Ugandan English."
      ],
      "metadata": {
        "id": "1IvM4vVlQUtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    batch[\"input_values\"] = processor(\n",
        "        batch[\"source\"], sampling_rate=16000\n",
        "    ).input_values\n",
        "    batch[\"labels\"] = processor(text=batch[\"target\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "validation_data_tokenised = ds_validation.map(\n",
        "    prepare_dataset,\n",
        "    batch_size=4,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "compute_metrics = salt.metrics.multilingual_eval_fn(\n",
        "      ds_validation, [evaluate.load('wer'), evaluate.load('cer')],\n",
        "      processor.tokenizer, log_first_N_predictions=2,\n",
        "      speech_processor=processor)"
      ],
      "metadata": {
        "id": "HQdwR84HPgDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers.Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    eval_dataset=validation_data_tokenised,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ").evaluate()"
      ],
      "metadata": {
        "id": "DxFUWzRAhp0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine tune the model with training data which is more representative. Note that we'll do some augmentation on the training data, adding some noise. This makes the task a little more difficult and adds some extra variation, which in practice makes the model more robust to audio samples where there is noise in the background. It's also possible to augment the speed and pitch, for example."
      ],
      "metadata": {
        "id": "1n7u0ZV8jJO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_config = f'''\n",
        "huggingface_load:\n",
        "  path: Sunbird/salt\n",
        "  split: train\n",
        "  name: multispeaker-{language}\n",
        "source:\n",
        "  type: speech\n",
        "  language: {language}\n",
        "  preprocessing:\n",
        "    - set_sample_rate:\n",
        "        rate: 16_000\n",
        "    - augment_audio_noise:\n",
        "        max_relative_amplitude: 0.5\n",
        "target:\n",
        "  type: text\n",
        "  language: {language}\n",
        "  preprocessing:\n",
        "    - lower_case\n",
        "    - clean_and_remove_punctuation:\n",
        "        allowed_punctuation: \"'\"\n",
        "shuffle: True\n",
        "'''\n",
        "\n",
        "config = yaml.safe_load(train_dataset_config)\n",
        "ds_train = salt.dataset.create(config)\n",
        "salt.utils.show_dataset(ds_train, N=5, audio_features=['source'])"
      ],
      "metadata": {
        "id": "HMBEok8QjZjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start training. We'll just run a few training steps here to see the processs, but leaving it for longer usually results in a better model."
      ],
      "metadata": {
        "id": "J5MIEZd5kEKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = yaml.safe_load('''\n",
        "    output_dir: stt\n",
        "    per_device_train_batch_size: 8\n",
        "    gradient_accumulation_steps: 4\n",
        "    evaluation_strategy: steps\n",
        "    max_steps: 100\n",
        "    gradient_checkpointing: True\n",
        "    gradient_checkpointing_kwargs:\n",
        "      use_reentrant: True\n",
        "    fp16: True\n",
        "    save_steps: 100\n",
        "    eval_steps: 20\n",
        "    logging_steps: 100\n",
        "    learning_rate: 3.0e-4\n",
        "    warmup_steps: 100\n",
        "    save_total_limit: 2\n",
        "    push_to_hub: False\n",
        "    load_best_model_at_end: True\n",
        "    metric_for_best_model: loss\n",
        "    greater_is_better: False\n",
        "    weight_decay: 0.01\n",
        "''')\n",
        "\n",
        "train_data_tokenised = ds_train.map(\n",
        "    prepare_dataset,\n",
        "    batch_size=4,\n",
        "    batched=True,\n",
        ")\n",
        "\n",
        "# We don't train all of the model, only the language-specific adapter layers.\n",
        "model.freeze_base_model()\n",
        "adapter_weights = model._get_adapters()\n",
        "for param in adapter_weights.values():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Set up the trainer and get started.\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=transformers.TrainingArguments(**training_args, report_to=\"none\"),\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_data_tokenised,\n",
        "    eval_dataset=validation_data_tokenised,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "76V0KNreguZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what the performance on the validation set is now, at whatever point training was stopped."
      ],
      "metadata": {
        "id": "1yX17SJVnGcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()"
      ],
      "metadata": {
        "id": "sINarhBcnEdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilingual speech recognition\n",
        "\n",
        "We can actually train a model to be able to recognise more than one language. This helps e.g. for code switching, where someone mainly speaks in one language but uses some terms from a different language. Here's an example of how we can create Luganda + English training data."
      ],
      "metadata": {
        "id": "COpEUkpP_Xby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multilingual_dataset_config = '''\n",
        "huggingface_load:\n",
        "  - path: Sunbird/salt\n",
        "    split: train\n",
        "    name: multispeaker-eng\n",
        "  - path: Sunbird/salt\n",
        "    split: train\n",
        "    name: multispeaker-lug\n",
        "source:\n",
        "  type: speech\n",
        "  language: [eng, lug]\n",
        "  preprocessing:\n",
        "    - set_sample_rate:\n",
        "        rate: 16_000\n",
        "target:\n",
        "  type: text\n",
        "  language: [eng, lug]\n",
        "  preprocessing:\n",
        "    - lower_case\n",
        "    - clean_and_remove_punctuation:\n",
        "        allowed_punctuation: \"'\"\n",
        "shuffle: True\n",
        "'''\n",
        "\n",
        "config = yaml.safe_load(multilingual_dataset_config)\n",
        "ds_multilingual = salt.dataset.create(config)\n",
        "salt.utils.show_dataset(ds_multilingual, N=5, audio_features=['source'])"
      ],
      "metadata": {
        "id": "AUodijsX_olV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your turn!\n",
        "\n",
        "# Speech recognition data collection\n",
        "\n",
        "Select a language you'd like to work on, and get the ISO 639-3 code [here](https://iso639-3.sil.org/code_tables/639/data). For example, Swahili is `swh` or Luganda is `lug`. Then we can form some groups in the classroom so that everyone interested in a particular language can work together. The more people's voices that can be used to train a model, the better it will work."
      ],
      "metadata": {
        "id": "qXSYC9IS6j0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_code = \"eng\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "yxdZuO10GJ6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to create some phrases in your language of interest and then make some recordings of what they sound like when spoken.\n",
        "\n",
        "For our simple example, make a copy of [this spreadsheet](https://docs.google.com/spreadsheets/d/1w6TbJsv1gTZmPI8kkZ0Wr_yKpRjBHqRNJsKYROx4zQQ/edit#gid=1347849995), which has English phrases. Translate some of the phrases to your language of interest. These are the sentences to be read out and recorded.\n",
        "\n",
        "Select some rows from the translation spreadsheet, and paste them into [this tool](https://sunbirdai.github.io/dsa2024-speech-data-recording/).\n",
        "Download the resulting files, and you will find that this can be used to create a HuggingFace dataset using [this notebook](https://colab.research.google.com/drive/1UuacvElXeS58GGw_-CfUXj2KuqJHqCbM#scrollTo=hzSVaYc3mYrJ) as an example."
      ],
      "metadata": {
        "id": "a8jyGNpizh5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_huggingface_repo = '' # e.g. yourusername/datasetname\n",
        "\n",
        "# Load the new data with this config\n",
        "eval_dataset_config = f'''\n",
        "huggingface_load:\n",
        "  path: {your_huggingface_repo}\n",
        "  split: train\n",
        "source:\n",
        "  type: speech\n",
        "  language: {language_code}\n",
        "  preprocessing:\n",
        "    - set_sample_rate:\n",
        "        rate: 16_000\n",
        "target:\n",
        "  type: text\n",
        "  language: {language_code}\n",
        "  preprocessing:\n",
        "    - lower_case\n",
        "    - clean_and_remove_punctuation:\n",
        "        allowed_punctuation: \"'\"\n",
        "shuffle: True\n",
        "'''\n",
        "\n",
        "config = yaml.safe_load(eval_dataset_config)\n",
        "ds_eval = salt.dataset.create(config)\n",
        "salt.utils.show_dataset(ds_eval, N=5, audio_features=['source'])"
      ],
      "metadata": {
        "id": "7sKICytSk4VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pointers on translation\n",
        "\n",
        "We'll also give some tips here on how text-to-text translation models can be trained. The output of a speech recognition model can be fed into a translation model, creating some interesting application possibilities.\n",
        "\n",
        "*   Make a copy of [this](https://docs.google.com/spreadsheets/d/1FNGvg_IkUNvRbK8_6XFcmI4J5DxFHYEmS7d4KF0R1YI/edit#gid=1347849995) spreadsheet. Notice that there are two tabs: train and test.\n",
        "*   Add a new column `text_[languagecode]`, where `languagecode` is the ISO 639-3 code you found above.\n",
        "*   Download the `train` and `test` tabs as a csv file.\n",
        "*   These can be uploaded to HuggingFace.\n",
        "*   [Reference code](https://github.com/SunbirdAI/salt/blob/main/notebooks/NLLB_training.ipynb) for training a translation model."
      ],
      "metadata": {
        "id": "1NjNESlMvzYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pointers on speech generation (text-to-speech)\n",
        "\n",
        "Text-to-speech models are trained using very similar data as for speech recognition. However, the recordings need to be made under more controlled conditions. Ideally this is done in a studio, so that the sound quality is good, without background noise, and with sentences spoken by someone who is trained as a presenter or voice actor.\n",
        "\n",
        "As above, the Meta MMS models do support text-to-speech for several African languages, though quality varies depending on the language and some need to be retrained for practical usage."
      ],
      "metadata": {
        "id": "3AOOVqgJESMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pan-African models 🌍\n",
        "\n",
        "All of this data can be joined up, so that we train single models which can understand and translate between many African languages.\n",
        "\n",
        "Add your HuggingFace repository IDs in the slack channel."
      ],
      "metadata": {
        "id": "B9SD0LGQztDh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8DtCIAWDsIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}